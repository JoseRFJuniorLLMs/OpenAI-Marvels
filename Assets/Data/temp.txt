 2  
 The   
Innovator’s 
Dilemma   
When New Technologies 
Cause Great Firms 
to Fail   
  
CLAYTON M. CHRISTENSEN  
  
  
Harvard Business School Press   
Boston, Massachusetts 
 
 
 
 
 
 
 
 
 
 
 
 
 
    3  
Copyright © 1997 by the President and Fellows of Ha rvard College 
All rights reserved 
 
The Library of Congress has catalogued the hardcove r edition of this title as follows:   
 
Christensen, Clayton M. 
   The innovator’s dilemma : when new technologies cause great firms to fail / Clayton M. Christensen.  
    p.  cm. — (The management of innovation and cha nge series) 
   Includes index. 
   ISBN 0-87584-585-1 (alk. paper) 
   1. Creative ability in business. 2. Industrial m anagement. 3. Customer services. 4. Success in 
business. I. Title. II. Series. 
   HD53.C49  1997 
   658—DC20       96-10894  
              CIP 
 
ISBN 0-87584-585-1 (Microsoft Reader edition) 
 
 
 
 
 
 
 
 
 
 
  4  
 
Contents 
  
    In Gratitude   
    Introduction  
PART ONE: WHY GREAT COMPANIES CAN FAIL   
    1 How Can Great Firms Fail? Insights from the Hard Disk Drive Industry   
    2 Value Networks and the Impetus to Innovate   
    3 Disruptive Technological Change in the Mechanical  Excavator Industry   
    4 What Goes Up, Can’t Go Down   
PART TWO: MANAGING DISRUPTIVE TECHNOLOGICAL CHANGE   
    5 Give Responsibility for Disruptive Technologies t o Organizations Whose Customers Need Them  
    6 Match the Size of the Organization to the Size of  the Market  
    7 Discovering New and Emerging Markets  
    8 How to Appraise Your Organization’s Capabilities and Disabilities  
    9 Performance Provided, Market Demand, and the Prod uct Life Cycle  
    10 Managing Disruptive Technological Change: A Case  Study  
    11 The Dilemmas of Innovation: A Summary  
   The Innovator’s Dilemma  Book Group Guide  
    About the Author   
 
 
 
 
 
  5  
 
In Gratitude 
 
Although this book lists only one author, in realit y the ideas it molds together were contributed and 
refined by many extraordinarily insightful and self less colleagues. The work began when Professors 
Kim Clark, Joseph Bower, Jay Light, and John McArth ur took the risk of admitting and financing a 
middle-aged man's way into and through the Harvard Business School's doctoral program in 1989. In 
addition to these mentors, Professors Richard Rosen bloom, Howard Stevenson, Dorothy Leonard, 
Richard Walton, Bob Hayes, Steve Wheelwright, and K ent Bowen helped throughout my doctoral 
research to keep my thinking sharp and my standards  for evidence high, and to embed what I was 
learning within the streams of strong scholarship t hat had preceded what I was attempting to research.  
None of these professors needed to spend so much of  their busy lives guiding me as they did, and I wil l 
be forever grateful for what they taught me about t he substance and process of scholarship. 
I am similarly indebted to the many executives and employees of companies in the disk drive industry 
who opened their memories and records to me as I tr ied to understand what had driven them in the 
particular courses they had taken. In particular, J ames Porter, editor of Disk/Trend Report,  opened his 
extraordinary archives of data, enabling me to meas ure what has happened in the disk drive industry 
with a level of completeness and accuracy that coul d be done in few other settings. The model of the 
industry’s evolution and revolution that these men and women helped me construct has formed the 
theoretical backbone for this book. I hope they fin d it to be a useful tool for making sense of their past, 
and a helpful guide for some of their decisions in the future. 
During my tenure on the Harvard Business School fac ulty, other colleagues have helped refine this 
book’s ideas even more. Professors Rebecca Henderso n and James Utterback of MIT, Robert 
Burgelman of Stanford, and David Garvin, Gary Pisan o, and Marco Iansiti of the Harvard Business 
School have been particularly helpful. Research ass ociates Rebecca Voorheis, Greg Rogers, Bret Baird, 
Jeremy Dann, Tara Donovan, and Michael Overdorf; ed itors Marjorie Williams, Steve Prokesch, and 
Barbara Feinberg; and assistants Cheryl Druckenmill er, Meredith Anderson, and Marguerite Dole, have 
likewise contributed untold amounts of data, advice , insight, and work. 
I am grateful to my students, with whom I have disc ussed and refined the ideas put forward in this 
book. On most days I leave class wondering why I ge t paid and why my students pay tuition, given that 
it is I who have learned the most from our interact ions. Every year they leave our school with their 
degrees and scatter around the world, without under standing how much they have taught their teachers. 
I love them and hope that those who come across thi s book will be able to recognize in it the fruits o f 
their puzzled looks, questions, comments, and criti cisms. 
My deepest gratitude is to my family—my wife Christ ine and our children Matthew, Ann, Michael, 
Spencer, and Catherine. With unhesitating faith and  support they encouraged me to pursue my lifelong 
dream to be a teacher, amidst all of the demands of  family life. Doing this research on disruptive 
technologies has indeed been disruptive to them in terms of time and absence from home, and I am 
forever grateful for their love and support. Christ ine, in particular, is the smartest and most patien t 
person I have known. Most of the ideas in this book  went home on some night over the past five years 
in half-baked condition and returned to Harvard the  next morning having been clarified, shaped, and 
edited through my conversations with her. She is a great colleague, supporter, and friend. I dedicate this 
book to her and our children.  6  
Clayton M. Christensen 
Harvard Business School 
Boston, Massachusetts 
April 1997 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  7  
Introduction 
 
This book is about the failure of companies to stay  atop their industries when they confront certain 
types of market and technological change. It’s not about the failure of simply any company, but of 
good  companies—the kinds that many managers have admire d and tried to emulate, the companies 
known for their abilities to innovate and execute. Companies stumble for many reasons, of course, 
among them bureaucracy, arrogance, tired executive blood, poor planning, short-term investment 
horizons, inadequate skills and resources, and just  plain bad luck. But this book is not about compani es 
with such weaknesses: It is about well-managed comp anies that have their competitive antennae up, 
listen astutely to their customers, invest aggressi vely in new technologies, and yet still lose market  
dominance. 
Such seemingly unaccountable failures happen in ind ustries that move fast and in those that move 
slow; in those built on electronics technology and those built on chemical and mechanical technology; 
in manufacturing and in service industries. Sears R oebuck, for example, was regarded for decades as 
one of the most astutely managed retailers in the w orld. At its zenith Sears accounted for more than 2  
percent of all retail sales in the United States. I t pioneered several innovations critical to the suc cess of 
today’s most admired retailers: for example, supply  chain management, store brands, catalogue 
retailing, and credit card sales. The esteem in whi ch Sears’ management was held shows in this 1964 
excerpt from Fortune:  “How did Sears do it? In a way, the most arresting  aspect of its story is that 
there was no gimmick. Sears opened no big bag of tr icks, shot off no skyrockets. Instead, it looked as  
though everybody in its organization simply did the  right thing, easily and naturally. And their 
cumulative effect was to create an extraordinary po werhouse of a company.” 1 
Yet no one speaks about Sears that way today. Someh ow, it completely missed the advent of discount 
retailing and home centers. In the midst of today’s  catalogue retailing boom, Sears has been driven 
from that business. Indeed, the very viability of i ts retailing operations has been questioned. One 
commentator has noted that “Sears’ Merchandise Grou p lost $1.3 billion (in 1992) even before a $1.7 
billion restructuring charge. Sears let arrogance b lind it to basic changes taking place in the Americ an 
marketplace.” 2 Another writer has complained, 
Sears has been a disappointment for investors who h ave watched its stock sink dismally in the face of 
unkept promises of a turnaround. Sears’ old merchan dising approach—a vast, middle-of-the-road array 
of mid-priced goods and services—is no longer compe titive. No question, the constant 
disappointments, the repeated predictions of a turn around that never seems to come, have reduced the 
credibility of Sears’ management in both the financ ial and merchandising communities. 3  
It is striking to note that Sears received its acco lades at exactly the time—in the mid-1960s—when it 
was ignoring the rise of discount retailing and hom e centers, the lower-cost formats for marketing 
name-brand hard goods that ultimately stripped Sear s of its core franchise. Sears was praised as one o f 
the best-managed companies in the world at the very  time it let Visa and MasterCard usurp the 
enormous lead it had established in the use of cred it cards in retailing. 
In some industries this pattern of leadership failu re has been repeated more than once. Consider the 
computer industry. IBM dominated the mainframe mark et but missed by years the emergence of 
minicomputers, which were technologically much simp ler than mainframes. In fact, no other major 
manufacturer of mainframe computers became a signif icant player in the minicomputer business.  8 Digital Equipment Corporation created the minicompu ter market and was joined by a set of other 
aggressively managed companies: Data General, Prime , Wang, Hewlett-Packard, and Nixdorf. But 
each of these companies in turn missed the desktop personal computer market. It was left to Apple 
Computer, together with Commodore, Tandy, and IBM’s  stand-alone PC division, to create the 
personal-computing market. Apple, in particular, wa s uniquely innovative in establishing the standard 
for user-friendly computing. But Apple and IBM lagg ed five years behind the leaders in bringing 
portable computers to market. Similarly, the firms that built the engineering workstation market—
Apollo, Sun, and Silicon Graphics—were all newcomer s to the industry. 
As in retailing, many of these leading computer man ufacturers were at one time regarded as among the 
best-managed companies in the world and were held u p by journalists and scholars of management as 
examples for all to follow. Consider this assessmen t of Digital Equipment, made in 1986: “Taking on 
Digital Equipment Corp. these days is like standing  in front of a moving train. The $7.6 billion 
computer maker has been gathering speed while most rivals are stalled in a slump in the computer 
industry.” 4 The author proceeded to warn IBM to watch out, bec ause it was standing on the tracks. 
Indeed, Digital was one of the most prominently fea tured companies in the McKinsey study that led to 
the book In Search of Excellence.  5  
Yet a few years later, writers characterized DEC qu ite differently: 
Digital Equipment Corporation is a company in need of triage. Sales are drying up in its key 
minicomputer line. A two-year-old restructuring pla n has failed miserably. Forecasting and production 
planning systems have failed miserably. Cost-cuttin g hasn’t come close to restoring profitability. . .  . 
But the real misfortune may be DEC’s lost opportuni ties. It has squandered two years trying halfway 
measures to respond to the low-margin personal comp uters and workstations that have transformed the 
computer industry. 6 
In Digital’s case, as in Sears, the very decisions that led to its decline were made at the time it wa s so 
widely regarded as being an astutely managed firm. It was praised as a paragon of managerial 
excellence at the very time it was ignoring the arr ival of the desktop computers that besieged it a fe w 
years later. 
Sears and Digital are in noteworthy company. Xerox long dominated the market for plain paper 
photocopiers used in large, high-volume copying cen ters. Yet it missed huge growth and profit 
opportunities in the market for small tabletop phot ocopiers, where it became only a minor player. 
Although steel minimills have now captured 40 perce nt of the North American steel market, including 
nearly all of the region’s markets for bars, rods, and structural steel, not a single  integrated steel 
company—American, Asian, or European—had by 1995 bu ilt a plant using minimill technology. Of 
the thirty manufacturers of cable-actuated power sh ovels, only four survived the industry’s twenty-fiv e-
year transition to hydraulic excavation technology.  
As we shall see, the list of leading companies that  failed when confronted with disruptive changes in 
technology and market structure is a long one. At f irst glance, there seems to be no pattern in the 
changes that overtook them. In some cases the new t echnologies swept through quickly; in others, the 
transition took decades. In some, the new technolog ies were complex and expensive to develop. In 
others, the deadly technologies were simple extensi ons of what the leading companies already did 
better than anyone else. One theme common to all of  these failures, however, is that the decisions tha t 
led to failure were made when the leaders in questi on were widely regarded as among the best 
companies in the world.  9 There are two ways to resolve this paradox. One mig ht be to conclude that firms such as Digital, IBM, 
Apple, Sears, Xerox, and Bucyrus Erie must never  have been well managed. Maybe they were 
successful because of good luck and fortuitous timi ng, rather than good management. Maybe they 
finally fell on hard times because their good fortu ne ran out. Maybe. An alternative explanation, 
however, is that these failed firms were as well-ru n as one could expect a firm managed by mortals to 
be—but that there is something about the way decisi ons get made in successful organizations that sows 
the seeds of eventual failure. 
The research reported in this book supports this la tter view: It shows that in the cases of well-manag ed 
firms such as those cited above, good  management was the most powerful reason they faile d to stay 
atop their industries. Precisely because these firms listened to their customers, invested a ggressively in 
new technologies that would provide their customers  more and better products of the sort they wanted, 
and because they carefully studied market trends an d systematically allocated investment capital to 
innovations that promised the best returns, they lo st their positions of leadership. 
What this implies at a deeper level is that many of  what are now widely accepted principles of good 
management are, in fact, only situationally appropr iate. There are times at which it is right not  to listen 
to customers, right to invest in developing lower-p erformance products that promise lower  margins, 
and right to aggressively pursue small, rather than  substantial, markets. This book derives a set of r ules, 
from carefully designed research and analysis of in novative successes and failures in the disk drive a nd 
other industries, that managers can use to judge wh en the widely accepted principles of good 
management should be followed and when alternative principles are appropriate. 
These rules, which I call principles of disruptive innovation,  show that when good companies fail, it 
often has been because their managers either ignore d these principles or chose to fight them. Managers  
can be extraordinarily effective in managing even t he most difficult innovations if they work to 
understand and harness the principles of disruptive  innovation. As in many of life’s most challenging 
endeavors, there is great value in coming to grips with “the way the world works,” and in managing 
innovative efforts in ways that accommodate such fo rces. 
The Innovator’s Dilemma  is intended to help a wide range of managers, cons ultants, and academics in 
manufacturing and service businesses—high tech or l ow—in slowly evolving or rapidly changing 
environments. Given that aim, technology,  as used in this book, means the processes by which  an 
organization transforms labor, capital, materials, and information into products and services of great er 
value. All firms have technologies. A retailer like  Sears employs a particular technology to procure, 
present, sell, and deliver products to its customer s, while a discount warehouse retailer like PriceCo stco 
employs a different technology. This concept of tec hnology therefore extends beyond engineering and 
manufacturing to encompass a range of marketing, in vestment, and managerial processes. Innovation  
refers to a change in one of these technologies. 
 
 
THE DILEMMA   
 
To establish the theoretical depth of the ideas in this book, the breadth of their usefulness, and the ir 
applicability to the future as well as the past, I have divided this book into two parts. Part One, ch apters 
1 through 4, builds a framework that explains why s ound decisions by great managers can lead firms to 
failure. The picture these chapters paint is truly that of an innovator’s dilemma: the logical, compet ent 
decisions of management that are critical to the su ccess of their companies are also the reasons why  10  they lose their positions of leadership. Part Two, chapters 5 through 10, works to resolve the dilemma . 
Building on our understanding of why and under what  circumstances new technologies have caused 
great firms to fail, it prescribes managerial solut ions to the dilemma—how executives can 
simultaneously do what is right for the near-term h ealth of their established businesses, while focusi ng 
adequate resources on the disruptive technologies t hat ultimately could lead to their downfall. 
 
 
Building a Failure Framework   
 
I begin this book by digging deep before extending the discussion to draw general conclusions. The 
first two chapters recount in some detail the histo ry of the disk drive industry, where the saga of “g ood-
companies-hitting-hard-times” has been played out o ver and over again. This industry is an ideal field  
for studying failure because rich data about it exi st and because, in the words of Harvard Business 
School Dean Kim B. Clark, it is “fast history.” In just a few years, market segments, companies, and 
technologies have emerged, matured, and declined. O nly twice in the six times that new architectural 
technologies have emerged in this field has the ind ustry’s dominant firm maintained its lead in the 
subsequent generation. This repetitive pattern of f ailure in the disk drive industry allowed me first to 
develop a preliminary framework that explained why the best and largest firms in the early generations  
of this industry failed and then to test this frame work across subsequent cycles in the industry’s his tory 
to see whether it was robust enough to continue to explain failures among the industry’s more recent 
leaders. 
Chapters 3 and 4 then deepen our understanding of w hy the leading firms stumbled repeatedly in the 
disk drive industry and, simultaneously, test the b readth of the framework’s usefulness by examining 
the failure of firms in industries with very differ ent characteristics. Hence, chapter 3, exploring th e 
mechanical excavator industry, finds that the same factors that precipitated the failure of the leadin g 
disk drive makers also proved to be the undoing of the leading makers of mechanical excavators, in an 
industry that moves with a very different pace and technological intensity. Chapter 4 completes the 
framework and uses it to show why integrated steel companies worldwide have proven so incapable of 
blunting the attacks of the minimill steel makers. 
 
 
WHY GOOD MANAGEMENT CAN LEAD TO FAILURE   
 
The failure framework is built upon three findings from this study. The first is that there is a 
strategically important distinction between what I call sustaining  technologies and those that are 
disruptive.  These concepts are very different from the increme ntal-versus-radical distinction that has 
characterized many studies of this problem. Second,  the pace of technological progress can, and often 
does, outstrip what markets need. This means that t he relevance and competitiveness of different 
technological approaches can change with respect to  different markets over time. And third, customers 
and financial structures of successful companies co lor heavily the sorts of investments that appear to  be 
attractive to them, relative to certain types of en tering firms. 
 
 
Sustaining versus Disruptive Technologies    11   
Most new technologies foster improved product perfo rmance. I call these sustaining technologies.  
Some sustaining technologies can be discontinuous o r radical in character, while others are of an 
incremental nature. What all sustaining technologie s have in common is that they improve the 
performance of established products, along the dime nsions of performance that mainstream customers 
in major markets have historically valued. Most tec hnological advances in a given industry are 
sustaining in character. An important finding revea led in this book is that rarely have even the most 
radically difficult sustaining technologies precipi tated the failure of leading firms. 
Occasionally, however, disruptive technologies emerge: innovations that result in worse  product 
performance, at least in the near-term. Ironically,  in each of the instances studied in this book, it was 
disruptive technology that precipitated the leading  firms’ failure. 
Disruptive technologies bring to a market a very di fferent value proposition than had been available 
previously. Generally, disruptive technologies unde rperform established products in mainstream 
markets. But they have other features that a few fr inge (and generally new) customers value. Products 
based on disruptive technologies are typically chea per, simpler, smaller, and, frequently, more 
convenient to use. There are many examples in addit ion to the personal desktop computer and discount 
retailing examples cited above. Small off-road moto rcycles introduced in North America and Europe 
by Honda, Kawasaki, and Yamaha were disruptive tech nologies relative to the powerful, over-the-road 
cycles made by Harley-Davidson and BMW. Transistors  were disruptive technologies relative to 
vacuum tubes. Health maintenance organizations were  disruptive technologies to conventional health 
insurers. In the near future, “internet appliances”  may become disruptive technologies to suppliers of  
personal computer hardware and software. 
 
 
Trajectories of Market Need versus Technology Impro vement   
 
The second element of the failure framework, the ob servation that technologies can progress faster tha n 
market demand, illustrated in Figure I.1, means tha t in their efforts to provide better products than their 
competitors and earn higher prices and margins, sup pliers often “overshoot” their market: They give 
customers more than they need or ultimately are wil ling to pay for. And more importantly, it means 
that disruptive technologies that may underperform today, relative to what users in the market demand,  
may be fully performance-competitive in that same m arket tomorrow. 
Many who once needed mainframe computers for their data processing requirements, for example, no 
longer need or buy mainframes. Mainframe performanc e has surpassed the requirements of many 
original customers, who today find that much of wha t they need to do can be done on desktop machines 
linked to file servers. In other words, the needs o f many computer users have increased more slowly 
than the rate of improvement provided by computer d esigners. Similarly, many shoppers who in 1965 
felt they had to shop at department stores to be as sured of quality and selection now satisfy those ne eds 
quite well at Target and Wal-Mart. 
 
 
Figure I.1  The Impact of Sustaining and Disruptive Technologi cal Change  
  12  
 
 
 
 
 
Disruptive Technologies versus Rational Investments   
 
The last element of the failure framework, the conc lusion by established companies that investing 
aggressively in disruptive technologies is not a ra tional financial decision for them to make, has thr ee 
bases. First, disruptive products are simpler and c heaper; they generally promise lower margins, not 
greater profits. Second, disruptive technologies ty pically are first commercialized in emerging or 
insignificant markets. And third, leading firms’ mo st profitable customers generally don’t want, and 
indeed initially can’t use, products based on disru ptive technologies. By and large, a disruptive 
technology is initially embraced by the least profi table customers in a market. Hence, most companies 
with a practiced discipline of listening to their b est customers and identifying new products that 
promise greater profitability and growth are rarely  able to build a case for investing in disruptive 
technologies until it is too late. 
 
 
TESTING THE FAILURE FRAMEWORK   
 
This book defines the problem of disruptive technol ogies and describes how they can be managed, 
taking care to establish what researchers call the internal  and external  validity of its propositions. 
Chapters 1 and 2 develop the failure framework in t he context of the disk drive industry, and the init ial 
pages of chapters 4 through 8 return to that indust ry to build a progressively deeper understanding of  
why disruptive technologies are such vexatious phen omena for good managers to confront 
successfully. The reason for painting such a comple te picture of a single industry is to establish the  
internal validity of the failure framework. If a fr amework or model cannot reliably explain what 
happened within a single industry, it cannot be app lied to other situations with confidence. 
Chapter 3 and the latter sections of chapters 4 thr ough 9 are structured to explore the external valid ity 
of the failure framework—the conditions in which we  might expect the framework to yield useful 
insights. Chapter 3 uses the framework to examine w hy the leading makers of cable excavators were 
driven from the earthmoving market by makers of hyd raulic machines, and chapter 4 discusses why the 
world’s integrated steel makers have floundered in the face of minimill technology. Chapter 5 uses the  
model to examine the success of discount retailers,  relative to conventional chain and department 
stores, and to probe the impact of disruptive techn ologies in the motor control and printer industries .  13  Chapter 6 examines the emerging personal digital as sistant industry and reviews how the electric motor  
control industry was upended by disruptive technolo gy. Chapter 7 recounts how entrants using 
disruptive technologies in motorcycles and logic ci rcuitry dethroned industry leaders; chapter 8 shows  
how and why computer makers fell victim to disrupti on; and chapter 9 spotlights the same phenomena 
in the accounting software and insulin businesses. Chapter 10 applies the framework to a case study of  
the electric vehicle, summarizing the lessons learn ed from the other industry studies, showing how the y 
can be used to assess the opportunity and threat of  electric vehicles, and describing how they might b e 
applied to make an electric vehicle commercially su ccessful. Chapter 11 summarizes the book’s 
findings. 
Taken in sum, these chapters present a theoreticall y strong, broadly valid, and managerially practical  
framework for understanding disruptive technologies  and how they have precipitated the fall from 
industry leadership of some of history’s best-manag ed companies. 
 
 
HARNESSING THE PRINCIPLES OF DISRUPTIVE INNOVATION   
 
Colleagues who have read my academic papers reporti ng the findings recounted in chapters 1 through 4 
were struck by their near-fatalism. If good managem ent practice drives the failure of successful firms  
faced with disruptive technological change, then th e usual answers to companies’ problems—planning 
better, working harder, becoming more customer-driv en, and taking a longer-term perspective—all 
exacerbate  the problem. Sound execution, speed-to-market, tot al quality management, and process 
reengineering are similarly ineffective. Needless t o say, this is disquieting news to people who teach  
future managers! 
Chapters 5 through 10, however, suggest that althou gh the solution to disruptive technologies cannot b e 
found in the standard tool kit of good management, there are, in fact, sensible ways to deal effective ly 
with this challenge. Every company in every industr y works under certain forces—laws of 
organizational nature—that act powerfully to define  what that company can and cannot do. Managers 
faced with disruptive technologies fail their compa nies when these forces overpower them. 
By analogy, the ancients who attempted to fly by st rapping feathered wings to their arms and flapping 
with all their might as they leapt from high places  invariably failed. Despite their dreams and hard 
work, they were fighting against some very powerful  forces of nature. No one could be strong enough 
to win this fight. Flight became possible only afte r people came to understand the relevant natural la ws 
and principles that defined how the world worked: t he law of gravity, Bernoulli’s principle, and the 
concepts of lift, drag, and resistance. When people  then designed flying systems that recognized or 
harnessed the power of these laws and principles, r ather than fighting them, they were finally able to  
fly to heights and distances that were previously u nimaginable. 
The objective of chapters 5 through 10 is to propos e the existence of five laws or principles of 
disruptive technology. As in the analogy with manne d flight, these laws are so strong that managers 
who ignore or fight them are nearly powerless to pi lot their companies through a disruptive technology  
storm. These chapters show, however, that if manage rs can understand and harness these forces, rather 
than fight them, they can in fact succeed spectacul arly when confronted with disruptive technological 
change. I am particularly anxious that managers rea d these chapters for understanding, rather than for 
simple answers. I am very confident that the great managers about whom this book is written will be 
very capable on their own of finding the answers th at best fit their circumstances. But they must firs t  14  understand what has caused those circumstances and what forces will affect the feasibility of their 
solutions. The following paragraphs summarize these  principles and what managers can do to harness 
or accommodate them. 
 
 
Principle #1: Companies Depend on Customers and Inv estors for Resources   
 
The history of the disk drive industry shows that t he established firms stayed atop wave after wave of  
sustaining technologies (technologies that their cu stomers needed), while consistently stumbling over 
simpler disruptive ones. This evidence supports the  theory of resource dependence.  7 Chapter 5 
summarizes this theory, which states that while man agers may think  they control the flow of resources 
in their firms, in the end it is really customers a nd investors who dictate how money will be spent 
because companies with investment patterns that don ’t satisfy their customers and investors don’t 
survive. The highest-performing companies, in fact,  are those that are the best at this, that is, they  have 
well-developed systems for killing ideas that their  customers don’t want. As a result, these companies  
find it very difficult to invest adequate resources  in disruptive technologies—lower-margin 
opportunities that their customers don’t want—until  their customers want them. And by then it is too 
late. 
Chapter 5 suggests a way for managers to align or h arness this law with their efforts to confront 
disruptive technology. With few exceptions, the onl y instances in which mainstream firms have 
successfully established a timely position in a dis ruptive technology were those in which the firms’ 
managers set up an autonomous organization charged with building a new and independent business 
around the disruptive technology. Such organization s, free of the power of the customers of the 
mainstream company, ensconce themselves among a dif ferent set of customers—those who want  the 
products of the disruptive technology. In other wor ds, companies can succeed in disruptive 
technologies when their managers align their organi zations with  the forces of resource dependence, 
rather than ignoring or fighting them. 
The implication of this principle for managers is t hat, when faced with a threatening disruptive 
technology, people and processes in a mainstream or ganization cannot be expected to allocate freely 
the critical financial and human resources needed t o carve out a strong position in the small, emergin g 
market. It is very difficult for a company whose co st structure is tailored to compete in high-end 
markets to be profitable in low-end markets as well . Creating an independent organization, with a cost  
structure honed to achieve profitability at the low  margins characteristic of most disruptive 
technologies, is the only viable way for establishe d firms to harness this principle. 
 
 
Principle #2: Small Markets Don’t Solve the Growth Needs of Large Companies   
 
Disruptive technologies typically enable new market s to emerge. There is strong evidence showing that 
companies entering these emerging markets early hav e significant first-mover advantages over later 
entrants. And yet, as these companies succeed and g row larger, it becomes progressively more difficult  
for them to enter the even newer small markets dest ined to become the large ones of the future.  15  To maintain their share prices and create internal opportunities for employees to extend the scope of 
their responsibilities, successful companies need t o continue to grow. But while a $40 million company  
needs to find just $8 million in revenues to grow a t 20 percent in the subsequent year, a $4 billion 
company needs to find $800 million in new sales. No  new markets are that large. As a consequence, the 
larger and more successful an organization becomes,  the weaker the argument that emerging markets 
can remain useful engines for growth. 
Many large companies adopt a strategy of waiting un til new markets are “large enough to be 
interesting.” But the evidence presented in chapter  6 shows why this is not often a successful strateg y. 
Those large established firms that have successfull y seized strong positions in the new markets enable d 
by disruptive technologies have done so by giving r esponsibility to commercialize the disruptive 
technology to an organization whose size matched th e size of the targeted market. Small organizations 
can most easily respond to the opportunities for gr owth in a small market. The evidence is strong that  
formal and informal resource allocation processes m ake it very difficult for large organizations to fo cus 
adequate energy and talent on small markets, even w hen logic says they might be big someday. 
 
 
Principle #3: Markets that Don’t Exist Can’t Be Ana lyzed   
 
Sound market research and good planning followed by  execution according to plan are hallmarks of 
good management. When applied to sustaining technol ogical innovation, these practices are invaluable; 
they are the primary reason, in fact, why establish ed firms led in every single instance of sustaining  
innovation in the history of the disk drive industr y. Such reasoned approaches are feasible in dealing  
with sustaining technology because the size and gro wth rates of the markets are generally known, 
trajectories of technological progress have been es tablished, and the needs of leading customers have 
usually been well articulated. Because the vast maj ority of innovations are sustaining in character, m ost 
executives have learned to manage innovation in a s ustaining context, where analysis and planning 
were feasible. 
In dealing with disruptive technologies leading to new markets, however, market researchers and 
business planners have consistently dismal records.  In fact, based upon the evidence from the disk 
drive, motorcycle, and microprocessor industries, r eviewed in chapter 7, the only thing we may know 
for sure when we read experts’ forecasts about how large emerging markets will become is that they 
are wrong. 
In many instances, leadership in sustaining innovat ions—about which information is known and for 
which plans can be made—is not competitively import ant. In such cases, technology followers do 
about as well as technology leaders. It is in disru ptive innovations, where we know least about the 
market, that there are such strong first-mover adva ntages. This is the innovator’s dilemma. 
Companies whose investment processes demand quantif ication of market sizes and financial returns 
before they can enter a market get paralyzed or mak e serious mistakes when faced with disruptive 
technologies. They demand market data when none exi sts and make judgments based upon financial 
projections when neither revenues or costs can, in fact, be known. Using planning and marketing 
techniques that were developed to manage sustaining  technologies in the very different context of 
disruptive ones is an exercise in flapping wings.  16  Chapter 7 discusses a different approach to strateg y and planning that recognizes the law that the rig ht 
markets, and the right strategy for exploiting them , cannot be known in advance. Called discovery-
based planning, it suggests that managers assume th at forecasts are wrong, rather than right, and that  
the strategy they have chosen to pursue may likewis e be wrong. Investing and managing under such 
assumptions drives managers to develop plans for le arning what needs to be known, a much more 
effective way to confront disruptive technologies s uccessfully. 
 
 
Principle #4: An Organization’s Capabilities Define  Its Disabilities   
 
When managers tackle an innovation problem, they in stinctively work to assign capable people to the 
job. But once they’ve found the right people, too m any managers then assume that the organization in 
which they’ll work will also be capable of succeedi ng at the task. And that is dangerous—because 
organizations have capabilities that exist independ ently of the people who work within them. An 
organization’s capabilities reside in two places. T he first is in its processes—the methods by which 
people have learned to transform inputs of labor, e nergy, materials, information, cash, and technology  
into outputs of higher value. The second is in the organization’s values, which are the criteria that 
managers and employees in the organization use when  making prioritization decisions. People are quite 
flexible, in that they can be trained to succeed at  quite different things. An employee of IBM, for 
example, can quite readily change the way he or she  works, in order to work successfully in a small 
start-up company. But processes and values are not flexible. A process that is effective at managing t he 
design of a minicomputer, for example, would be ine ffective at managing the design of a desktop 
personal computer. Similarly, values that cause emp loyees to prioritize projects to develop high-margi n 
products, cannot simultaneously accord priority to low-margin products. The very processes and values 
that constitute an organization’s capabilities in o ne context, define its dis abilities in another context. 
Chapter 8 will present a framework that can help a manager understand precisely where in his or her 
organization its capabilities and disabilities resi de. Drawing on studies in the disk drive and comput er 
industries, it offers tools that managers can use t o create new capabilities, when the processes and 
values of the present organization would render it incapable of successfully addressing a new problem.  
 
 
Principle #5: Technology Supply May Not Equal Marke t Demand   
 
Disruptive technologies, though they initially can only be used in small markets remote from the 
mainstream, are disruptive because they subsequentl y can become fully performance-competitive 
within the mainstream market against established pr oducts. As depicted in Figure I.1 , this happens 
because the pace of technological progress in produ cts frequently exceeds the rate of performance 
improvement that mainstream customers demand or can  absorb. As a consequence, products whose 
features and functionality closely match market nee ds today often follow a trajectory of improvement 
by which they overshoot mainstream market needs tom orrow. And products that seriously 
underperform today, relative to customer expectatio ns in mainstream markets, may become directly 
performance-competitive tomorrow. 
Chapter 9 shows that when this happens, in markets as diverse as disk drives, accounting software, and  
diabetes care, the basis of competition—the criteri a by which customers choose one product over  17  another—changes. When the performance of two or mor e competing products has improved beyond 
what the market demands, customers can no longer ba se their choice upon which is the higher 
performing product. The basis of product choice oft en evolves from functionality to reliability, then to 
convenience, and, ultimately, to price. 
Many students of business have described phases of the product life cycle in various ways. But chapter  
9 proposes that the phenomenon in which product per formance overshoots market demands is the 
primary mechanism driving shifts in the phases of t he product life cycle. 
In their efforts to stay ahead by developing compet itively superior products, many companies don’t 
realize the speed at which they are moving up-marke t, over-satisfying the needs of their original 
customers as they race the competition toward highe r-performance, higher-margin markets. In doing 
so, they create a vacuum at lower price points into  which competitors employing disruptive 
technologies can enter. Only those companies that c arefully measure trends in how their mainstream 
customers use  their products can catch the points at which the b asis of competition will change in the 
markets they serve. 
 
 
LESSONS FOR SPOTTING DISRUPTIVE THREATS AND OPPORTU NITIES   
 
Some managers and researchers familiar with these i deas have arrived at this point in the story in an 
anxious state because the evidence is very strong t hat even the best managers have stumbled badly 
when their markets were invaded by disruptive techn ologies. Most urgently, they want to know 
whether their own businesses are targets for an att acking disruptive technologist and how they can 
defend their business against such an attack before  it is too late. Others, interested in finding 
entrepreneurial opportunities, wonder how they can identify potentially disruptive technologies around  
which new companies and markets can be built. 
Chapter 10 addresses these questions in a rather un conventional way. Rather than offering a checklist 
of questions to ask or analyses to perform, it crea tes a case study of a particularly vexing but well-
known problem in technological innovation: the elec tric vehicle. Positioning myself in the role of 
protagonist—as the program manager responsible for electric vehicle development in a major 
automobile manufacturing company wrestling with the  mandate of the California Air Resources Board 
to begin selling electric vehicles in that state—I explore the question of whether electric vehicles a re in 
fact a disruptive technology and then suggest ways to organize this program, set its strategy, and 
manage it to succeed. In the spirit of all case stu dies, the purpose of this chapter is not  to advance what 
I believe to be the correct answer to this innovato r’s challenge. Rather, it suggests a methodology an d a 
way of thinking about the problem of managing disru ptive technological change that should prove 
useful in many other contexts. 
Chapter 10 thus takes us deeply into the innovator’ s dilemma that “good” companies often begin their 
descent into failure by aggressively investing in t he products and services that their most profitable  
customers want. No automotive company is currently threatened by electric cars, and none 
contemplates a wholesale leap into that arena. The automobile industry is healthy. Gasoline engines 
have never been more reliable. Never before has suc h high performance and quality been available at 
such low prices. Indeed, aside from governmental ma ndates, there is no reason why we should expect 
the established car makers to pursue electric vehic les.  18   
  
 
 
 
 
  
But the electric car is  a disruptive technology and potential future threa t. The innovator’s task is to 
ensure that this innovation—the disruptive technolo gy that doesn’t make sense—is taken seriously 
within the company without putting at risk the need s of present customers who provide profit and 
growth. As chapter 10 concretely lays out, the prob lem can be resolved only when new markets are 
considered and carefully developed around new defin itions of value—and when responsibility for 
building the business is placed within a focused or ganization whose size and interest are carefully 
aligned with the unique needs of the market’s custo mers. 
 
 
WHERE DISRUPTIONS ARE HAPPENING TODAY   
 
One of the most gratifying aspects of my life since  the first edition of The Innovator’s Dilemma  was  19  published has been the number of people who have ca lled, representing industries that I had never 
thought about, who have suggested that forces simil ar to those historical examples I described in thes e 
pages are disrupting their industries as well. Some  of these are described in the accompanying table. 
Not surprisingly, the Internet looms as an infrastr uctural technology that is enabling the disruption of 
many industries. 
Each of the innovations in the right column—in the form of a new technology or a new business 
model—is now in the process of disrupting the estab lished order described in the left column. Will the  
companies that currently lead their industries usin g the technologies in the left column survive these  
attacks? My hope is that the future might be differ ent than the past. I believe that the future can  be 
different, if managers will recognize these disrupt ions for what they are, and address them in a way t hat 
accounts for or harnesses the fundamental principle s described in the pages that follow. 
 
 
NOTES   
 
1.  John McDonald, “Sears Makes It Look Easy,” Fortune , May, 1964, 120-121.  
2.  Zina Moukheiber, “Our Competitive Advantage,” Forb es, April 12, 1993, 59.  
3.  Steve Weiner, “It’s Not Over Until It’s Over,” For bes, May 28, 1990, 58.  
4.  Business Week, March 24, 1986, 98.  
5. Thomas J. Peters and Robert H. Waterman, In Search  of Excellence (New York: Harper & Row, 
1982).  
6.  Business Week, May 9, 1994, 26.  
7.  Jeffrey Pfeffer and Gerald R. Salancik, The Extern al Control of Organizations: A Resource 
Dependence Perspective (New York: Harper & Row, 197 8).  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  20  Part One 
WHY GREAT COMPANIES  
CAN FAIL 
 
CHAPTER ONE  
How Can Great Firms Fail?  
Insights from the Hard Disk Drive Industry 
 
 
When I began my search for an answer to the puzzle of why the best firms can fail, a friend offered 
some sage advice. “Those who study genetics avoid s tudying humans,” he noted. “Because new 
generations come along only every thirty years or s o, it takes a long time to understand the cause and  
effect of any changes. Instead, they study fruit fl ies, because they are conceived, born, mature, and die 
all within a single day. If you want to understand why something happens in business, study the disk 
drive industry. Those companies are the closest thi ngs to fruit flies that the business world will eve r 
see.”  
Indeed, nowhere in the history of business has ther e been an industry like disk drives, where changes in 
technology, market structure, global scope, and ver tical integration have been so pervasive, rapid, an d 
unrelenting. While this pace and complexity might b e a nightmare for managers, my friend was right 
about its being fertile ground for research. Few in dustries offer researchers the same opportunities f or 
developing theories about how different types of ch ange cause certain types of firms to succeed or fai l 
or for testing those theories as the industry repea ts its cycles of change. 
This chapter summarizes the history of the disk dri ve industry in all its complexity. Some readers wil l 
be interested in it for the sake of history itself.1 But the value of understanding this history is tha t out of 
its complexity emerge a few stunningly simple and c onsistent factors that have repeatedly determined 
the success and failure of the industry’s best firm s. Simply put, when the best firms succeeded, they did 
so because they listened responsively to their cust omers and invested aggressively in the technology, 
products, and manufacturing capabilities that satis fied their customers’ next-generation needs. But, 
paradoxically, when the best firms subsequently fai led, it was for the same reasons—they listened 
responsively to their customers and invested aggres sively in the technology, products, and 
manufacturing capabilities that satisfied their cus tomers’ next-generation needs. This is one of the 
innovator’s dilemmas: Blindly following the maxim t hat good managers should keep close to their 
customers can sometimes be a fatal mistake. 
The history of the disk drive industry provides a f ramework for understanding when “keeping close to 
your customers” is good advice—and when it is not. The robustness of this framework could only be 
explored by researching the industry’s history in c areful detail. Some of that detail is recounted her e, 
and elsewhere in this book, in the hope that reader s who are immersed in the detail of their own 
industries will be better able to recognize how sim ilar patterns have affected their own fortunes and 
those of their competitors.  21   
 
HOW DISK DRIVES WORK   
 
Disk drives write and read information that compute rs use. They comprise read-write heads mounted at 
the end of an arm that swings over the surface of a  rotating disk in much the same way that a 
phonograph needle and arm reach over a record; alum inum or glass disks coated with magnetic 
material; at least two electric motors, a spin moto r that drives the rotation of the disks and an actu ator 
motor that moves the head to the desired position o ver the disk; and a variety of electronic circuits that 
control the drive’s operation and its interface wit h the computer. See Figure 1.1 for an illustration of a 
typical disk drive. 
 
  
Figure 1.1 Primary Components of a Typical Disk Drive  
 
 
 
The read-write head is a tiny electromagnet whose p olarity changes whenever the direction of the 
electrical current running through it changes. Beca use opposite magnetic poles attract, when the 
polarity of the head becomes positive, the polarity  of the area on the disk beneath the head switches to 
negative, and vice versa. By rapidly changing the d irection of current flowing through the head’s 
electromagnet as the disk spins beneath the head, a  sequence of positively and negatively oriented 
magnetic domains are created in concentric tracks o n the disk’s surface. Disk drives can use the 
positive and negative domains on the disk as a bina ry numeric system— 1 and 0—to “write” 
information onto disks. Drives read information fro m disks in essentially the opposite process: Change s 
in the magnetic flux fields on the disk surface ind uce changes in the micro current flowing through th e 
head. 
 
 
EMERGENCE OF THE EARLIEST DISK DRIVES   
 
A team of researchers at IBM’s San Jose research la boratories developed the first disk drive between 
1952 and 1956. Named RAMAC (for Random Access Metho d for Accounting and Control), this drive  22  was the size of a large refrigerator, incorporated fifty twenty-four-inch disks, and could store 5 
megabytes (MB) of information (see Figure 1.2). Mos t of the fundamental architectural concepts and 
component technologies that defined today’s dominan t disk drive design were also developed at IBM. 
These include its removable packs of rigid disks (i ntroduced in 1961); the floppy disk drive (1971); a nd 
the Winchester architecture (1973). All had a power ful, defining influence on the way engineers in the  
rest of the industry defined what disk drives were and what they could do. 
 
  
Figure 1.2 The First Disk Drive, Developed by IBM  
 
 
 
Source:  Courtesy of International Business Machines Corpor ation. 
 
  
As IBM produced drives to meet its own needs, an in dependent disk drive industry emerged serving 
two distinct markets. A few firms developed the plu g-compatible market (PCM) in the 1960s, selling 
souped-up copies of IBM drives directly to IBM cust omers at discount prices. Although most of IBM’s 
competitors in computers (for example, Control Data , Burroughs, and Univac) were integrated 
vertically into the manufacture of their own disk d rives, the emergence in the 1970s of smaller, 
nonintegrated computer makers such as Nixdorf, Wang , and Prime spawned an original equipment 
market (OEM) for disk drives as well. By 1976 about  $1 billion worth of disk drives were produced, of 
which captive production accounted for 50 percent a nd PCM and OEM for about 25 percent each.  
The next dozen years unfolded a remarkable story of  rapid growth, market turbulence, and technology-
driven performance improvements. The value of drive s produced rose to about $18 billion by 1995. By 
the mid-1980s the PCM market had become insignifica nt, while OEM output grew to represent about 
three-fourths of world production. Of the seventeen  firms populating the industry in 1976—all of 
which were relatively large, diversified corporatio ns such as Diablo, Ampex, Memorex, EMM, and 
Control Data—all except IBM’s disk drive operation had failed or had been acquired by 1995. During 
this period an additional 129 firms entered the ind ustry, and 109 of those also failed. Aside from IBM , 
Fujitsu, Hitachi, and NEC, all of the producers rem aining by 1996 had entered the industry as start-up s 
after 1976.  23  Some have attributed the high mortality rate among the integrated firms that created the industry to i ts 
nearly unfathomable pace of technological change. I ndeed, the pace of change has been breathtaking. 
The number of megabits (Mb) of information that the  industry’s engineers have been able to pack into 
a square inch of disk surface has increased by 35 p ercent per year, on average, from 50 Kb in 1967 to 
1.7 Mb in 1973, 12 Mb in 1981, and 1100 Mb by 1995.  The physical size of the drives was reduced at a 
similar pace: The smallest available 20 MB drive sh rank from 800 cubic inches (in. 3) in 1978 to 1.4 in. 3 
by 1993—a 35 percent annual rate of reduction. 
Figure 1.3 shows that the slope of the industry’s e xperience curve (which correlates the cumulative 
number of terabytes (one thousand gigabytes) of dis k storage capacity shipped in the industry’s histor y 
to the constant-dollar price per megabyte of memory ) was 53 percent—meaning that with each 
doubling of cumulative terabytes shipped, cost per megabyte fell to 53 percent of its former level. Th is 
is a much steeper rate of price decline than the 70  percent slope observed in the markets for most oth er 
microelectronics products. The price per megabyte h as declined at about 5 percent per quarter  for more 
than twenty years. 
 
 
THE IMPACT OF TECHNOLOGICAL CHANGE   
 
My investigation into why leading firms found it so  difficult to stay atop the disk drive industry led  me 
to develop the “technology mudslide hypothesis”: Co ping with the relentless onslaught of technology 
change was akin to trying to climb a mudslide ragin g down a hill. You have to scramble with 
everything you’ve got to stay on top of it, and if you ever once stop to catch your breath, you get 
buried. 
 
  
Figure 1.3 Disk Drive Price Experience Curve  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
   24  To test this hypothesis, I assembled and analyzed a  database consisting of the technical and 
performance specifications of every model of disk d rive introduced by every company in the world 
disk drive industry for each of the years between 1 975 and 1994. 2 This database enabled me to identify 
the firms that led in introducing each new technolo gy; to trace how new technologies were diffused 
through the industry over time; to see which firms led and which lagged; and to measure the impact 
each technological innovation had on capacity, spee d, and other parameters of disk drive performance. 
By carefully reconstructing the history of each tec hnological change in the industry, the changes that  
catapulted entrants to success or that precipitated  the failure of established leaders could be identi fied. 
This study led me to a very different view of techn ology change than the work of prior scholars on thi s 
question had led me to expect. Essentially, it reve aled that neither the pace nor the difficulty of 
technological change lay at the root of the leading  firms’ failures. The technology mudslide hypothesi s 
was wrong. 
The manufacturers of most products have established  a trajectory of performance improvement over 
time. 3 Intel, for example, pushed the speed of its microp rocessors ahead by about 20 percent per year, 
from its 8 megahertz (MHz) 8088 processor in 1979 t o its 133 MHz Pentium chip in 1994. Eli Lilly and 
Company improved the purity of its insulin from 50, 000 impure parts per million (ppm) in 1925 to 10 
ppm in 1980, a 14 percent annual rate of improvemen t. When a measurable trajectory of improvement 
has been established, determining whether a new tec hnology is likely to improve a product’s 
performance relative to earlier products is an unam biguous question. 
But in other cases, the impact of technological cha nge is quite different. For instance, is a notebook  
computer better than a mainframe? This is an ambigu ous question because the notebook computer 
established a completely new performance trajectory , with a definition of performance that differs 
substantially from the way mainframe performance is  measured. Notebooks, as a consequence, are 
generally sold for very different uses. 
This study of technological change over the history  of the disk drive industry revealed two types of 
technology change, each with very different effects  on the industry’s leaders. Technologies of the fir st 
sort sustained  the industry’s rate of improvement in product perf ormance (total capacity and recording 
density were the two most common measures) and rang ed in difficulty from incremental to radical. The 
industry’s dominant firms always led in developing and adopting these technologies. By contrast, 
innovations of the second sort disrupted  or redefined performance trajectories—and consiste ntly 
resulted in the failure of the industry’s leading f irms. 4 
 
The remainder of this chapter illustrates the disti nction between sustaining and disruptive technologi es 
by describing prominent examples of each and summar izing the role these played in the industry’s 
development. This discussion focuses on differences  in how established firms came to lead or lag in 
developing and adopting new technologies, compared with entrant firms. To arrive at these examples, 
each new technology in the industry was examined. I n analyzing which firms led and lagged at each of 
these points of change, I defined established firms to be those that had been established in the indust ry 
prior to the advent of the technology in question, practicing the prior technology. I defined entrant 
firms as those that were new to the industry at that poin t of technology change. Hence, a given firm 
would be considered an entrant at one specific poin t in the industry’s history, for example, at the 
emergence of the 8-inch drive. Yet the same firm wo uld be considered an established firm when 
technologies that emerged subsequent to the firm’s entry were studied.  25   
 
SUSTAINING TECHNOLOGICAL CHANGES   
 
In the history of the disk drive industry, most tec hnology changes have sustained or reinforced 
established trajectories of product performance imp rovement. Figure 1.4, which compares the average 
recording density of drives that employed successiv e generations of head and disk technologies, maps 
an example of this. The first curve plots the densi ty of drives that used conventional particulate oxi de 
disk technology and ferrite head technology; the se cond charts the average density of drives that used  
new-technology thin-film heads and disks; the third  marks the improvements in density achievable with 
the latest head technology, magneto-resistive heads . 5  
 
  
Figure 1.4 Impact of New Read-Write Head Technologies in Susta ining the Trajectory of Improvement 
in Recording Density  
 
 
 
Source:  Data are from various issues of Disk/Trend Report.   
 
  
The way such new technologies as these emerge to su rpass the performance of the old resembles a 
series of intersecting technology S-curves. 6 Movement along a given S-curve is generally the re sult of 
incremental improvements within an existing technol ogical approach, whereas jumping onto the next 
technology curve implies adopting a radically new t echnology. In the cases measured in Figure 1.4, 
incremental advances, such as grinding the ferrite heads to finer, more precise dimensions and using 
smaller and more finely dispersed oxide particles o n the disk’s surface, led to the improvements in 
density from 1 to 20 megabits per square inch (Mbps i) between 1976 and 1989. As S-curve theory 
would predict, the improvement in recording density  obtainable with ferrite/oxide technology began to 
level off toward the end of the period, suggesting a maturing technology. The thin-film head and disk 
technologies’ effect on the industry sustained perf ormance improvement at its historical rate. Thin-fi lm 
heads were barely established in the early 1990s, w hen even more advanced magneto-resistive head 
technology emerged. The impact of magneto-resistive  technology sustained, or even accelerated, the 
rate of performance improvement.  26  Figure 1.5 describes a sustaining technological cha nge of a very different character: an innovation in  
product architecture, in which the 14-inch Winchest er drive is substituted for removable disk packs, 
which had been the dominant design between 1962 and  1978. Just as in the thin-film for ferrite/oxide 
substitution, the impact of Winchester technology s ustained the historically established rate of 
performance improvement. Similar graphs could be co nstructed for most other technological 
innovations in the industry, such as embedded servo  systems, RLL and PRML recording codes, higher 
RPM motors, and embedded interfaces. Some of these were straightforward technology improvements; 
others were radical departures. But all had a simil ar impact on the industry: They helped manufacturer s 
to sustain the rate of historical performance impro vement that their customers had come to expect. 7 
 
In literally every case of sustaining technology ch ange in the disk drive industry, established firms led 
in development and commercialization. The emergence  of new disk and head technologies illustrates 
this.  
In the 1970s, some manufacturers sensed that they w ere reaching the limit on the number of bits of 
information they could pack onto oxide disks. In re sponse, disk drive manufacturers began studying 
ways of applying super-thin films of magnetic metal  on aluminum to sustain the historical rate of 
improvements in recording density. The use of thin- film coatings was then highly developed in the 
integrated circuit industry, but its application to  magnetic disks still presented substantial challen ges. 
Experts estimate that the pioneers of thin-film dis k technology—IBM, Control Data, Digital 
Equipment, Storage Technology, and Ampex—each took more than eight years and spent more than 
$50 million in that effort. Between 1984 and 1986, about two-thirds of the producers active in 1984 
introduced drives with thin-film disks. The overwhe lming majority of these were established industry 
incumbents. Only a few entrant firms attempted to u se thin-film disks in their initial products, and m ost 
of those folded shortly after entry. 
 
  
Figure 1.5  Sustaining Impact of the Winchester Architecture o n the Recording Density of 14-inch Disk 
Drives  
 
 
 
Source:  Data are from various issues of Disk/Trend Report.    27   
  
The same pattern was apparent in the emergence of t hin-film heads. Manufacturers of ferrite heads saw 
as early as 1965 the approaching limit to improveme nts in this technology; by 1981 many believed that 
the limits of precision would soon be reached. Rese archers turned to thin-film technology, produced by  
sputtering thin films of metal on the recording hea d and then using photolithography to etch much fine r 
electromagnets than could be attained with ferrite technology. Again, this proved extraordinarily 
difficult. Burroughs in 1976, IBM in 1979, and othe r established firms first successfully incorporated  
thin-film heads in disk drives. In the period betwe en 1982 and 1986, during which some sixty firms 
entered the rigid disk drive industry, only four (a ll commercial failures) attempted to do so using th in-
film heads in their initial products as a source of  performance advantage. All other entrant firms—eve n 
aggressively performance-oriented firms such as Max tor and Conner Peripherals—found it preferable 
to learn their way using conventional ferrite heads  first, before tackling thin-film technology.  
As was the case with thin-film disks, the introduct ion of thin-film heads entailed the sort of sustain ed 
investment that only established firms could handle . IBM and its rivals each spent more than $100 
million developing thin-film heads. The pattern was  repeated in the next-generation magneto-resistive 
head technology: The industry’s largest firms—IBM, Seagate, and Quantum—led the race. 
The established firms were the leading innovators n ot just in developing risky, complex, and expensive  
component technologies such as thin-film heads and disks, but in literally every other one of the 
sustaining innovations in the industry’s history.  Even in relatively simple innovations, such as RLL  
recording codes (which took the industry from doubl e- to triple-density disks), established firms were  
the successful pioneers, and entrant firms were the  technology followers. This was also true for those  
architectural innovations—for example, 14-inch and 2.5-inch Winchester drives—whose impact was to 
sustain established improvement trajectories. Estab lished firms beat out the entrants. 
Figure 1.6 summarizes this pattern of technology le adership among established and entrant firms 
offering products based on new sustaining technolog ies during the years when those technologies were 
emerging. The pattern is stunningly consistent. Whe ther the technology was radical or incremental, 
expensive or cheap, software or hardware, component  or architecture, competence-enhancing or 
competence-destroying, the pattern was the same. Wh en faced with sustaining technology change that 
gave existing customers something more and better i n what they wanted, the leading practitioners of 
the prior technology led the industry in the develo pment and adoption of the new. Clearly, the leaders  
in this industry did not fail because they became p assive, arrogant, or risk-averse or because they 
couldn’t keep up with the stunning rate of technolo gical change. My technology mudslide hypothesis 
wasn’t correct. 
 
  
Figure 1.6 Leadership of Established Firms in Susta ining Technologies  
  28  
 
 
Source:  Data are from various issues of Disk/Trend Report.   
 
FAILURE IN THE FACE OF DISRUPTIVE TECHNOLOGICAL CHA NGES   
 
Most technological change in the disk drive industr y has consisted of sustaining innovations of the so rt 
described above. In contrast, there have been only a few of the other sort of technological change, 
called disruptive technologies. These were the chan ges that toppled the industry’s leaders. 
The most important disruptive technologies were the  architectural innovations that shrunk the size of 
the drives—from 14-inch diameter disks to diameters  of 8, 5.25, and 3.5-inches and then from 2.5 to 
1.8 inches. Table 1.1 illustrates the ways these in novations were disruptive. Based on 1981 data, it 
compares the attributes of a typical 5.25-inch driv e, a new architecture that had been in the market f or 
less than a year, with those of a typical 8-inch dr ive, which at that time was the standard drive used  by 
minicomputer manufacturers. Along the dimensions of  performance important to established 
minicomputer manufacturers—capacity, cost per megab yte, and access time—the 8-inch product was 
vastly superior. The 5.25-inch architecture did not  address the perceived needs of minicomputer 
manufacturers at that time. On the other hand, the 5.25-inch drive had features that appealed to the 
desktop personal computer market segment just emerg ing in the period between 1980 and 1982. It was 
small and lightweight, and, priced at around $2,000 , it could be incorporated into desktop machines 
economically. 
Generally disruptive innovations were technological ly straightforward, consisting of off-the-shelf 
components put together in a product architecture t hat was often simpler than prior approaches. 8 They 
offered less of what customers in established marke ts wanted and so could rarely be initially employed  
there. They offered a different package of attribut es valued only in emerging markets remote from, and  
unimportant to, the mainstream.  29  The trajectory map in Figure 1.7 shows how this ser ies of simple but disruptive technologies proved to  
be the undoing of some very aggressive, astutely ma naged disk drive companies. Until the mid-1970s, 
14-inch drives with removable packs of disks accoun ted for nearly all disk drive sales. The 14-inch 
Winchester architecture then emerged to sustain the  trajectory of recording density improvement. 
Nearly all of these drives (removable disks and Win chesters) were sold to mainframe computer 
manufacturers, and the same companies that led the market in disk pack drives led the industry‘s 
transition to the Winchester technology. 
The trajectory map shows that the hard disk capacit y provided in the median priced, typically 
configured mainframe computer system in 1974 was ab out 130 MB per computer. This increased at a 
15 percent annual rate over the next fifteen years— a trajectory representing the disk capacity 
demanded by the typical users of new mainframe comp uters. At the same time, the capacity of the 
average 14-inch drive introduced for sale each year  increased at a faster, 22 percent rate, reaching 
beyond the mainframe market to the large scientific  and supercomputer markets. 9 
 
  
Figure 1.7 Intersecting Trajectories of Capacity Demanded vers us Capacity Supplied in Rigid Disk 
Drives  
 
 
 
Source:  Clayton M. Christensen, “The Rigid Disk Drive Indu stry: A History of Commercial and 
Technological Turbulence,” Business History Review 67, no. 4 (Winter 1993): 559. Reprinted by 
permission.  
 
   30  Between 1978 and 1980, several entrant firms—Shugar t Associates, Micropolis, Priam, and 
Quantum—developed smaller 8-inch drives with 10, 20 , 30, and 40 MB capacity. These drives were of 
no interest to mainframe computer manufacturers, wh ich at that time were demanding drives with 300 
to 400 MB capacity. These 8-inch entrants therefore  sold their disruptive drives into a new 
application—minicomputers. 10  The customers—Wang, DEC, Data General, Prime, and Hewlett-
Packard—did not manufacture mainframes, and their c ustomers often used software substantially 
different from that used in mainframes. These firms  hitherto had been unable to offer disk drives in 
their small, desk-side minicomputers because 14-inc h models were too big and expensive. Although 
initially the cost per megabyte of capacity of 8-in ch drives was higher than that of 14-inch drives, t hese 
new customers were willing to pay a premium for oth er attributes that were important to them—
especially smaller size. Smallness had little value  to mainframe users. 
Once the use of 8-inch drives became established in  minicomputers, the hard disk capacity shipped 
with the median-priced minicomputer grew about 25 p ercent per year: a trajectory determined by the 
ways in which minicomputer owners learned to use th eir machines. At the same time, however, the 8-
inch drive makers found that, by aggressively adopt ing sustaining innovations, they could increase the  
capacity of their products at a rate of more than 4 0 percent per year—nearly double the rate of increa se 
demanded by their original “home” minicomputer mark et. In consequence, by the mid-1980s, 8-inch 
drive makers were able to provide the capacities re quired for lower-end mainframe computers. Unit 
volumes had grown significantly so that the cost pe r megabyte of 8-inch drives had declined below that  
of 14-inch drives, and other advantages became appa rent: For example, the same percentage 
mechanical vibration in an 8-inch drive, as opposed  to a 14-inch drive, caused much less variance in 
the absolute position of the head over the disk. Wi thin a three-to-four-year period, therefore, 8-inch  
drives began to invade the market above them, subst ituting for 14-inch drives in the lower-end 
mainframe computer market. 
As the 8-inch products penetrated the mainframe mar ket, the established manufacturers of 14-inch 
drives began to fail. Two-thirds of them never intr oduced an 8-inch model. The one-third that 
introduced 8-inch models did so about two years beh ind the 8-inch entrant manufacturers. Ultimately, 
every 14-inch drive maker was driven from the indus try. 11  
The 14-inch drive makers were not toppled by the 8- inch entrants because of technology. The 8-inch 
products generally incorporated standard off-the-sh elf components, and when those 14-inch drive 
makers that did introduce 8-inch models got around to doing so, their products were very performance-
competitive in capacity, areal density, access time , and price per megabyte. The 8-inch models 
introduced by the established firms in 1981 were ne arly identical in performance to the average of 
those introduced that year by the entrant firms. In  addition, the rates of improvement in key attribut es 
(measured between 1979 and 1983) were stunningly si milar between established and entrant firms. 12  
 
 
Held Captive by Their Customers   
 
Why were the leading drive makers unable to launch 8-inch drives until it was too late? Clearly, they 
were technologically capable of producing these dri ves. Their failure resulted from delay in making th e 
strategic commitment to enter the emerging market i n which the 8-inch drives initially could be sold. 
Interviews with marketing and engineering executive s close to these companies suggest that the 
established 14-inch drive manufacturers were held c aptive by customers. Mainframe computer 
manufacturers did not need an 8-inch drive. In fact , they explicitly did not want it: they wanted driv es  31  with increased capacity at a lower cost per megabyt e. The 14-inch drive manufacturers were listening 
and responding to their established customers. And their customers—in a way that was not apparent to 
either the disk drive manufacturers or their comput er-making customers—were pulling them along a 
trajectory of 22 percent capacity growth in a 14-in ch platform that would ultimately prove fatal. 13   
Figure 1.7 maps the disparate trajectories of perfo rmance improvement demanded in the computer 
product segments that emerged later, compared to th e capacity that changes in component technology 
and refinements in system design made available wit hin each successive architecture. The solid lines 
emanating from points A, B, C, D, and E measure the  disk drive capacity provided with the median-
priced computer in each category, while the dotted lines from the same points measure the average 
capacity of all disk drives introduced for sale in each architecture, for each year. These transitions  are 
briefly described below. 
 
 
The Advent of the 5.25-inch Drive   
 
In 1980, Seagate Technology introduced 5.25-inch di sk drives. Their capacities of 5 and 10 MB were 
of no interest to minicomputer manufacturers, who w ere demanding drives of 40 and 60 MB from their 
suppliers. Seagate and other firms that entered wit h 5.25-inch drives in the period 1980 to 1983 (for 
example, Miniscribe, Computer Memories, and Interna tional Memories) had to pioneer new 
applications for their products and turned primaril y to desktop personal computer makers. By 1990, the  
use of hard drives in desktop computers was an obvi ous application for magnetic recording. It was not 
at all clear in 1980, however, when the market was just emerging, that many people could ever afford 
or use a hard drive on the desktop. The early 5.25- inch drive makers found this application (one might  
even say that they enabled  it) by trial and error, selling drives to whomever  would buy them. 
Once the use of hard drives was established in desk top PCs, the disk capacity shipped with the median-
priced machine (that is, the capacity demanded by t he general PC user) increased about 25 percent per 
year. Again, the technology improved at nearly twic e the rate demanded in the new market: The 
capacity of new 5.25-inch drives increased about 50  percent per year between 1980 and 1990. As in the 
8-inch for 14-inch substitution, the first firms to  produce 5.25-inch drives were entrants; on average , 
established firms lagged behind entrants by two yea rs. By 1985, only half of the firms producing 8-inc h 
drives had introduced 5.25-inch models. The other h alf never did. 
Growth in the use of 5.25-inch drives occurred in t wo waves. The first followed creation of a new 
application for rigid disk drives: desktop computin g, in which product attributes such as physical siz e, 
relatively unimportant in established applications,  were highly valued. The second wave followed 
substitution of 5.25-inch disks for larger drives i n established minicomputer and mainframe computer 
markets, as the rapidly increasing capacity of 5.25 -inch drives intersected the more slowly growing 
trajectories of capacity demanded in these markets.  Of the four leading 8-inch drive makers—Shugart 
Associates, Micropolis, Priam, and Quantum—only Mic ropolis survived to become a significant 
manufacturer of 5.25-inch drives, and that was acco mplished only with Herculean managerial effort, as 
described in chapter 5. 
 
 
The Pattern Is Repeated: The Emergence of the 3.5-i nch Drive    32   
The 3.5-inch drive was first developed in 1984 by R odime, a Scottish entrant. Sales of this architectu re 
were not significant, however, until Conner Periphe rals, a spinoff of 5.25-inch drive makers Seagate 
and Miniscribe, started shipping product in 1987. C onner had developed a small, lightweight drive 
architecture that was much more rugged than its 5.2 5-inch ancestors. It handled electronically functio ns 
that had previously been managed with mechanical pa rts, and it used microcode to replace functions 
that had previously been addressed electronically. Nearly all of Conner’s first year revenues of $113 
million 14  came from Compaq Computer, which had aided Conner’ s start-up with a $30 million 
investment. The Conner drives were used primarily i n a new application—portable and laptop 
machines, in addition to “small footprint” desktop models—where customers were willing to accept 
lower capacities and higher costs per megabyte to g et lighter weight, greater ruggedness, and lower 
power consumption. 
Seagate engineers were not oblivious to the coming of the 3.5-inch architecture. Indeed, in early 1985 , 
less than one year after Rodime introduced the firs t 3.5-inch drive and two years before  Conner 
Peripherals started shipping its product, Seagate p ersonnel showed working 3.5-inch prototype drives 
to customers for evaluation. The initiative for the  new drives came from Seagate’s engineering 
organization. Opposition to the program came primar ily from the marketing organization and Seagate’s 
executive team; they argued that the market wanted higher capacity drives at a lower cost per megabyte  
and that 3.5-inch drives could never be built at a lower cost per megabyte than 5.25-inch drives. 
Seagate’s marketers tested the 3.5-inch prototypes with customers in the desktop computing market it 
already served—manufacturers like IBM, and value-ad ded resellers of full-sized desktop computer 
systems. Not surprisingly, they indicated little in terest in the smaller drive. They were looking for 
capacities of 40 and 60 megabytes for their next-ge neration machines, while the 3.5-inch architecture 
could provide only 20 MB—and at higher costs. 15  
In response to lukewarm reviews from customers, Sea gate’s program manager lowered his 3.5-inch 
sales estimates, and the firm’s executives canceled  the program. Their reasoning? The markets for 
5.25-inch products were larger, and the sales gener ated by spending the engineering effort on new 5.25 -
inch products would create greater revenues for the  company than would efforts targeted at new 3.5-
inch products. 
In retrospect, it appears that Seagate executives r ead the market—at least their own market—very 
accurately. With established applications and produ ct architectures of their own, such as the IBM XT 
and AT, these customers saw no value in the improve d ruggedness or the reduced size, weight, and 
power consumption of 3.5-inch products. 
Seagate finally began shipping 3.5-inch drives in e arly 1988—the same year in which the performance 
trajectory of 3.5-inch drives (shown in Figure 1.7)  intersected the trajectory of capacity demanded in  
desktop computers. By that time, the industry had s hipped, cumulatively, nearly $750 million in 3.5-
inch products. Interestingly, according to industry  observers, as of 1991 almost none of Seagate’s 3.5 -
inch products had been sold to manufacturers of por table/laptop/notebook computers. In other words, 
Seagate’s primary customers were still desktop comp uter manufacturers, and many of its 3.5-inch 
drives were shipped with frames for mounting them i n computers designed for 5.25-inch drives. 
The fear of cannibalizing sales of existing product s is often cited as a reason why established firms 
delay the introduction of new technologies. As the Seagate-Conner experience illustrates, however, if 
new technologies enable new market applications to emerge, the introduction of new technology may 
not be inherently cannibalistic. But when establish ed firms wait until a new technology has become  33  commercially mature in its new applications and lau nch their own version of the technology only in 
response to an attack on their home markets, the fe ar of cannibalization can become a self-fulfilling 
prophecy. 
Although we have been looking at Seagate’s response  to the development of the 3.5-inch drive 
architecture, its behavior was not atypical; by 198 8, only 35 percent of the drive manufacturers that had 
established themselves making 5.25-inch products fo r the desktop PC market had introduced 3.5-inch 
drives. Similar to earlier product architecture tra nsitions, the barrier to development of a competiti ve 
3.5-inch product does not appear to have been engin eering-based. As in the 14- to 8-inch transition, t he 
new-architecture drives introduced by the incumbent , established firms during the transitions from 8 t o 
5.25 inches and from 5.25 to 3.5 inches were fully performance-competitive with those of entrant 
drives. Rather, the 5.25-inch drive manufacturers s eem to have been misled by their customers, notably  
IBM and its direct competitors and resellers, who t hemselves seemed as oblivious as Seagate to the 
potential benefits and possibilities of portable co mputing and the new disk drive architecture that mi ght 
facilitate it. 
 
 
Prairietek, Conner, and the 2.5-inch Drive   
 
In 1989 an industry entrant in Longmont, Colorado, Prairietek, upstaged the industry by announcing a 
2.5-inch drive, capturing nearly all $30 million of  this nascent market. But Conner Peripherals 
announced its own 2.5-inch product in early 1990 an d by the end of that year had claimed 95 percent of  
the 2.5-inch drive market. Prairietek declared bank ruptcy in late 1991, by which time each of the othe r 
3.5-inch drivemakers—Quantum, Seagate, Western Digi tal, and Maxtor—had introduced 2.5-inch 
drives of their own. 
What had changed? Had the incumbent leading firms f inally learned the lessons of history? Not really. 
Although Figure 1.7 shows the 2.5-inch drive had si gnificantly less capacity than the 3.5-inch drives,  
the portable computing markets into which the small er drives were sold valued other  attributes: weight, 
ruggedness, low power consumption, small physical s ize, and so on. Along these  dimensions, the 2.5-
inch drive offered improved performance over that o f the 3.5-inch product: It was a sustaining  
technology. In fact, the computer makers who bought  Conner’s 3.5-inch drive—laptop computer 
manufacturers such as Toshiba, Zenith, and Sharp—we re the leading makers of notebook computers, 
and these firms needed the smaller 2.5-inch drive a rchitecture. Hence, Conner and its competitors in t he 
3.5-inch market followed their customers seamlessly  across the transition to 2.5-inch drives. 
In 1992, however, the 1.8-inch drive emerged, with a distinctly disruptive character. Although its sto ry 
will be recounted in detail later, it suffices to s tate here that by 1995, it was entrant  firms that 
controlled 98 percent of the $130 million 1.8-inch drive market. Moreover, the largest initial market for 
1.8-inch drives wasn’t in computing at all. It was in portable heart monitoring devices! 
Figure 1.8 summarizes this pattern of entrant firms ‘ leadership in disruptive technology. It shows, fo r 
example, that two years after the 8-inch drive was introduced, two-thirds of the firms producing it (f our 
of six), were entrants. And, two years after the fi rst 5.25-inch drive was introduced, 80 percent of t he 
firms producing these disruptive drives were entran ts. 
 
   34  Figure 1.8 Leadership of Entrant Firms in Disruptive Technolog y  
 
 
 
Source:  Data are from various issues of Disk/Trend Report .  
 
SUMMARY   
 
There are several patterns in the history of innova tion in the disk drive industry. The first is that the 
disruptive innovations were technologically straigh tforward. They generally packaged known 
technologies in a unique architecture and enabled t he use of these products in applications where 
magnetic data storage and retrieval previously had not been technologically or economically feasible. 
The second pattern is that the purpose of advanced technology development in the industry was always 
to sustain  established trajectories of performance improvemen t: to reach the higher-performance, 
higher-margin domain of the upper right of the traj ectory map. Many of these technologies were 
radically new and difficult, but they were not disr uptive. The customers of the leading disk drive 
suppliers led them toward these achievements. Susta ining technologies, as a result, did not precipitat e 
failure. 
The third pattern shows that, despite the establish ed firms’ technological prowess in leading sustaini ng 
innovations, from the simplest to the most radical,  the firms that led the industry in every instance of 
developing and adopting disruptive technologies wer e entrants to the industry, not its incumbent 
leaders. 
This book began by posing a puzzle: Why was it that  firms that could be esteemed as aggressive, 
innovative, customer-sensitive organizations could ignore or attend belatedly to technological 
innovations with enormous strategic importance? In the context of the preceding analysis of the disk 
drive industry, this question can be sharpened cons iderably. The established firms were, in fact, 
aggressive, innovative, and customer-sensitive in t heir approaches to sustaining innovations of every 
sort. But the problem established firms seem unable  to confront successfully is that of downward  vision 
and mobility, in terms of the trajectory map. Findi ng new applications and markets for these new 
products seems to be a capability that each of thes e firms exhibited once, upon entry, and then 
apparently lost. It was as if the leading firms wer e held captive by their customers, enabling attacki ng  35  entrant firms to topple the incumbent industry lead ers each time a disruptive technology emerged. 16  
Why this happened, and is still happening, is the s ubject of the next chapter. 
 
 
APPENDIX 1.1:  
A NOTE ON THE DATA AND METHOD  
USED TO GENERATE FIGURE 1.7   
 
The trajectories mapped in Figure 1.7 were calculat ed as follows. Data on the capacity provided with 
computers was obtained from Data Sources,  an annual publication listing the technical specif ications of 
all computer models available from every computer m anufacturer. For instances in which particular 
models were available with different features and c onfigurations, the manufacturer provided Data 
Sources  with a “typical” system configuration with defined  random access memory (RAM) capacity, 
performance specifications of peripheral equipment (including disk drives), list prices, and year of 
introduction. For instances in which a given comput er model was offered for sale over a sequence of 
years, the hard disk capacity provided in the typic al configuration typically increased. Data Sources  
used the categories mainframe, mini/midrange, deskt op personal, portable and laptop, and notebook. 
As of 1993, 1.8-inch drives were not being used in hand-held computers, so no data on that potential 
market existed. 
For Figure 1.7, for each year and each class of com puters, all models available for sale were ranked b y 
price and the hard disk capacity provided with the median-priced model identified. The best-fit lines 
through the resultant time series were plotted as t he solid lines in Figure 1.7 for expository 
simplification to indicate the trend in typical mac hines. In reality, of course, there is a wide band 
around these lines. The frontier  of performance—the highest capacity offered with t he most expensive 
computers—was substantially higher than the typical  values shown. 
The dotted lines in Figure 1.7 represent the best-f it line through the unweighted average capacity of all 
disk drives introduced for sale in each given archi tecture for each year. This data was taken from 
Disk/Trend Report.  Again, for expository simplification, only this av erage line is shown. There was a 
wide band of capacities introduced for sale in each  year, so that the frontier or highest capacity dri ve 
introduced in each year was substantially above the  average shown. Stated in another way, a distinctio n 
must be made between the full range of products ava ilable for purchase and those in typical systems. 
The upper and lower bands around the median and ave rage figures shown in Figure 1.7 are generally 
parallel to the lines shown. 
Because higher capacity drives were available in th e market than were offered with the median-priced 
systems, the solid-line trajectories in Figure 1.7,  as I state in the text, represent the capacities 
“demanded” in each market. In other words, the capa city per machine was not constrained by 
technological availability. Rather, it represents t he selection of hard disk capacity by computer user s, 
given the prevailing cost. 
 
 
NOTES   
 
1.  A more complete history of the disk drive industry  can be found in Clayton M. Christensen, “The  36  Rigid Disk Drive Industry: A History of Commercial and Technological Turbulence,” Business History 
Review  (67), Winter, 1993, 531–588. This history focuses only on the manufacturers of rigid disk 
drives or hard drives—products on which data are st ored on rigid metal platters. Companies 
manufacturing floppy disk drives (removable diskett es of flexible mylar coated with iron oxide on 
which data are stored) historically were different firms from those making hard disk drives.  
2.  Much of the data for this analysis came from Disk/Trend Report,  a highly respected annual market 
research publication, augmented with more detailed product-specification sheets obtained from the disk  
drive manufacturers themselves. I am grateful to th e editors and staff at Disk/Trend, Inc., for their 
patient and generous assistance in this project.  
3.  The concept of trajectories of technological progr ess was examined by Giovanni Dosi in 
“Technological Paradigms and Technological Trajecto ries,” Research Policy  (11), 1982, 147–162.  
4.  The ways in which the findings of this study diffe r from those of some earlier scholars of technology  
change while building upon those of others are disc ussed in greater detail in chapter 2.  
5.  The first technology for making heads built an ele ctromagnet by wrapping a fine thread of copper 
wire around a core of iron oxide (ferrite); hence t he term ferrite head.  Incremental improvements to 
this approach involved learning to grind the ferrit e to finer and finer dimensions, using better lappi ng 
techniques, and strengthening the ferrite by doping  it with barium. Thin-film heads  were made 
photolithographically, using technology similar to that used in making integrated circuits on silicon 
wafers to etch the electromagnet on the surface of the head. This was difficult because it involved mu ch 
thicker layers of material than were common in IC m anufacturing. The third technology, adopted 
starting in the mid-1990s, was called magneto-resistive heads.  These were also made with thin-film 
photolithography, but used the principle that chang es in the magnetic flux field on the disk surface 
changed the electrical resistivity of the circuitry  in the head. By measuring changes in resistivity r ather 
than changes in the direction of current flow, magn eto-resistive heads were much more sensitive, and 
hence permitted denser data recording, than prior t echnology. In the evolution of disk technology, the  
earliest disks were made by coating fine needle-sha ped particles of iron oxide—literally rust—over the  
surface of a flat, polished aluminum platter. Hence , these disks were called oxide  disks. Incremental 
improvements to this technology involved making fin er and finer iron oxide particles, and dispersing 
them more uniformly, with fewer uncoated voids on t he aluminum platter’s surface. This was 
supplanted by a sputtering technology, also borrowe d from semiconductor processing, that coated the 
aluminum platter with a thin film of metal a few an gstroms thick. The thinness of this layer; its 
continuous, rather than particulate nature; and the  process’s flexibility in depositing magnetic mater ials 
with higher coercivity, enabled denser recording on  thin-film disks than was feasible on oxide disks.  
6.  Richard J. Foster, Innovation: The Attacker’s Advantage  (New York: Summit Books, 1986).  
7.  The examples of technology change presented in Fig ures 1.1 and 1.2 introduce some ambiguity to 
the unqualified term discontinuity,  as used by Giovanni Dosi (see “Technological Parad igms and 
Technological Trajectories,” Research Policy  [11] 1982), Michael L. Tushman and Philip Anderson  
(see “Technological Discontinuities and Organizatio nal Environments,” Administrative Science 
Quarterly  [31], 1986), and others. The innovations in head a nd disk technology described in Figure 1.4 
represent positive discontinuities in an established technological tra jectory, while the trajectory-
disrupting technologies charted in Figure 1.7 repre sent negative  discontinuities. As will be shown 
below, established firms seemed quite capable of le ading the industry over positive discontinuities, b ut 
generally lost their industry lead when faced with negative discontinuities.  
8.  This tendency consistently appears across a range of industries. Richard S. Rosenbloom and Clayton 
M. Christensen (in “Technological Discontinuities, Organizational Capabilities, and Strategic 
Commitments,” Industrial and Corporate Change  [3], 1994, 655–685) suggest a much broader set of 
industries in which leading firms may have been top pled by technologically straightforward disruptive 
innovations than is covered in this book.  
9.  A summary of the data and procedures used to gener ate Figure 1.7 is included in Appendix 1.1.  
10.  The minicomputer market was not new in 1978, but i t was a new application for Winchester- 37  technology disk drives.  
11.  This statement applies only to independent drive m akers competing in the OEM market. Some of 
the vertically integrated computer makers, such as IBM, have survived across these generations with 
the benefit of a captive internal market. Even IBM,  however, addressed the sequence of different 
emerging markets for disk drives by creating autono mous “start-up” disk drive organizations to address  
each one. Its San Jose organization focused on high -end (primarily mainframe) applications. A separate  
division in Rochester, MN, focused on mid-range com puters and workstations. IBM created a different 
organization in Fujisawa, Japan, to produce drives for the desktop personal computer market.  
12.  This result is very different from that observed b y Rebecca M. Henderson (see The Failure of 
Established Firms in the Face of Technological Chan ge: A Study of the Semiconductor 
Photolithographic Alignment Industry,  dissertation, Harvard University, 1988), who found  the new-
architecture aligners produced by the established m anufacturers to be inferior in performance to those  
produced by entrant firms. One possible reason for these different results is that the successful entr ants 
in the photolithographic aligner industry studied b y Henderson brought to the new product a well-
developed body of technological knowledge and exper ience developed and refined in other markets. In 
the case studied here, none of the entrants brought  such well-developed knowledge with them. Most, in 
fact, were de novo  start-ups composed of managers and engineers who h ad defected from established 
drive manufacturing firms.  
13.  This finding is similar to the phenomenon observed  by Joseph L. Bower, who saw that explicit 
customer demands have tremendous power as a source of impetus in the resource allocation process: 
“When the discrepancy (the problem to be solved by a proposed investment) was defined in terms of 
cost and quality, the projects languished. In all f our cases, the definition process moved toward 
completion when capacity to meet sales was perceive d to be inadequate. . . . In short, pressure from t he 
market reduces both the probability and the cost of  being wrong.” Although Bower specifically refers 
to manufacturing capacity, the same fundamental phe nomenon—the power of the known needs of 
known customers in marshaling and directing the inv estments of a firm—affects response to disruptive 
technology. See Joseph L. Bower, Managing the Resource Allocation Process  (Homewood, IL: 
Richard D. Irwin, 1970) 254.  
14.  In booking $113 million in revenues, Conner Periph erals set a record for booking more revenues in 
its first year of operation than any manufacturing company in United States history.  
15.  This finding is consistent with what Robert Burgel man has observed. He noted that one of the 
greatest difficulties encountered by corporate entr epreneurs has been finding the right “beta test sit es” 
where products could be interactively developed and  refined with customers. Generally, a new 
venture’s entrée to the customer was provided by th e salesperson representing the firm’s established 
product lines. This helped the firm develop new pro ducts for established markets but not to identify 
new applications for new technology. See Robert A. Burgelman and Leonard Sayles, Inside Corporate 
Innovation  (New York: The Free Press, 1986) 76–80.  
16.  I believe this insight—that attacking firms have a n advantage in disruptive innovations but not in 
sustaining ones—clarifies, but is not in conflict w ith, Foster’s assertions about the attacker’s advan tage. 
The historical examples Foster uses to substantiate  his theory generally seem to have been disruptive 
innovations. See Richard J. Foster, Innovation: The Attacker’s Advantage  (New York: Summit Books, 
1986).  
 
 
 
 
   38   
CHAPTER TWO 
Value Networks and the  
Impetus to Innovate 
 
 
From the earliest studies of the problems of innova tion, scholars, consultants, and managers have trie d 
to explain why leading firms frequently stumble whe n confronting technology change. Most 
explanations either zero in on managerial, organiza tional, and cultural responses to technological 
change or focus on the ability of established firms  to deal with radically new technology; doing the 
latter requires a very different set of skills from  those that an established firm historically has 
developed. Both approaches, useful in explaining wh y some companies stumble in the face of 
technological change, are summarized below. The pri mary purpose of this chapter, however, is to 
propose a third theory of why good companies can fa il, based upon the concept of a value network.  The 
value network concept seems to have much greater po wer than the other two theories in explaining 
what we observed in the disk drive industry.  
 
ORGANIZATIONAL AND MANAGERIAL EXPLANATIONS OF FAILU RE   
 
One explanation for why good companies fail points to organizational impediments as the source of the 
problem. While many analyses of this type stop with  such simple rationales as bureaucracy, 
complacency, or “risk-averse” culture, some remarka bly insightful studies exist in this tradition. 
Henderson and Clark, 1 for example, conclude that companies’ organization al structures typically 
facilitate component-level innovations, because mos t product development organizations consist of 
subgroups that correspond to a product’s components . Such systems work very well as long as the 
product’s fundamental architecture does not require  change. But, say the authors, when architectural 
technology change is required, this type of structu re impedes innovations that require people and 
groups to communicate and work together in new ways . 
This notion has considerable face validity. In one incident recounted in Tracy Kidder’s Pulitzer Prize -
winning narrative, The Soul of a New Machine,  Data General engineers developing a next-generatio n 
minicomputer intended to leapfrog the product posit ion of Digital Equipment Corporation were 
allowed by a friend of one team member into his fac ility in the middle of the night to examine Digital ’s 
latest computer, which his company had just bought.  When Tom West, Data General’s project leader 
and a former long-time Digital employee, removed th e cover of the DEC minicomputer and examined 
its structure, he saw “Digital’s organization chart  in the design of the product.” 2 
Because an organization’s structure and how its gro ups work together may have been established to 
facilitate the design of its dominant product, the direction of causality may ultimately reverse itsel f: 
The organization’s structure and the way its groups  learn to work together can then affect the way it 
can and cannot design new products. 
 
 
CAPABILITIES AND RADICAL TECHNOLOGY AS AN EXPLANATI ON    39   
In assessing blame for the failure of good companie s, the distinction is sometimes made between 
innovations requiring very different technological capabilities, that is, so-called radical change, an d 
those that build upon well-practiced technological capabilities, often called incremental innovations.3 
The notion is that the magnitude of the technologic al change relative to the companies’ capabilities w ill 
determine which firms triumph after a technology in vades an industry. Scholars who support this view 
find that established firms tend to be good at impr oving what they have long been good at doing, and 
that entrant firms seem better suited for exploitin g radically new technologies, often because they 
import the technology into one industry from anothe r, where they had already developed and practiced 
it. 
Clark, for example, has reasoned that companies bui ld the technological capabilities in a product such  
as an automobile hierarchically and experientially.4 An organization’s historical choices about which 
technological problems it would solve and which it would avoid determine the sorts of skills and 
knowledge it accumulates. When optimal resolution o f a product or process performance problem 
demands a very different set of knowledge than a fi rm has accumulated, it may very well stumble. The 
research of Tushman, Anderson, and their associates  supports Clark’s hypothesis. 5 They found that 
firms failed when a technological change destroyed the value of competencies previously cultivated 
and succeeded when new technologies enhanced them. 
The factors identified by these scholars undoubtedl y affect the fortunes of firms confronted with new 
technologies. Yet the disk drive industry displays a series of anomalies accounted for by neither set of 
theories. Industry leaders first introduced sustain ing technologies of every  sort, including architectural 
and component innovations that rendered prior compe tencies irrelevant and made massive investments 
in skills and assets obsolete. Nevertheless, these same firms stumbled over technologically 
straightforward but disruptive changes such as the 8-inch drive. 
The history of the disk drive industry, indeed, giv es a very different meaning to what constitutes a 
radical innovation among leading, established firms . As we saw, the nature of the technology involved 
(components versus architecture and incremental ver sus radical), the magnitude of the risk, and the 
time horizon over which the risks needed to be take n had little relationship to the patterns of leader ship 
and followership observed. Rather, if their custome rs needed an innovation, the leading firms somehow 
mustered the resources and wherewithal to develop a nd adopt it. Conversely, if their customers did not  
want or need an innovation, these firms found it im possible to commercialize even technologically 
simple innovations. 
 
 
VALUE NETWORKS AND NEW PERSPECTIVE ON THE DRIVERS O F FAILURE   
 
What, then, does  account for the success and failure of entrant and  established firms? The following 
discussion synthesizes from the history of the disk  drive industry a new perspective on the relation 
between success or failure and changes in technolog y and market structure. The concept of the value 
network —the context within which a firm identifies and res ponds to customers’ needs, solves 
problems, procures input, reacts to competitors, an d strives for profit—is central to this synthesis. 6 
Within a value network, each firm’s competitive str ategy, and particularly its past choices of markets , 
determines its perceptions of the economic value of  a new technology. These perceptions, in turn, 
shape the rewards different firms expect to obtain through pursuit of sustaining and disruptive 
innovations. 7 In established firms, expected rewards, in their t urn, drive the allocation of resources  40  toward sustaining innovations and away from disrupt ive ones. This pattern of resource allocation 
accounts for established firms’ consistent leadersh ip in the former and their dismal performance in th e 
latter. 
 
 
Value Networks Mirror Product Architecture   
 
Companies are embedded in value networks because th eir products generally are embedded, or nested 
hierarchically, as components within other products  and eventually within end systems of use. 8 
Consider a 1980s-vintage management information sys tem (MIS) for a large organization, as illustrated 
in Figure 2.1. The architecture of the MIS ties tog ether various components—a mainframe computer; 
peripherals such as line printers and tape and disk  drives; software; a large, air-conditioned room wi th 
cables running under a raised floor; and so on. At the next level, the mainframe computer is itself an  
architected system, comprising such components as a  central processing unit, multi-chip packages and 
circuit boards, RAM circuits, terminals, controller s, and disk drives. Telescoping down still further,  the 
disk drive is a system whose components include a m otor, actuator, spindle, disks, heads, and 
controller. In turn, the disk itself can be analyze d as a system composed of an aluminum platter, 
magnetic material, adhesives, abrasives, lubricants , and coatings. 
Although the goods and services constituting such a  system of use may all be produced within a single,  
extensively integrated corporation such as AT&T or IBM, most are tradable, especially in more mature 
markets. This means that, while Figure 2.1 is drawn  to describe the nested physical  architecture of a 
product system, it also implies the existence of a nested network of producers and markets  through 
which the components at each level are made and sol d to integrators at the next higher level in the 
system. Firms that design and assemble disk drives,  for example, such as Quantum and Maxtor, 
procure read-write heads from firms specializing in  the manufacture of those heads, and they buy disks  
from other firms and spin motors, actuator motors, and integrated circuitry from still others. At the next 
higher level, firms that design and assemble comput ers may buy their integrated circuits, terminals, 
disk drives, IC packaging, and power supplies from various firms that manufacture those particular 
products. This nested commercial system is a value network.  
 
  
Figure 2.1 A Nested, or Telescoping, System of Product Archite cture  
  41  
 
 
Source: Reprinted from Research Policy  24, Clayton M. Chistensen and Richard S. Rosenbloo m, 
“Explaining the Attacker's Advantage: Technological  Paradigms, Organizational Dynamics, and the 
Value Network,” 233–257, 1995 with kind permission of Elsevier Science—NL, Sara Burgerhartstraat 
25, 1055 KV Amsterdam, The Netherlands.  
 
  
Figure 2.2 illustrates three value networks for com puting applications: Reading top to bottom they are  
the value network for a corporate MIS system-of-use , for portable personal computing products, and 
for computer-automated design (CAD). Drawn only to convey the concept of how networks are 
bounded and may differ from each other, these depic tions are not meant to represent complete 
structures. 
 
 
Metrics of Value   
 
The way value is measured differs across networks. 9 In fact, the unique rank-ordering of the 
importance of various product performance attribute s defines, in part, the boundaries of a value 
network. Examples in Figure 2.2, listed to the righ t of the center column of component boxes, show  42  how each value network exhibits a very different ra nk-ordering of important product attributes, even 
for the same product. In the top-most value network , disk drive performance is measured in terms of 
capacity, speed, and reliability, whereas in the po rtable computing value network, the important 
performance attributes are ruggedness, low power co nsumption, and small size. Consequently, parallel 
value networks, each built around a different defin ition of what makes a product valuable, may exist 
within the same broadly defined industry. 
 
  
 
Figure 2.2 Examples of Three Value Networks 
  
 
Source: Reprinted from Research Policy 24, Clayton M. Christensen and Richard S. Rosenbloom, 
“Explaining the Attacker's Advantage: Technological  Paradigms, Organizational Dynamics, and the 
Value Network,” 233–257, 1995 with kind permission of Elsevier Science—NL, Sara Burgerhartstraat 
25, 1055 KV Amsterdam, The Netherlands.  
 
  
Although many components in different systems-of-us e may carry the same labels (for example, each 
network in Figure 2.2 involves read-write heads, di sk drives, RAM circuits, printers, software, and so  
on), the nature of components used may be quite dif ferent. Generally, a set of competing firms, each  43  with its own value chain, 10  is associated with each box in a network diagram, and the firms supplying 
the products and services used in each network ofte n differ (as illustrated in Figure 2.2 by the firms  
listed to the left of the center column of componen t boxes).  
As firms gain experience within a given network, th ey are likely to develop capabilities, organization al 
structures, and cultures tailored to their value ne twork’s distinctive requirements. Manufacturing 
volumes, the slope of ramps to volume production, p roduct development cycle times, and 
organizational consensus identifying the customer a nd the customer’s needs may differ substantially 
from one value network to the next. 
Given the data on the prices, attributes, and perfo rmance characteristics of thousands of disk drive 
models sold between 1976 and 1989, we can use a tec hnique called hedonic regression analysis  to 
identify how markets valued individual attributes a nd how those attribute values changed over time. 
Essentially, hedonic regression analysis expresses the total price of a product as the sum of individu al 
so-called shadow prices (some positive, others nega tive) that the market places on each of the product ’s 
characteristics. Figure 2.3 shows some results of t his analysis to illustrate how different value netw orks 
can place very different values on a given performa nce attribute. Customers in the mainframe computer 
value network in 1988 were willing on average to pa y $1.65 for an incremental megabyte of capacity; 
but moving across the minicomputer, desktop, and po rtable computing value networks, the shadow 
price of an incremental megabyte of capacity declin es to $1.50, $1.45, and $1.17, respectively. 
Conversely, portable and desktop computing customer s were willing to pay a high price in 1988 for a 
cubic inch of size reduction, while customers in th e other networks placed no value on that attribute at 
all. 11  
 
  
Figure 2.3  Difference in the Valuation of Attributes Across D ifferent Value Networks  
 
 
 
 
 
Cost Structures and Value Networks   
 
The definition of a value network goes beyond the a ttributes of the physical product. For example, 
competing within the mainframe computer network sho wn in Figure 2.2 entails a particular cost  44  structure. Research, engineering, and development c osts are substantial. Manufacturing overheads are 
high relative to direct costs because of low unit v olumes and customized product configurations. 
Selling directly to end users involves significant sales force costs, and the field service network to  
support the complicated machines represents a subst antial ongoing expense. All these costs must be 
incurred in order to provide the types of products and services customers in this value network requir e. 
For these reasons, makers of mainframe computers, a nd makers of the 14-inch disk drives sold to them, 
historically needed gross profit margins of between  50 percent and 60 percent to cover the overhead 
cost structure inherent to the value network in whi ch they competed. 
Competing in the portable computer value network, h owever, entails a very different cost structure. 
These computer makers incur little expense research ing component technologies, preferring to build 
their machines with proven component technologies p rocured from vendors. Manufacturing involves 
assembling millions of standard products in low-lab or-cost regions. Most sales are made through 
national retail chains or by mail order. As a resul t, companies in this value network can be profitabl e 
with gross margins of 15 percent to 20 percent. Hen ce, just as a value network is characterized by a 
specific rank-ordering of product attributes valued  by customers, it is also characterized by a specif ic 
cost structure required to provide the valued produ cts and services. 
Each value network’s unique cost structure is illus trated in Figure 2.4. Gross margins typically obtai ned 
by manufacturers of 14-inch disk drives, about 60 p ercent, are similar to those required by mainframe 
computer makers: 56 percent. Likewise, the margins 8-inch drive makers earned were similar to those 
earned by minicomputer companies (about 40 percent) , and the margins typical of the desktop value 
network, 25 percent, also typified both the compute r makers and their disk drive suppliers. 
The cost structures characteristic of each value ne twork can have a powerful effect on the sorts of 
innovations firms deem profitable. Essentially, inn ovations that are valued within a firm’s value 
network, or in a network where characteristic gross  margins are higher, will be perceived as profitabl e. 
Those technologies whose attributes make them valua ble only in networks with lower  gross margins, 
on the other hand, will not be viewed as profitable , and are unlikely to attract resources or manageri al 
interest. (We will explore the impact of each value  network’s characteristic cost structures upon the 
established firms’ mobility and fortunes more fully  in chapter 4 .) 
 
  
Figure 2.4 Characteristic Cost Structures of Different Value N etworks  
  45  
 
 
Source:  Data are from company annual reports and personal interviews with executives from several 
representative companies in each network.  
 
  
In sum, the attractiveness of a technological oppor tunity and the degree of difficulty a producer will  
encounter in exploiting it are determined by, among  other factors, the firm’s position in the relevant  
value network. As we shall see, the manifest streng th of established firms in sustaining innovation an d 
their weakness in disruptive innovation—and the opp osite manifest strengths and weaknesses of entrant 
firms—are consequences not of differences in techno logical or organizational capabilities between 
incumbent and entrant firms, but of their positions  in the industry’s different value networks.  
 
TECHNOLOGY S-CURVES AND VALUE NETWORKS   
 
The technology S-curve forms the centerpiece of thi nking about technology strategy. It suggests that 
the magnitude of a product’s performance improvemen t in a given time period or due to a given 
amount of engineering effort is likely to differ as  technologies mature. The theory posits that in the  
early stages of a technology, the rate of progress in performance will be relatively slow. As the 
technology becomes better understood, controlled, a nd diffused, the rate of technological improvement 
will accelerate. 12  But in its mature stages, the technology will asym ptotically approach a natural or 
physical limit such that ever greater periods of ti me or inputs of engineering effort will be required  to 
achieve improvements. Figure 2.5 illustrates the re sulting pattern. 
Many scholars have asserted that the essence of str ategic technology management is to identify when 
the point of inflection on the present technology’s  S-curve has been passed, and to identify and devel op 
whatever successor technology rising from below wil l eventually supplant the present approach. Hence, 
as depicted by the dotted curve in Figure 2.5, the challenge is to successfully switch technologies at  the 
point where S-curves of old and new intersect. The inability to anticipate new technologies threatenin g 
from below and to switch to them in a timely way ha s often been cited as the cause of failure of 
established firms and as the source of advantage fo r entrant or attacking firms. 13   46   
  
Figure 2.5 The Conventional Technology S-Curve  
 
 
 
Source:  Clayton M. Christensen, “Exploring the Limits of t he Technology S-Curve. Part I: Component 
Technologies,” Production and Operations Management  1, no. 4 (Fall 1992): 340. Reprinted by 
permission.  
How do the concepts of S-curves and of value networ ks relate to each other? 14  The typical framework 
of intersecting S-curves illustrated in Figure 2.5 is a conceptualization of sustaining  technological 
changes within a single value network, where the ve rtical axis charts a single measure of product 
performance (or a rank-ordering of attributes). Not e its similarity to Figure 1.4 , which measured the 
sustaining impact of new recording head technologie s on the recording density of disk drives. 
Incremental improvements within each technology dro ve improvements along each of the individual 
curves, while movement to new head technologies inv olved a more radical leap. Recall that there was 
not a single  example in the history of technological innovation  in the disk drive industry of an entrant 
firm leading the industry or securing a viable mark et position with a sustaining innovation. In every 
instance, the firms that anticipated the eventual f lattening of the current technology and that led in  
identifying, developing, and implementing the new t echnology that sustained the overall pace of 
progress were the leading practitioners of the prio r technology. These firms often incurred enormous 
financial risks, committing to new technologies a d ecade or more in advance and wiping out substantial  
bases of assets and skills. Yet despite these chall enges, managers of the industry’s established firms  
navigated the dotted line course shown in Figure 2. 5 with remarkable, consistent agility. 
A disruptive innovation, however, cannot be plotted  in a figure such as 2.5, because the vertical axis  for 
a disruptive innovation, by definition, must measur e different  attributes of performance than those 
relevant in established value networks. Because a d isruptive technology gets its commercial start in 
emerging value networks before invading established  networks, an S-curve framework such as that in 
Figure 2.6 is needed to describe it. Disruptive tec hnologies emerge and progress on their own, uniquel y 
defined trajectories, in a home value network. If a nd when they progress to the point that they can 
satisfy the level and nature of performance demande d in another value network, the disruptive 
technology can then invade it, knocking out the est ablished technology and its established practitione rs, 
with stunning speed.  47  Figures 2.5 and 2.6 illustrate clearly the innovato r’s dilemma that precipitates the failure of leadin g 
firms. In disk drives (and in the other industries covered later in this book), prescriptions such as 
increased investments in R&D; longer investment and  planning horizons; technology scanning, 
forecasting, and mapping; as well as research conso rtia and joint ventures are all relevant to the 
challenges posed by the sustaining  innovations whose ideal pattern is depicted in Fig ure 2.5. Indeed, 
the evidence suggests that many of the best establi shed firms have applied these remedies and that the y 
can work when managed well in treating sustaining t echnologies. But none of these solutions addresses 
the situation in Figure 2.6, because it represents a threat of a fundamentally different nature. 
 
  
Figure 2.6 Disruptive Technology S-Curve  
 
 
 
Source: Clayton M. Christensen, “Exploring the Limits of th e Technology S-Curve. Part I: Component 
Technologies,” Production and Operations Management  1, no. 4 (Fall 1992): 361. Reprinted by 
permission.  
 
 
MANAGERIAL DECISION MAKING AND DISRUPTIVE TECHNOLOG ICAL CHANGE   
 
Competition within the value networks in which comp anies are embedded defines in many ways how 
the firms can earn their money. The network defines  the customers’ problems to be addressed by the 
firm’s products and services and how much can be pa id for solving them. Competition and customer 
demands in the value network in many ways shape the  firms’ cost structure, the firm size required to 
remain competitive, and the necessary rate of growt h. Thus, managerial decisions that make sense for 
companies outside a value network may make no sense  at all for those within it, and vice versa. 
We saw, in chapter 1, a stunningly consistent patte rn of successful implementation of sustaining 
innovations by established firms and their failure to deal with disruptive ones. The pattern was 
consistent because the managerial decisions that le d to those outcomes made sense. Good managers do 
what makes sense, and what makes sense is primarily  shaped by their value network. 
This decision-making pattern, outlined in the six s teps below, emerged from my interviews with more 
than eighty managers who played key roles in the di sk drive industry’s leading firms, both incumbents 
and entrants, at times when disruptive technologies  had emerged. In these interviews I tried to 
reconstruct, as accurately and from as many points of view as possible, the forces that influenced the se  48  firms’ decision-making processes regarding the deve lopment and commercialization of technologies 
either relevant or irrelevant to the value networks  in which the firms were at the time embedded. My 
findings consistently showed that established firms  confronted with disruptive technology change did 
not have trouble developing the requisite technology:  Prototypes of the new drives had often been 
developed before management was asked to make a dec ision. Rather, disruptive projects stalled when it 
came to allocating scarce resources among competing  product and technology development proposals 
(allocating resources between the two value network s shown at right and left in Figure 2.6, for 
example). Sustaining projects addressing the needs of the firms’ most powerful customers (the new 
waves of technology within the value network depict ed in Figure 2.5) almost always  preempted 
resources from disruptive technologies with small m arkets and poorly defined customer needs. 
This characteristic pattern of decisions is summari zed in the following pages. Because the experience 
was so archetypical, the struggle of Seagate Techno logy, the industry’s dominant maker of 5.25-inch 
drives, to successfully commercialize the disruptiv e 3.5-inch drive is recounted in detail to illustra te 
each of the steps in the pattern. 15  
 
 
Step 1: Disruptive Technologies Were First Develope d within Established Firms   
 
Although entrants led in commercializing  disruptive technologies, their development was oft en the 
work of engineers at established firms, using bootl egged resources. Rarely initiated by senior 
management, these architecturally innovative design s almost always employed off-the-shelf 
components. Thus, engineers at Seagate Technology, the leading 5.25-inch drive maker, were, in 1985, 
the second in the industry to develop working proto types of 3.5-inch models. They made some eighty 
prototype models before the issue of formal project  approval was raised with senior management. The 
same thing happened earlier at Control Data and Mem orex, the dominant 14-inch drive makers, where 
engineers had designed working 8-inch drives intern ally, nearly two years before the product appeared 
in the market. 
 
 
Step 2: Marketing Personnel Then Sought Reactions f rom Their Lead Customers   
 
The engineers then showed their prototypes to marke ting personnel, asking whether a market for the 
smaller, less expensive (and lower performance) dri ves existed. The marketing organization, using its 
habitual procedure for testing the market appeal of  new drives, showed the prototypes to lead 
customers of the existing product line, asking them  for an evaluation. 16  Thus, Seagate marketers tested 
the new 3.5-inch drives with IBM’s PC Division and other makers of XT- and AT-class desktop 
personal computers—even though the drives had signi ficantly less capacity than the mainstream 
desktop market demanded. 
Not surprisingly, therefore, IBM showed no interest  in Seagate’s disruptive 3.5-inch drives. IBM’s 
engineers and marketers were looking for 40 and 60 MB drives, and they already had a slot for 5.25-
inch drives designed into their computer; they need ed new drives that would take them further along 
their established performance trajectory. Finding l ittle customer interest, Seagate’s marketers drew u p 
pessimistic sales forecasts. In addition, because t he products were simpler, with lower performance, 
forecast profit margins were lower than those for h igher performance products, and Seagate’s financial   49  analysts, therefore, joined their marketing colleag ues in opposing the disruptive program. Working 
from such input, senior managers shelved the 3.5-in ch drive—just as it was becoming firmly 
established in the laptop market. 
This was a complex decision, made in a context of c ompeting proposals to expend the same resources 
to develop new products that marketers felt were cr itical to remaining competitive with current 
customers and achieving aggressive growth and profi t targets. “We needed a new model,” recalled a 
former Seagate manager, “which could become the nex t ST412 [a very successful product generating 
$300 million sales annually in the desktop market t hat was near the end of its life cycle]. Our foreca sts 
for the 3.5-inch drive were under $50 million becau se the laptop market was just emerging, and the 3.5 -
inch product just didn’t fit the bill.” 
Seagate managers made an explicit decision not to p ursue the disruptive technology. In other cases, 
managers did approve resources for pursuing a disru ptive product—but, in the day-to-day decisions 
about how time and money would actually be allocate d, engineers and marketers, acting in the best 
interests of the company, consciously and unconscio usly starved the disruptive project of resources 
necessary for a timely launch.  
When engineers at Control Data, the leading 14-inch  drive maker, were officially chartered to develop 
CDC’s initial 8-inch drives, its customers were loo king for an average of 300 MB per computer, 
whereas CDC’s earliest 8-inch drives offered less t han 60 MB. The 8-inch project was given low 
priority, and engineers assigned to its development  kept getting pulled off to work on problems with 
14-inch drives being designed for more important cu stomers. Similar problems plagued the belated 
launches of Quantum’s and Micropolis’s 5.25-inch pr oducts. 
 
 
Step 3: Established Firms Step Up the Pace of Susta ining Technological Development   
 
In response to the needs of current customers, the marketing managers threw impetus behind 
alternative sustaining projects, such as incorporat ing better heads or developing new recording codes.  
These gave customers what they wanted and could be targeted at large markets to generate the 
necessary sales and profits for maintaining growth.  Although often involving greater development 
expense, such sustaining investments appeared far  less risky than investments in the disruptive 
technology: The customers existed, and their needs were known. 
Seagate’s decision to shelve the 3.5-inch drive in 1985 to 1986, for example, seems starkly rational. Its 
view downmarket (in terms of the disk drive traject ory map) was toward a small total market forecast 
for 1987 for 3.5-inch drives. Gross margins in that  market were uncertain, but manufacturing 
executives predicted that costs per megabyte for 3. 5-inch drives would be much higher than for 5.25-
inch drives. Seagate’s view upmarket was quite diff erent. Volumes in 5.25-inch drives with capacities 
of 60 to 100 MB were forecast to be $500 million by  1987. Companies serving the 60 to 100 MB 
market were earning gross margins of between 35 and  40 percent, whereas Seagate’s margins in its 
high-volume 20 MB drives were between 25 and 30 per cent. It simply did not make sense for Seagate 
to put its resources behind the 3.5-inch drive when  competing proposals to move upmarket by 
developing its ST251 line of drives were also being  actively evaluated. 
After Seagate executives shelved the 3.5-inch proje ct, the firm began introducing new 5.25-inch 
models at a dramatically accelerated rate. In 1985,  1986, and 1987, the numbers of new models  50  annually introduced as a percentage of the total nu mber of its models on the market in the prior year 
were 57, 78, and 115 percent, respectively. And dur ing the same period, Seagate incorporated complex 
and sophisticated new component technologies such a s thin-film disks, voice-coil actuators, 17  RLL 
codes, and embedded SCSI interfaces. Clearly, the m otivation in doing this was to win the competitive 
wars against other established firms, which were ma king similar improvements, rather than to prepare 
for an attack by entrants from below. 18  
 
 
Step 4: New Companies Were Formed, and Markets for the Disruptive Technologies Were Found by 
Trial and Error   
 
New companies, usually including frustrated enginee rs from established firms, were formed to exploit 
the disruptive product architecture. The founders o f the leading 3.5-inch drive maker, Conner 
Peripherals, were disaffected employees from Seagat e and Miniscribe, the two largest 5.25-inch 
manufacturers. The founders of 8-inch drive maker M icropolis came from Pertec, a 14-inch drive 
manufacturer, and the founders of Shugart and Quant um defected from Memorex. 19   
The start-ups, however, were as unsuccessful as the ir former employers in attracting established 
computer makers to the disruptive architecture. Con sequently, they had to find new  customers. The 
applications that emerged in this very uncertain, p robing process were the minicomputer, the desktop 
personal computer, and the laptop computer. In retr ospect, these were obvious markets for hard drives,  
but at the time, their ultimate size and significan ce were highly uncertain. Micropolis was founded 
before the emergence of the desk-side minicomputer and word processor markets in which its products 
came to be used. Seagate began when personal comput ers were simple toys for hobbyists, two years 
before IBM introduced its PC. And Conner Peripheral s got its start before Compaq knew the potential 
size of the portable computer market. The founders of these firms sold their products without a clear 
marketing strategy—essentially selling to whoever w ould buy. Out of what was largely a trial-and-error  
approach to the market, the ultimately dominant app lications for their products emerged. 
 
 
Step 5: The Entrants Moved Upmarket   
 
Once the start-ups had discovered an operating base  in new markets, they realized that, by adopting 
sustaining improvements in new component technologi es, 20  they could increase the capacity of their 
drives at a faster rate than their new market requi red. They blazed trajectories of 50 percent annual 
improvement, fixing their sights on the large, esta blished computer markets immediately above them 
on the performance scale. 
The established firms’ views downmarket and the ent rant firms’ views upmarket were asymmetrical. In 
contrast to the unattractive margins and market siz e that established firms saw when eyeing the new, 
emerging markets for simpler drives, the entrants s aw the potential volumes and margins in the upscale , 
high-performance markets above them as highly attra ctive. Customers in these established markets 
eventually embraced the new architectures they had rejected earlier, because once their needs for 
capacity and speed were met, the new drives’ smalle r size and architectural simplicity made them 
cheaper, faster, and more reliable than the older a rchitectures. Thus, Seagate, which started in the 
desktop personal computer market, subsequently inva ded and came to dominate the minicomputer,  51  engineering workstation, and mainframe computer mar kets for disk drives. Seagate, in turn, was driven 
from the desktop personal computer market for disk drives by Conner and Quantum, the pioneering 
manufacturers of 3.5-inch drives. 
 
 
Step 6: Established Firms Belatedly Jumped on the B andwagon to Defend Their Customer Base   
 
When the smaller models began to invade established  market segments, the drive makers that had 
initially controlled those markets took their proto types off the shelf (where they had been put in Ste p 3) 
and introduced them in order to defend their custom er base in their own market. By this time, of 
course, the new architecture had shed its disruptiv e character and become fully performance-
competitive with the larger drives in the establish ed markets. Although some established manufacturers  
were able to defend their market positions through belated introduction of the new architecture, many 
found that the entrant firms had developed insurmou ntable advantages in manufacturing cost and 
design experience, and they eventually withdrew fro m the market. The firms attacking from value 
networks below brought with them cost structures se t to achieve profitability at lower gross margins. 
The attackers therefore were able to price their pr oducts profitably, while the defending, established  
firms experienced a severe price war. 
For established manufacturers that did succeed in i ntroducing the new architectures, survival was the 
only reward. None ever won a significant share of t he new market; the new drives simply cannibalized 
sales of older products to existing customers. Thus , as of 1991, almost none of Seagate’s 3.5-inch 
drives had been sold to portable/laptop manufacture rs: Its 3.5-inch customers still were desktop 
computer manufacturers, and many of its 3.5-inch dr ives continued to be shipped with frames 
permitting them to be mounted in XT- and AT-class c omputers designed to accommodate 5.25-inch 
drives. 
Control Data, the 14-inch leader, never captured ev en a 1 percent share of the minicomputer market. It  
introduced its 8-inch drives nearly three years aft er the pioneering start-ups did, and nearly all of its 
drives were sold to its existing mainframe customer s. Miniscribe, Quantum, and Micropolis all had the 
same cannibalistic experience when they belatedly i ntroduced disruptive technology drives. They failed  
to capture a significant share of the new market, a nd at best succeeded in defending a portion of thei r 
prior business. 
The popular slogan “stay close to your customers” a ppears not always to be robust advice. 21  One 
instead might expect customers to lead their suppli ers toward sustaining innovations and to provide no  
leadership—or even to explicitly mis lead—in instances of disruptive technology change. 22  
 
 
FLASH MEMORY AND THE VALUE NETWORK   
 
The predictive power of the value network framework  is currently being tested with the emergence of 
flash memory:  a solid-state semiconductor memory technology that  stores data on silicon memory 
chips. Flash differs from conventional dynamic rand om access memory (DRAM) technology in that the 
chip retains the data even when the power is off. F lash memory is a disruptive technology. Flash chips  
consume less than 5 percent of the power that a dis k drive of equivalent capacity would consume, and  52  because they have no moving parts, they are far mor e rugged than disk memory. Flash chips have 
disadvantages, of course. Depending on the amount o f memory, the cost per megabyte of flash can be 
between five and fifty times greater than disk memo ry. And flash chips are not as robust for writing: 
They can only be overwritten a few hundred thousand  times before wearing out, rather than a few 
million times for disk drives. 
The initial applications for flash memory were in v alue networks quite distant from computing; they 
were in devices such as cellular phones, heart moni toring devices, modems, and industrial robots in 
which individually packaged flash chips were embedd ed. Disk drives were too big, too fragile, and 
used too much power to be used in these markets. By  1994, these applications for individually 
packaged flash chips—“socket flash” in industry par lance—accounted for $1.3 billion in industry 
revenues, having grown from nothing in 1987. 
In the early 1990s, the flash makers produced a new  product format, called a flash card: credit card-
sized devices on which multiple flash chips, linked  and governed by controller circuitry, were mounted . 
The chips on flash cards were controlled by the sam e control circuitry, SCSI (Small Computer Standard 
Interface, an acronym first used by Apple), as was used in disk drives, meaning that in concept, a fla sh 
card could be used like a disk drive for mass stora ge. The flash card market grew from $45 million in 
1993 to $80 million in 1994, and forecasters were e yeing a $230 million flash card market by 1996. 
Will flash cards invade the disk drive makers’ core  markets and supplant magnetic memory? If they do, 
what will happen to the disk drive makers? Will the y stay atop their markets, catching this new 
technological wave? Or will they be driven out? 
 
 
The Capabilities Viewpoint   
 
Clark’s concept of technological hierarchies ( see note 4 ) focuses on the skills and technological 
understanding that a company accumulates as the res ult of the product and process technology 
problems it has addressed in the past. In evaluatin g the threat to the disk drive makers of flash memo ry, 
someone using Clark’s framework, or the related fin dings of Tushman and Anderson ( see note 5 ), 
would focus on the extent to which disk drive maker s have historically developed expertise in 
integrated circuit design and in the design and con trol of devices composed of multiple integrated 
circuits. These frameworks would lead us to expect that drive makers will stumble badly in their 
attempts to develop flash products if they have lim ited expertise in these domains and will succeed if  
their experience and expertise are deep. 
On its surface, flash memory involves radically dif ferent electronics  technology than the core 
competence of disk drive makers (magnetics and mech anics). But such firms as Quantum, Seagate, and 
Western Digital have developed deep expertise in cu stom integrated circuit design through embedding 
increasingly intelligent control circuitry and cach e memory in their drives. Consistent with the pract ice 
in much of the ASIC (application-specific integrate d circuit) industry, their controller chips are 
fabricated by independent, third-party fabricators that own excess clean room semiconductor 
processing capacity. 
Each of today’s leading disk drive manufacturers go t its start by designing drives, procuring 
components from independent suppliers, assembling t hem either in its own factories or by contract, and  
then selling them. The flash card business is very similar. Flash card makers design the card and  53  procure the component flash chips; they design and have fabricated an interface circuit, such as SCSI,  
to govern the drive’s interaction with the computin g device; they assemble them either in-house or by 
contract; and they then market them. 
In other words, flash memory actually builds upon  important competencies that many drive makers 
have developed. The capabilities viewpoint, therefo re, would lead us to expect that disk drive makers 
may not  stumble badly in bringing flash storage technology  to the market. More specifically, the 
viewpoint predicts that those firms with the deepes t experience in IC design—Quantum, Seagate, and 
Western Digital—will bring flash products to market  quite readily. Others, which historically 
outsourced much of their electronic circuit design,  may face more of a struggle. 
This has, indeed, been the case to date. Seagate en tered the flash market in 1993 via its purchase of a 
25 percent equity stake in Sundisk Corporation. Sea gate and SunDisk together designed the chips and 
cards; the chips were fabricated by Matsushita, and  the cards were assembled by a Korean 
manufacturer, Anam. Seagate itself marketed the car ds. Quantum entered with a different partner, 
Silicon Storage Technology, which designed the chip s that were then fabricated and assembled by 
contract. 
 
 
The Organizational Structure Framework   
 
Flash technology is what Henderson and Clark would call radical  technology. Its product architecture 
and fundamental technological concept are novel com pared to disk drives. The organizational structure 
viewpoint would predict that, unless they created o rganizationally independent groups to design flash 
products, established firms would stumble badly. Se agate and Quantum did, indeed, rely on 
independent groups and did develop competitive prod ucts. 
 
 
The Technology S-Curve Framework   
 
The technology S-curve is often used to predict whe ther an emerging technology is likely to supplant 
an established one. The operative trigger is the sl ope of the curve of the established technology. If the 
curve has passed its point of inflection, so that i ts second derivative is negative (the technology is  
improving at a decreasing rate), then a new technol ogy may emerge to supplant the established one. 
Figure 2.7 shows that the S-curve for magnetic disk  recording still has not hit its point of inflectio n: 
Not only is the areal density improving, as of 1995 , it was improving at an increasing  rate. 
The S-curve framework would lead us to predict, the refore, that whether or not established disk drive 
companies possess the capability to design flash ca rds, flash memory will not pose a threat to them 
until the magnetic memory S-curve has passed its po int of inflection and the rate of improvement in 
density begins to decline. 
 
   54  Figure 2.7 Improvements in Areal Density of New Disk Drives (D ensities in Millions of Bits per 
Square Inch)  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
Insights from the Value Network Framework   
 
The value network framework asserts that none of th e foregoing frameworks is a sufficient predictor of  
success. Specifically, even where established firms  did not possess the requisite technological skills  to 
develop a new technology, they would marshal the re sources to develop or acquire them if their 
customers demanded it. Furthermore, the value netwo rk suggests that technology S-curves are useful 
predictors only with sustaining technologies. Disru ptive technologies generally improve at a parallel 
pace with established ones—their trajectories do no t intersect. The S-curve framework, therefore, asks  
the wrong question  when it is used to assess disruptive technology. W hat matters instead is whether the 
disruptive technology is improving from below along  a trajectory that will ultimately intersect with 
what the market  needs. 
The value network framework would assert that even though firms such as Seagate and Quantum are 
able technologically  to develop competitive flash memory products, whet her they invest the resources 
and managerial energy to build strong market positi ons in the technology will depend on whether flash 
memory can be initially valued and deployed within the value networks in which the firms make their 
money. 
As of 1996, flash memory can only be used in value networks different from those of the typical disk 
drive maker. This is illustrated in Figure 2.8, whi ch plots the average megabytes of capacity of flash  
cards introduced each year between 1992 and 1995, c ompared with the capacities of 2.5- and 1.8-inch 
drives and with the capacity demanded in the notebo ok computer market. Even though they are rugged 
and consume little power, flash cards simply don’t yet pack the capacity to become the main mass 
storage devices in notebook computers. And the pric e of the flash capacity required to meet what the 
low end of the portable computing market demands (a bout 350 MB in 1995) is too high: The cost of 
that much flash capacity would be fifty times highe r than comparable disk storage. 23  The low power 
consumption and ruggedness of flash certainly have no value and command no price premium on the 
desktop. There is, in other words, no way to use fl ash today in the markets where firms such as 
Quantum and Seagate make their money.  55   
  
Figure 2.8 Comparison of Disk Drive Memory Capacity to Flash C ard Memory Capacity  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
  
Hence, because flash cards are being used in market s completely different from those Quantum and 
Seagate typically engage—palmtop computers, electro nic clipboards, cash registers, electronic 
cameras, and so on—the value network framework woul d predict that firms similar to Quantum and 
Seagate are not  likely to build market-leading positions in flash memory. This is not because the 
technology is too difficult or their organizational  structures impede effective development, but becau se 
their resources will become absorbed in fighting fo r and defending larger chunks of business in the 
mainstream disk drive value networks in which they currently make their money. 
Indeed, the marketing director for a leading flash card producer observed, “We’re finding that as hard  
disk drive manufacturers move up to the gigabyte ra nge, they are unable to be cost competitive at the 
lower capacities. As a result, disk drive makers ar e pulling out of markets in the 10 to 40 megabyte 
range and creating a vacuum into which flash can mo ve.” 24  
The drive makers’ efforts to build flash card busin esses have in fact floundered. By 1995, neither 
Quantum nor Seagate had built market shares of even  1 percent of the flash card market. Both 
companies subsequently concluded that the opportuni ty in flash cards was not yet substantial enough, 
and withdrew their products from the market the sam e year. Seagate retained its minority stake in 
SunDisk (renamed SanDisk), however, a strategy whic h, as we shall see, is an effective way to address 
disruptive technology.  56   
 
IMPLICATIONS OF THE VALUE NETWORK FRAMEWORK FOR INN OVATION   
 
Value networks strongly define and delimit what com panies within them can and cannot do. This 
chapter closes with five propositions about the nat ure of technological change and the problems 
successful incumbent firms encounter, which the val ue network perspective highlights. 
1. The context, or value network, in which a firm c ompetes has a profound influence on its ability to 
marshal and focus the necessary resources and capab ilities to overcome the technological and 
organizational hurdles that impede innovation. The boundaries of a value network are determined by a 
unique definition of product performance—a rank-ord ering of the importance of various performance 
attributes differing markedly from that employed in  other systems-of-use in a broadly defined industry . 
Value networks are also defined by particular cost structures inherent in addressing customers’ needs 
within the network. 
2. A key determinant of the probability of an innov ative effort’s commercial success is the degree to 
which it addresses the well-understood needs of kno wn actors within the value network. Incumbent 
firms are likely to lead their industries in innova tions of all sorts—architecture and components—that  
address needs within their value network, regardles s of intrinsic technological character or difficult y. 
These are straightforward innovations; their value and application are clear. Conversely, incumbent 
firms are likely to lag in the development of techn ologies—even those in which the technology 
involved is intrinsically simple—that only address customers’ needs in emerging value networks. 
Disruptive innovations are complex because their va lue and application are uncertain, according to the  
criteria used by incumbent firms. 
3. Established firms’ decisions to ignore technolog ies that do not address their customers’ needs 
become fatal when two distinct trajectories interac t. The first defines the performance demanded over 
time within a given value network, and the second t races the performance that technologists are able t o 
provide within a given technological paradigm. The trajectory of performance improvement that 
technology is able to provide may have a distinctly  different slope from the trajectory of performance  
improvement demanded in the system-of-use by downst ream customers within any given value 
network. When the slopes of these two trajectories are similar, we expect the technology to remain 
relatively contained within its initial value netwo rk. But when the slopes differ, new technologies th at 
are initially performance-competitive only within e merging or commercially remote value networks 
may migrate into other networks, providing a vehicl e for innovators in new networks to attack 
established ones. When such an attack occurs, it is  because technological progress has diminished the 
relevancce of differences in the rank-ordering of p erformance attributes across different value 
networks. For example, the disk drive attributes of  size and weight were far more important in the 
desktop computing value network than they were in t he mainframe and minicomputer value networks. 
When technological progress in 5.25-inch drives ena bled manufacturers to satisfy the attribute 
prioritization in the mainframe and minicomputer ne tworks, which prized total capacity and high 
speed, as well as  that in the desktop network, the boundaries betwee n the value networks ceased to be 
barriers to entry for 5.25-inch drive makers. 
4. Entrant firms have an attacker’s advantage over established firms in those innovations—generally 
new product architectures involving little new tech nology per se—that disrupt or redefine the level, 
rate, and direction of progress in an established t echnological trajectory. This is so because such 
technologies generate no value within the establish ed network. The only way established firms can lead   57  in commercializing such technologies is to enter th e value network in which they create value. As 
Richard Tedlow noted in his history of retailing in  America (in which supermarkets and discount 
retailing play the role of disruptive technologies) , “the most formidable barrier the established firm s 
faced is that they did not want to do this.” 25  
5. In these instances, although this “attacker’s ad vantage” is associated  with a disruptive technology 
change, the essence of the attacker’s advantage is in the ease with which entrants, relative to 
incumbents, can identify and make strategic commitm ents to attack and develop emerging market 
applications, or value networks. At its core, there fore, the issue may be the relative flexibility of 
successful established firms versus entrant firms t o change strategies and cost structures,  not 
technologies. 
These propositions provide new dimensions for analy zing technological innovation. In addition to the 
required capabilities inherent in new technologies and in the innovating organization, firms faced wit h 
disruptive technologies must examine the implicatio ns of innovation for their relevant value networks.  
The key considerations are whether the performance attributes implicit in the innovation will be value d 
within the networks already served by the innovator ; whether other networks must be addressed or new 
ones created in order to realize value for the inno vation; and whether market and technological 
trajectories may eventually intersect, carrying tec hnologies that do not address customers’ needs toda y 
to squarely address their needs in the future. 
These considerations apply not simply to firms grap pling with the most modern technologies, such as 
the fast-paced, complex advanced electronic, mechan ical, and magnetics technologies covered in this 
chapter. Chapter 3 examines them in the context of a very different industry: earthmoving equipment. 
 
 
NOTES   
 
1.  See Rebecca M. Henderson and Kim B. Clark, “Archit ectural Innovation: The Reconfiguration of 
Existing Systems and the Failure of Established Fir ms” Administrative Science Quarterly  (35), 1990, 
9–30.  
2.  Tracy Kidder, The Soul of a New Machine  (New York: Avon Books, Inc., 1981).  
3.  A few scholars have sought to measure the proporti on of technological progress attributable to 
radical versus incremental advances. In an empirica l study of a series of novel processes in petroleum  
refining, for example, John Enos found that half th e economic benefits of new technology came from 
process improvements introduced after a new technol ogy was commercially established. See J. L. 
Enos, “Invention and Innovation in the Petroleum Re fining Industry,” in The Rate and Direction of 
Inventive Activity: Economic and Social Factors,  National Bureau of Economic Research Report 
(Princeton, NJ: Princeton University Press, 1962), 299–321. My study of the disk drive industry has 
shown the same result. Half the advance in areal de nsity (megabits per square inch of disk surface) ca n 
be attributed to new component technologies and hal f to incremental improvements in existing 
components and refinements in system design. See Cl ayton M. Christensen, “Exploring the Limits of 
the Technology S-Curve,” Production and Operations Management  (1), 1992, 334–366.  
4.  See Kim B. Clark, “The Interaction of Design Hiera rchies and Market Concepts in Technological 
Evolution,” Research Policy  (14), 1985, 235–251. Clark suggests, for example, that the early selections 
by automotive engineers of gasoline over steam or e lectrically powered engines defined the technical 
agenda for subsequent generations of engineers, who  consequently did not pursue refinements in 
electric or steam propulsion. Clark has thus shown that the design skills and technological knowledge  58  resident in companies today result from the cumulat ive choices engineers have made of what to tackle 
versus what to leave alone. Clark posits that techn ological improvements requiring that companies 
build upon or extend an existing cumulative body of  knowledge favor an industry’s established firms. 
Conversely, when changes require a completely diffe rent body of knowledge, established firms will be 
at a disadvantage compared to firms that had alread y accumulated a different hierarchically structured  
body of knowledge, most likely in another industry.   
5.  See, for example, Michael L. Tushman and Philip An derson, “Technological Discontinuities and 
Organizational Environments,” Administrative Science Quarterly  (31), 1986, 439–465; and Philip 
Anderson and Michael Tushman, “Technological Discon tinuities and Dominant Designs,” 
Administrative Science Quarterly  (35), 1990, 604–633.  
6.  The concept of value network  builds on Giovanni Dosi’s concept of technological paradigms.  See 
Giovanni Dosi, “Technological Paradigms and Technol ogical Trajectories,” Research Policy  (11), 
1982, 147–162. Dosi characterizes a technological p aradigm as a “pattern of solution of selected 
technological problems, based on selected principle s derived from natural sciences and on selected 
material technologies” (152). New paradigms represe nt discontinuities in trajectories of progress as 
defined within earlier paradigms. They tend to rede fine the very meaning of progress, and point 
technologists toward new classes of problems as the  targets of ensuing normal technology 
development. The question examined by Dosi—how new technologies are selected and retained—is 
closely related to the question of why firms succee d or fail as beneficiaries of such changes.  
7.  Value network, as presented here, draws heavily on  ideas I developed jointly with Professor Richard 
S. Rosenbloom and which are summarized in two journ al articles: Clayton M. Christensen and Richard 
S. Rosenbloom, “Explaining the Attacker’s Advantage : The Technological Paradigms, Organizational 
Dynamics, and the Value Network,” Research Policy  (24), 1995, 233–257; and Richard S. Rosenbloom 
and Clayton M. Christensen, “Technological Disconti nuities, Organizational Capabilities, and Strategic  
Commitments,” Industrial and Corporate Change  (3), 1994, 655–685. I am heavily indebted to 
Professor Rosenbloom for his contributions to the d evelopment of these perspectives.  
8.  See D. L. Marples, “The Decisions of Engineering D esign,” IEEE Transactions on Engineering 
Management  EM8, 1961, 55–71; and C. Alexander, Notes on the Synthesis of Form  (Cambridge, MA: 
Harvard University Press, 1964).  
9.  On this point, too, correspondence between the con cept of the value network and Dosi’s concept of 
technological paradigms is strong. (See note 6.) Th e scope and boundaries of a value network are 
defined by the dominant technological paradigm and the corresponding technological trajectory 
employed at the higher levels of the network. As Do si suggests, value  can be defined as a function of 
the dominant technological paradigm in the ultimate  system of use in the value network.  
10.  Michael Porter, Competitive Advantage  (New York: The Free Press, 1985).  
11.  A more complete report of this analysis can be fou nd in chapter 7 of Clayton M. Christensen, The 
Innovator’s Challenge: Understanding the Influence of Market Environment on Processes of 
Technology Development in the Rigid Disk Drive Indu stry,  thesis, Harvard University Graduate School 
of Business Administration, 1992.  
12.  D. Sahal, Patterns of Technological Innovation  (London: Addison Wesley, 1981).  
13.  The most widely read proponent of this view is Ric hard Foster; see, for example, his Innovation: 
The Attacker’s Advantage  (New York: Summit Books, 1986).  
14.  The insights summarized here are articulated more completely in C. M. Christensen, “Exploring the 
Limits of the Technology S-Curve,” Production and Operations Management  (1), 1992, 334–366.  
15.  A fuller account of similar decisions made in othe r firms can be found in Clayton M. Christensen, 
The Innovator’s Challenge: Understanding the Influe nce of Market Environment on Processes of 
Technology Development in the Rigid Disk Drive Indu stry, thesis, Harvard University Graduate School 
of Business Administration, 1992.  
16.  This procedure is consistent with Robert Burgelman ’s observation that one of the greatest 
difficulties encountered by corporate entrepreneurs  is in finding the right “beta test sites,” where  59  products can be interactively developed and refined  with customers. Generally, the entrée to the 
customer was provided by the salesperson who sold t he firm’s established product lines. This helped 
the firm develop new products for established marke ts, but not identify new applications for its new 
technology. See Robert Burgelman and Leonard Sayles , Inside Corporate Innovation  (New York: The 
Free Press, 1986) 76–80. Professor Rebecca Henderso n pointed out to me that this tendency always to 
take new technologies to mainstream customers refle cts a rather narrow marketing  competence—that 
although many scholars tend to frame the issue as o ne of technological competence, such inability to 
find new markets for new technologies may be a firm ’s most serious handicap in innovation.  
17.  Voice coil motors were more expensive than the ste pper motors that Seagate had previously used. 
While not new to the market, they were new to Seaga te.  
18.  This is consistent with the findings reported by A rnold Cooper and Dan Schendel in “Strategic 
Responses to Technological Threats,” Business Horizons  (19), February, 1976, 61–69.  
19.  Ultimately, nearly all North American disk drive m anufacturers can trace their founders’ genealogy 
to IBM’s San Jose division, which developed and man ufactured its magnetic recording products. See 
Clayton M. Christensen, “The Rigid Disk Drive Indus try: A History of Commercial and Technological 
Turbulence,” Business History Review  (67), Winter, 1993, 531–588.  
20.  In general, these component technologies were deve loped within the largest of the established firms 
that dominated the established markets above these entrants. This is because new components generally 
(but not always) have a sustaining impact on techno logy trajectories. These high-end, established firm s 
typically were engaged in the hottest pursuit of su staining innovations.  
21.  The research of Eric von Hippel, frequently cited as evidence of the value of listening to customers,  
indicates that customers originate a large majority  of new product ideas (see Eric von Hippel, The 
Sources of Innovation  [New York: Oxford University Press, 1988]). One fr uitful avenue for future 
research would be to revisit von Hippel’s data in l ight of the framework presented here. The value 
network framework would predict that the innovation s toward which the customers in von Hippel’s 
study led their suppliers would have been sustainin g innovations. We would expect disruptive 
innovations to have come from other sources.  
22.  Henderson saw similar potential danger for being m isled by customers in her study of 
photolithographic aligner equipment manufacturers. See Rebecca M. Henderson, “Keeping Too Close 
to Your Customers,” Massachusetts Institute of Tech nology Sloan School of Management working 
paper, 1993.  
23.  Many industry observers have noted that there seem s to be a floor on the cost of making a disk 
drive, somewhere around $120 per device, below whic h even the best manufacturers cannot plunge. 
This is the basic cost of designing, producing, and  assembling the requisite components. Drive makers 
keep reducing costs per megabyte by continuously in creasing the number of megabytes available in 
that basic $120 box. The effect of this floor on th e competition between disk drives and flash cards m ay 
be profound. It means that in low-capacity applicat ions, as the price of flash memory falls, flash wil l 
become cost-competitive with disk memory. The front ier above which magnetic disk drives have lower 
costs per megabyte than flash will keep moving upma rket, in a manner perfectly analogous to the 
upmarket movement of larger disk drive architecture s. Experts predicted, in fact, that by 1997, a 40 M B 
flash card would be priced comparably to a 40 MB di sk drive.  
24.  Lewis H. Young, “Samsung Banks on Tiny Flash Cell, ” Electronic Business Buyer  (21), July, 
1995, 28.  
25.  Richard Tedlow, New and Improved: A History of Mass Marketing in Am erica  (Boston: Harvard 
Business School Press, 1994). 
 
 
  60   
CHAPTER THREE  
Disruptive Technological  
Change in the Mechanical  
Excavator Industry 
 
 
 
Excavators and their steam shovel predecessors are huge pieces of capital equipment sold to excavation  
contractors. While few observers consider this a fa st-moving, technologically dynamic industry, it has  
points in common with the disk drive industry: Over  its history, leading firms have successfully 
adopted a series of sustaining  innovations, both incremental and radical, in comp onents and 
architecture, but almost the entire population of m echanical shovel manufacturers was wiped out by a 
disruptive technology—hydraulics—that the leaders’ customers and their economic structure had 
caused them initially to ignore. Although in disk d rives such invasions of established markets occurre d 
within a few years of the initial emergence of each  disruptive technology, the triumph of hydraulic 
excavators took twenty years. Yet the disruptive in vasion proved just as decisive and difficult to 
counter in excavators as those in the disk drive in dustry. 1  
 
LEADERSHIP IN SUSTAINING TECHNOLOGICAL CHANGE   
 
From William Smith Otis’ invention of the steam sho vel in 1837 through the early 1920s, mechanical 
earthmoving equipment was steam-powered. A central boiler sent steam through pipes to small steam 
engines at each point where power was required in t he machine. Through a system of pulleys, drums, 
and cables, these engines manipulated frontward-sco oping buckets, as illustrated in Figure 3.1. 
Originally, steam shovels were mounted on rails and  used to excavate earth in railway and canal 
construction. American excavator manufacturers were  tightly clustered in northern Ohio and near 
Milwaukee. 
 
  
Figure 3.1 Cable-Actuated Mechanical Shovel Manufactured by Os good General  
  61  
 
 
Source: Osgood General photo in Herbert L. Nichols, Jr., Moving the Earth: The Workbook of 
Excavation (Greenwich, CT: North Castle, 1955).  
 
  
In the early 1920s, when there were more than thirt y-two steam shovel manufacturers based in the 
United States, the industry faced a major technolog ical upheaval, as gasoline-powered engines were 
substituted for steam power. 2 This transition to gasoline power falls into the c ategory that Henderson 
and Clark label radical technological transition. T he fundamental technological concept in a key 
component (the engine) changed from steam to intern al combustion, and the basic architecture of the 
product changed. Where steam shovels used steam pre ssure to power a set of steam engines to extend 
and retract the cables that actuated their buckets,  gasoline shovels used a single engine and a very 
different system of gearing, clutches, drums, and b rakes to wind and unwind the cable. Despite the 
radical nature of the technological change, however , gasoline technology had a sustaining  impact on 
the mechanical excavator industry. Gasoline engines  were powerful enough to enable contractors to 
move earth faster, more reliably, and at lower cost  than any but the very largest steam shovels. 
The leading innovators in gasoline engine technolog y were the industry’s dominant firms, such as 
Bucyrus, Thew, and Marion. Twenty-three of the twen ty-five largest makers of steam shovels 
successfully negotiated the transition to gasoline power. 3 As Figure 3.2 shows, there were a few entrant 
firms among the gasoline technology leaders in the 1920s, but the established firms dominated this 
transition. 
Beginning in about 1928, the established manufactur ers of gasoline-powered shovels initiated the next 
major, but less radical, sustaining technological t ransition—to shovels powered by diesel engines and 
electric motors. A further transition, made after W orld War II, introduced the arched boom design, 
which allowed longer reach, bigger buckets, and bet ter down-reaching flexibility. The established firm s 
continued to embrace and succeed with each of these  innovations. 
 
  
Figure 3.2 Manufacturers of Gasoline-Powered Cable Shovels, 19 20-1934  
  62  
 
 
Source: Data are from the Historical Construction Equipment  Association and from The Thomas 
Register, various years.  
 
  
Excavation contractors themselves actually pioneere d a number of other important sustaining 
innovations, first modifying their own equipment in  the field to make it perform better and then 
manufacturing excavators incorporating those featur es to sell to the broader market. 4  
 
THE IMPACT OF DISRUPTIVE HYDRAULICS TECHNOLOGY   
 
The next major technological change precipitated wi despread failure in the industry. Beginning shortly  
after World War II and continuing through the late 1960s, while the dominant source of power 
remained the diesel engine, a new mechanism emerged  for extending and lifting the bucket: 
hydraulically actuated systems replaced the cable-a ctuated systems. Only four of the thirty or so 
established manufacturers of cable-actuated equipme nt in business in the 1950s (Insley, Koehring, 
Little Giant, and Link Belt) had successfully trans formed themselves into sustainable hydraulic 
excavator manufacturers by the 1970s. A few others survived by withdrawing into making such 
equipment as huge, cable-actuated draglines for str ip mining and dredging. 5 Most of the others failed. 
The firms that overran the excavation equipment ind ustry at this point were all entrants into the 
hydraulics generation: J. I. Case, John Deere, Drot t, Ford, J. C. Bamford, Poclain, International 
Harvester, Caterpillar, O & K, Demag, Leibherr, Kom atsu, and Hitachi. 6 Why did this happen? 
 
 
Performance Demanded in the Mechanical Excavator Ma rket   
 
Excavators are one of many types of earthmoving equ ipment. Some equipment, such as bulldozers, 
loaders, graders, and scrapers, essentially push, s mooth, and lift earth. Excavators 7 have been used to 
dig holes and trenches, primarily in three markets:  first and largest, the general excavation market, 
composed of contractors who dig holes for basements  or civil engineering projects such as canal 
construction; second, sewer and piping contractors,  who generally dig long trenches; and third, open p it 
or strip mining. In each of these markets, contract ors have tended to measure the functionality of  63  mechanical excavators by their reach or extension d istance and by the cubic yards of earth lifted in a  
single scoop. 8  
In 1945, sewer and piping contractors used machines  whose bucket capacity averaged about 1 cubic 
yard (best for digging relatively narrow trenches),  while the average general excavation contractor us ed 
excavators that hefted 2 1/2 cubic yards per scoop and mining contractors used shovels holding about 5  
cubic yards. The average bucket capacity used in ea ch of these markets increased at about 4 percent pe r 
year, a rate of increase constrained by other facto rs in the broader system-of-use. The logistical 
problems of transporting large machines into and ou t of typical construction sites, for example, helpe d 
limit the rate of increase demanded by contractors.  
 
 
The Emergence and Trajectory of Improvement of Hydr aulic Excavation   
 
The first hydraulic excavator was developed by a Br itish company, J. C. Bamford, in 1947. Similar 
products then emerged simultaneously in several Ame rican companies in the late 1940s, among them, 
the Henry Company, of Topeka, Kansas, and Sherman P roducts, Inc., of Royal Oak, Michigan. The 
approach was labeled “Hydraulically Operated Power Take-Off,” yielding an acronym that became the 
name of the third entrant to hydraulic excavating i n the late 1940s, HOPTO. 9  
Their machines were called backhoes  because they were mounted on the back of industria l or farm 
tractors. Backhoes excavated by extending the shove l out, pushing it down into the earth, 10  curling or 
articulating the shovel under the slice of earth, a nd lifting it up out of the hole. Limited by the po wer 
and strength of available hydraulic pumps’ seals, t he capacity of these early machines was a mere 1/4 
cubic yard, as graphed in Figure 3.3. Their reach w as also limited to about six feet. Whereas the best  
cable excavators could rotate a full 360 degrees on  their track base, the most flexible backhoes could  
rotate only 180 degrees. 
 
  
Figure 3.3 Disruptive Impact of Hydraulics Technology in the M echanical Excavator Market  
 
 
 
Source: Data are from the Historical Construction Equipment  Association.   64   
  
Because their capacity was so small and their reach  so short, hydraulic excavators were of no use to 
mining, general excavation, or sewer contractors, w ho were demanding machines with buckets that 
held 1 to 4 cubic yards. As a result, the entrant f irms had to develop a new application for their 
products. They began to sell their excavators as at tachments for the back of small industrial and farm  
tractors made by Ford, J. I. Case, John Deere, Inte rnational Harvester, and Massey Ferguson. Small 
residential contractors purchased these units to di g narrow ditches from water and sewer lines in the 
street to the foundations of houses under construct ion. These very small jobs had never warranted the 
expense or time required to bring in a big, impreci se, cable-actuated, track-driven shovel, so the 
trenches had always been dug by hand. Hydraulic bac khoes attached to highly mobile tractors could do 
these jobs in less than an hour per house, and they  became extremely popular with contractors building  
large tract subdivisions during the housing booms t hat followed World War II and the Korean War. 
These early backhoes were sold through tractor and implement dealerships accustomed to dealing with 
small customers. 
The early users of hydraulic excavators were, in a word, very  different from the mainstream customers 
of the cable shovel manufacturers—in size, in needs , and in the distribution channels through which 
they bought. They constituted a new value network f or mechanical excavation. Interestingly, just as th e 
performance of smaller-architecture disk drives was  measured in different metrics than the performance  
of large drives (weight, ruggedness, and power cons umption versus capacity and speed), the 
performance of the first backhoes was measured diff erently from the performance of cable-actuated 
equipment. The metrics featured most prominently in  early product literature of hydraulic backhoes 
were shovel width  (contractors wanted to dig narrow, shallow trenche s) and the speed and 
maneuverability of the tractor.  Figure 3.4, excerpted from an early product brochu re from Sherman 
Products for its “Bobcat” hydraulic backhoe, illust rates this. Sherman called its Bobcat a “digger,” 
showed it operating in tight quarters, and claimed it could travel over sod with minimum damage. The 
Bobcat was mounted on a Ford tractor. (Ford subsequ ently acquired the Sherman Bobcat line.) The 
featured attributes, of course, were simply irrelev ant to contractors whose bread was buttered by big 
earthmoving projects. These differences in the rank -ordering of performance attributes defined the 
boundaries of the industry’s value networks. 
 
  
Figure 3.4 Hydraulic Backhoe Manufactured by Sherman Products  
  65  
 
 
Source: Brochure from Sherman Products, Inc., Royal Oak, Mi chigan, early 1950s.  
 
  
The solid line in Figure 3.3 charts the rate of imp rovement in bucket size that hydraulics engineers 
were able to provide in the new excavator architect ure. The maximum available bucket size had 
reached 3/8 cubic yard by 1955, 1/2 cubic yard by 1 960, and 2 cubic yards by 1965. By 1974, the 
largest hydraulic excavators had the muscle to lift  10 cubic yards. This trajectory of improvement, 
which was far more rapid than the rate of improveme nt demanded in any of the excavator markets, 
carried this disruptive hydraulics technology upwar d from its original market through the large, 
mainstream excavation markets. The use of hydraulic  excavators in general contracting markets was 
given a boost in 1954 when another entrant firm in Germany, Demag, introduced a track-mounted 
model that could rotate on its base a full 360 degr ees. 
 
 
THE RESPONSE TO HYDRAULICS BY THE ESTABLISHED EXCAV ATOR 
MANUFACTURERS   
 
Just as Seagate Technology was one of the first fir ms to develop prototype 3.5-inch drives, Bucyrus 
Erie, the leading cable shovel maker, was keenly aw are of the emergence of hydraulic excavating 
technology. By 1950 (about two years after the firs t backhoe appeared) Bucyrus purchased a fledgling 
hydraulic backhoe company, the Milwaukee Hydraulics  Corporation. Bucyrus faced precisely the same 
problem in marketing its hydraulic backhoe as Seaga te had faced with its 3.5-inch drives: Its most 
powerful mainstream customers had no use for it. 
Bucyrus Erie’s response was a new product, introduc ed in 1951, called the “Hydrohoe.” Instead of 
using three hydraulic cylinders, it used only two, one to curl the shovel into the earth and one to 
“crowd” or draw the shovel toward the cab; it used a cable mechanism to lift the shovel. The Hydrohoe  66  was thus a hybrid of the two technologies, reminisc ent of the early transoceanic steamships outfitted 
with sails. 11  There is no evidence, however, that the Hydrohoe’s  hybrid design resulted from Bucyrus 
engineers’ being “stuck” in some sort of cable-base d engineering paradigm. Rather, the cable lift 
mechanism was the only  viable way at that time, based on the state of hyd raulics technology, to give 
the Hydrohoe the bucket capacity and reach that Buc yrus marketers thought they needed to appeal to 
their existing customers’ needs. 
Figure 3.5 presents an excerpt from an early Hydroh oe product brochure. Note the differences from 
Sherman’s marketing approach: Bucyrus labeled the H ydrohoe a “dragshovel,” showed it in an open 
field, and claimed it could “get a heaping load on every pass”—all intended to appeal to general 
excavation contractors. Rather than commercialize t he disruptive technology in the value network in 
which the current attributes of hydraulics were pri zed, Bucyrus tried to adapt the technology to fit i ts 
own value network. Despite this attempt, the Hydroh oe was still too limited in capacity and reach and 
did not sell well to Bucyrus’ customers. Bucyrus ke pt its Hydrohoe on the market for over a decade, 
attempting periodically to upgrade its performance to make it acceptable to its customers, but the 
machine was never commercially successful. Ultimate ly, the company returned to the cable shovels 
that its customers needed. 
 
  
Figure 3.5 Hydrohoe Manufactured by Bucyrus Erie  
 
 
 
Source: Brochure from Bucyrus Erie Company, South Milwaukee , Wisconsin, 1951.  
 
   67  Bucyrus Erie was the only maker of cable-actuated s hovels known to have launched a hydraulic 
excavator between 1948 and 1961: All of the other m anufacturers continued serving their established 
customers, well and prosperously. 12  In fact, the largest makers of cable-actuated exca vators, Bucyrus 
Erie and Northwest Engineering, logged record profi ts until 1966—the point at which the disruptive 
hydraulics technology had squarely intersected with  customers’ needs in the sewer and piping segment. 
This is typical of industries facing a disruptive t echnology: The leading firms in the established 
technology remain financially strong until the disr uptive technology is, in fact, in the midst of thei r 
mainstream market. 
Between 1947 and 1965, twenty-three companies enter ed the mechanical excavation market with 
hydraulic products. Figure 3.6, which measures the total number of active entrants and established 
firms offering hydraulic excavators (net of the com panies that had exited), shows how completely the 
entrants dominated the hydraulic excavator market. 
In the 1960s, some of the strongest cable shovel ma kers introduced shovels with hydraulics. Almost all  
of these models were hybrids, however, like Bucyrus  Erie’s Hydrohoe, generally employing a 
hydraulic cylinder to articulate or curl the bucket  and using cables to extend the bucket out and to l ift 
the boom. When used in this way in the 1960s, hydra ulics had a sustaining impact on the established 
manufacturers’ products, improving their performanc e in the mainstream value networks. Some of the 
methods that engineers found to use hydraulics on t he cable excavators were truly ingenious. All of th is 
innovative energy, however, was targeted at existin g customers. 
The strategies employed by the excavator manufactur ers during this period highlight an important 
choice that confronts companies encountering disrup tive technological change. In general, the 
successful entrants accepted the capabilities of hy draulics technology in the 1940s and 1950s as a giv en 
and cultivated new market applications in which the  technology, as it existed, could create value. And  
as a general rule, the established firms saw the si tuation the other way around: They took the market’s  
needs as the given. They consequently sought to ada pt or improve the technology in ways that would 
allow them to market the new technology to their ex isting customers as a sustaining improvement. The 
established firms steadfastly focused their innovat ive investments on their customers. Subsequent 
chapters will show that this strategic choice is pr esent in most instances of disruptive innovation. 
Consistently, established firms attempt to push the  technology into their established markets, while t he 
successful entrants find a new market that values t he technology. 
 
  
Figure 3.6 Manufacturers of Hydraulic Excavators, 1948-1965  
  68  
 
 
Source: Data are from the Historical Construction Equipment  Association.  
 
  
Hydraulics technology ultimately did  progress to the point where it could address the n eeds of 
mainstream excavation contractors. That progress wa s achieved, however, by the entrant companies, 
who had first found a market for the initial capabi lities of the technology, accumulated design and 
manufacturing experience in that market, and then u sed that commercial platform to attack the value 
networks above them. The established firms lost thi s contest. Only four cable excavator companies—
Insley, Koehring, Little Giant, and Link Belt—remai ned as viable suppliers to excavation contractors 
by successfully but belatedly introducing lines of hydraulic excavators to defend their markets. 13   
Aside from these, however, the other leading manufa cturers of big cable machines in the mainstream 
excavation markets never introduced a commercially successful hydraulic excavator. Although some 
had employed hydraulics to a modest degree as a buc ket-curling mechanism, they lacked the design 
expertise and volume-based manufacturing cost posit ion to compete as hydraulics invaded the 
mainstream. By the early 1970s, all of these firms had been driven from the sewer, piping, and general  
excavation markets by the entrants, most of which h ad refined their technological capabilities initial ly 
in the small-contractor market. 14  
This contrast in strategies for profiting from chan ge characterizes the approaches employed by entrant  
and established firms in many of the other industri es affected by disruptive technologies—particularly  
disk drives, steel, computers, and electric cars. 
 
 
THE CHOICE BETWEEN CABLE AND HYDRAULICS   
 
In the trajectory map of Figure 3.3, when hydraulic s technology became capable of addressing the 
bucket-size needs of sewer and piping contractors ( and a similar trajectory could be sketched for arm-
reach), the competitive dynamics in the industry ch anged, and the mainstream excavation contractors 
changed the criteria by which they purchased their equipment. Even today, the cable-actuated 
architecture can attain much longer reach and great er lift than can hydraulic excavators: They have 
roughly parallel technology trajectories. But once both  cable- and hydraulics-actuated systems could 
satisfy mainstream market requirements, excavation contractors could no longer base their choice of  69  equipment on which had longer reach and greater buc ket capacity. Both were good enough, and the fact 
that cable was better ceased to have competitive re levance. 
Contractors found, however, that hydraulic machines  were much less prone to breakdowns than cable-
actuated excavators. In particular, those who had e xperienced the life-threatening snap of a cable whi le 
hefting a heavy bucket embraced reliable hydraulics  quickly, as soon as it was capable of doing the jo b. 
Once both technologies were good enough in the basi c capabilities demanded, therefore, the basis of 
product choice in the market shifted to reliability . Sewer and piping contractors began adopting 
hydraulic equipment rapidly beginning in the early 1960s, and general excavation contractors followed 
later in the decade. 
 
 
CONSEQUENCES AND IMPLICATIONS OF THE HYDRAULICS ERU PTION   
 
What went wrong within the companies that made cabl e-actuated excavators? Clearly, with the benefit 
of hindsight, they should have invested in hydrauli cs machines and embedded that piece of their 
organizations charged with making hydraulic product s in the value network that needed them. But the 
dilemma in managing the disruptive technology in th e heat of the battle is that nothing went wrong 
inside these companies. Hydraulics was a technology  that their customers didn’t need—indeed, 
couldn’t use. Each cable shovel manufacturer was on e of at least twenty manufacturers doing 
everything they could to steal each other’s custome rs: If they took their eyes off their customers’ ne xt-
generation needs, existing business would have been  put at risk. Moreover, developing bigger, better, 
and faster cable excavators to steal share from exi sting competitors constituted a much more obvious 
opportunity for profitable growth than did a ventur e into hydraulic backhoes, given how small the 
backhoe market was when it appeared in the 1950s. S o, as we have seen before, these companies did 
not fail because the technology wasn’t available. T hey did not fail because they lacked information 
about hydraulics or how to use it; indeed, the best  of them used it as soon as it could help their 
customers. They did not fail because management was  sleepy or arrogant. They failed because 
hydraulics didn’t make sense—until it was too late.  
The patterns of success and failure we see among fi rms faced with sustaining and disruptive technology  
change are a natural or systematic result of good  managerial decisions. That is, in fact, why disrup tive 
technologies confront innovators with such a dilemm a. Working harder, being smarter, investing more 
aggressively, and listening more astutely to custom ers are all solutions to the problems posed by new 
sustaining technologies. But these paradigms of sou nd management are useless—even 
counterproductive, in many instances—when dealing w ith disruptive technology. 
 
NOTES   
 
1.  A summary of how this same mechanism might have af fected a broader range of industries can be 
found in Richard S. Rosenbloom and Clayton M. Chris tensen, “Technological Discontinuities, 
Organizational Capabilities, and Strategic Commitme nts,” Industrial and Corporate Change  (3), 1994, 
655-686.  
2.  This information and the data used to calculate th e graphs in this section were provided by Dimitrie 
Toth, Jr., and Keith Haddock, both National Directo rs of the Historical Construction Equipment 
Association. The association has a wealth of inform ation about the earthmoving equipment industry in  70  its archives, and Toth and Haddock were most gracio us in sharing their knowledge and information 
with me. I am also indebted to them for their helpf ul comments on an earlier draft of this chapter. Ot her 
useful sources of information are Peter Grimshaw, Excavators  (Poole, England: Blandford Press, 
1985); The Olyslager Organisation, Inc., Earthmoving Vehicles  (London: Frederick Warne & Co., Ltd., 
1972); Harold F. Williamson and Kenneth H. Myers, Designed for Digging: The First 75 Years of 
Bucyrus Erie Company  (Evanston, IL: Northwestern University Press, 1955 ); and J. L. Allhands, Tools 
of the Earthmover  (Huntsville, TX: Sam Houston College Press, 1951).   
3.  Interestingly, the high success rate was only amon gst the industry’s twenty-five largest firms. Only 
one of the seven smallest steam shovel manufacturer s survived this sustaining technology change to 
internal gasoline combustion. Almost no information  is available about these companies other than 
what is provided by their product brochures. I susp ect, however, that the fact that the large and mid-
sized firms cruised through this transition while t he small ones were killed indicates that resources 
played a part in the story, a conclusion that compl ements the theoretical perspectives summarized in 
chapter 2 above. Some sustaining technologies clear ly are so expensive to develop and implement or so 
dependent on proprietary or scarce expertise that s ome companies simply cannot successfully manage 
the transition. I am indebted to Professor Richard Rosenbloom for sharing his perspective on this issu e.  
4.  An example of this is the development of the first  dragline, by Page, a Chicago area contractor. Page  
dug Chicago’s system of canals, and invented the dr agline in 1903 to do that job more effectively. Pag e 
draglines were later used extensively in digging th e Panama Canal, alongside steam shovels made by 
Bucyrus Erie and Marion. This finding that customer s were significant sources of sustaining 
innovations is consistent with Professor Eric von H ippel’s findings; see The Sources of Innovation  
(New York: Oxford University Press, 1988).  
5.  The companies that survived the invasion of hydrau lics in this way found safe haven in a particular 
high-end market. Bucyrus Erie and Marion, for examp le, became the dominant makers of the huge 
stripping shovels used in strip mines. Marion’s mod el 6360 stripping shovel was the largest frontward-
scooping shovel ever built, able to heft 180 cubic yards in its bucket. (An advertisement showing Paul  
Bunyan standing aside the 6360 is one of the most s tunning pieces of advertising art I have seen.) 
Harnischfeger is the world’s largest maker of elect ric mining shovels, while Unit found a niche making  
the huge pedestal cranes used on offshore oil rigs.  For a time, Northwest survived by making draglines  
for dredging ocean shipping lanes. P & H and Lorain  made huge cranes and draglines (all cable-
actuated).  
6.  As the hydraulic excavator has matured, these comp anies have met with varying degrees of 
subsequent success. In 1996, the world’s highest-vo lume excavator companies, Demag and O & K, 
were based in Germany.  
7.  Technically, excavators that scoop their buckets f orward are power shovels.  This was the dominant 
design from 1837 through the early 1900s, and persi sted as a major market segment through much of 
this century. Excavators that pull earth backward t oward the cab are backhoes.  As the hydraulic 
excavator became the dominant design during the 197 0s, both types came to be called excavators. Until 
hydraulic actuation required the booms to be perman ently attached to the unit, contractors could attac h 
different booms or arms to their basic power units so that the same unit could work as a shovel, 
backhoe, or crane. Similarly, different buckets, so metimes called dippers,  could be attached to move 
different types of material.  
8.  The true measure of performance in excavation was the number of cubic yards of earth that could be 
moved per minute. This measure was so dependent upo n operator skill and upon the type of earth being 
dug, however, that contractors adopted bucket size as the more robust, verifiable metric. 9.  These 
British and American pioneers were followed by seve ral European manufacturers, each of which was 
also an entrant to the excavator industry, includin g France’s Poclain and Italy’s Bruneri Brothers.  
10.  The ability to push the shovel into the earth was a major advantage to the hydraulics approach. The 
cable-actuated excavators that pulled earth toward the operator all had to rely on gravity to drive th e 
teeth of the heavy shovel into the earth.   71  11.  Makers of early hybrid ocean transports, which wer e steam-powered but still outfitted with sails, 
used the same rationale for their design as did the  Bucyrus Erie engineers: Steam power still was not 
reliable enough for the transoceanic market, so ste am power plants had to be backed up by 
conventional technology. The advent of steam-powere d ships and their substitution for wind-powered 
ships in the transoceanic business is itself a clas sic study of disruptive technology. When Robert Ful ton 
sailed the first steamship up the Hudson River in 1 819, it underperformed transoceanic sailing ships o n 
nearly every dimension of performance: It cost more  per mile to operate; it was slower; and it was 
prone to frequent breakdowns. Hence, it could not b e used in the transoceanic value network and could 
only be applied in a different value network, inlan d waterways, in which product performance was 
measured very differently. In rivers and lakes, the  ability to move against the wind or in the absence  of 
a wind was the attribute most highly valued by ship  captains, and along that dimension, steam 
outperformed sail. Some scholars (see, for example,  Richard Foster, in Innovation: The Attacker’s 
Advantage  [New York: Summit Books, 1986]) have marveled at h ow myopic were the makers of 
sailing ships, who stayed with their aging technolo gy until the bitter end, in the early 1900s, comple tely 
ignoring steam power. Indeed, not a single maker of  sailing ships survived the industry’s transition t o 
steam power. The value network framework offers a p erspective on this problem that these scholars 
seem to have ignored, however. It was not a problem  of knowing  about steam power or of having 
access to technology. The problem was that the cust omers of the sailing ship manufacturers, who were 
transoceanic shippers, could not use steam-powered ships until the turn of the century. To cultivate a  
position in steamship building, the makers of saili ng ships would have had to engineer a major strateg ic 
reorientation into the inland waterway market, beca use that was the only value network where steam-
powered vessels were valued throughout most of the 1800s. Hence, it was these firms’ reluctance or 
inability to change strategy, rather than their ina bility to change technology, that lay at the root o f their 
failure in the face of steam-powered vessels.  
12.  An exception to this is an unusual product introdu ced by Koehring in 1957: the Skooper combined 
cables and hydraulics to dig earth away from a faci ng wall; it did not dig down into the earth.  
13.  Bucyrus Erie does not fit easily into either of th ese groups. It introduced a large hydraulic excavat or 
in the 1950s, but subsequently withdrew it from the  market. In the late 1960s, it acquired the 
“Dynahoe” line of hydraulic loader-backhoes from Hy -Dynamic Corporation and sold them as utility 
machines to its general excavation customers, but, again, dropped this product line as well.  
14.  Caterpillar was a very late (but successful) entra nt into the hydraulic excavation equipment 
industry, introducing its first model in 1972. Exca vators were an extension of its line of dozers, 
scrapers, and graders. Caterpillar never participat ed in the excavation machine market when cable 
actuation was the dominant design. 
 
 
 
 
 
 
 
 
 
 
 
 
   72  CHAPTER FOUR  
What Goes Up, Can’t Go Down 
 
 
It is clear from the histories of the disk drive an d excavator industries that the boundaries of value  
networks do not completely imprison the companies w ithin them: There is considerable upward  
mobility into other networks. It is in restraining downward  mobility into the markets enabled by 
disruptive technologies that the value networks exe rcise such unusual power. In this chapter we will 
explore these questions: Why could leading companie s migrate so readily toward high-end markets, 
and why does moving downmarket appear to have been so difficult? Rational managers, as we shall 
see, can rarely build a cogent case for entering sm all, poorly defined low-end markets that offer only  
lower profitability. In fact, the prospects for gro wth and improved profitability in upmarket value 
networks often appear to be so much more attractive  than the prospect of staying within the current  
value network, that it is not unusual to see well-m anaged companies leaving (or becoming 
uncompetitive with) their original customers as the y search for customers at higher price points. In 
good companies, resources and energy coalesce most readily behind proposals to attack upmarket into 
higher-performance products that can earn higher ma rgins.  
Indeed, the prospects for improving financial perfo rmance by moving toward upmarket value networks 
are so strong that one senses a huge magnet in the northeast corner of the disk drive and excavator 
trajectory maps. This chapter examines the power of  this “northeastern pull” by looking at evidence 
from the history of the disk drive industry. It the n generalizes this framework by exploring the same 
phenomenon in the battle between minimill and integ rated steel makers. 
 
 
THE GREAT NORTHEAST MIGRATION IN DISK DRIVES   
 
Figure 4.1 plots in more detail the upmarket moveme nt of Seagate Technology, whose strategy was 
typical of most disk drive manufacturers. Recall th at Seagate had spawned, and then grew to dominate, 
the value network for desktop computing. Its produc t position relative to capacity demanded in its 
market is mapped by vertical lines which span from the lowest- to the highest-capacity drives in its 
product line, in each of the years shown. The black  rectangle on the line measuring each year’s 
capacity span shows the median capacity of the driv es Seagate introduced in each of those years. 
 
  
Figure 4.1 Upmarket Migration of Seagate Products  
  73  
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
  
Between 1983 and 1985, the center of gravity of Sea gate’s product line was positioned squarely on the 
average capacity demanded in the desktop segment. I t was between 1987 and 1989 that the disruptive 
3.5-inch form invaded the desktop market from below . Seagate responded to that attack, not by fighting  
the disruptive technology head-on, but by retreatin g upmarket. It continued to offer models in the 
capacity ranges the desktop PC market demanded, but  by 1993 the focus of its energy had clearly 
shifted to the market for mid-range computers, such  as file servers and engineering workstations. 
Indeed, disruptive technologies have such a devasta ting impact because the firms that first 
commercialized each generation of disruptive disk d rives chose not  to remain contained within their 
initial value network. Rather, they reached as far upmarket as they could in each new product 
generation, until their drives packed the capacity to appeal to the value networks above them. It is t his 
upward mobility that makes disruptive technologies so dangerous to established firms—and so 
attractive to entrants. 
 
 
VALUE NETWORKS AND CHARACTERISTIC COST STRUCTURES   
 
What lies behind this asymmetric mobility? As we ha ve already seen, it is driven by resource allocatio n 
processes that direct resources toward new product proposals that promise higher margins and larger 
markets. These are almost always better in the nort heast portions of trajectory maps (such as Figures 
1.7  and 3.3 ) than in the southeast. The disk drive manufacture rs migrated to the northeast corner of the 
product-market map because the resource allocation processes they employed took them there. 
As we saw in chapter 2, a characteristic of each va lue network is a particular cost structure that fir ms 
within it must create if they are to provide the pr oducts and services in the priority their customers  
demand. Thus, as the disk drive makers became large  and successful within their “home” value 
network, they developed a very specific economic ch aracter: tuning their levels of effort and expenses  
in research, development, sales, marketing, and adm inistration to the needs of their customers and the   74  challenges of their competitors. Gross margins tend ed to evolve in each value network to levels that 
enabled the better disk drive makers to make money,  given these costs of doing business. 
In turn, this gave these companies a very specific model for improving profitability. Generally, they 
found it difficult to improve profitability by hack ing out cost while steadfastly standing in their 
mainstream market: The research, development, marke ting, and administrative costs they were 
incurring were all critical to remaining competitiv e in their mainstream business. Moving upmarket 
toward higher-performance products that promised hi gher gross margins was usually a more 
straightforward path to profit improvement. Moving downmarket was anathema to that objective. 
The obviousness of the path toward profit improveme nt is shown in Figure 4.2. The three bars on the 
left depict the size of the desktop, minicomputer, and mainframe computer value networks in 1981 and 
are labeled with the characteristic margins enjoyed  by disk drive makers in each of those networks. 
Gross margins are clearly higher in higher-end mark ets, compensating manufacturers for the higher 
levels of overhead characteristic of those business es. 
The differences in the size of these markets and th e characteristic cost structures across these value  
networks created serious asymmetries in the combat among these firms. Firms making 8-inch drives for 
the minicomputer market, for example, had cost stru ctures requiring gross margins of 40 percent. 
Aggressively moving downmarket would have pitted th em against foes who had honed their cost 
structures to make money at 25 percent gross margin s. On the other hand, moving upmarket enabled 
them to take a relatively lower-cost structure into  a market that was accustomed to giving its supplie rs 
60 percent gross margins. Which direction made sens e? A similar asymmetry faced the makers of 5.25-
inch drives in 1986, as they decided whether to spe nd their resources building a position in the 
emerging market for 3.5-inch drives in portable com puters or to move up toward the minicomputer and 
mainframe companies. 
Committing development resources to launch higher-p erformance products that could garner higher 
gross margins generally both offered greater return s and caused less pain. As their managers were 
making repeated decisions about which new product d evelopment proposals they should fund and 
which they should shelve, proposals to develop high er-performance products targeted at the larger, 
higher-margin markets immediately above them always  got the resources. In other words, sensible 
resource allocation processes were at the root of c ompanies’ upward mobility and downmarket 
immobility across the boundaries of the value netwo rks in the disk drive industry. 
 
  
Figure 4.2 Views Upmarket and Downmarket for Established Disk Drive Manufacturers  
  75  
 
 
Source: Data are from various issues of Disk/Trend Report, corporate annual reports, and data provided 
in personal interviews.  
Note:  Percentages above each bar indicate typical gross margins in each value network.  
 
  
The hedonic regression analysis summarized in chapt er 2 showed that higher-end markets consistently 
paid significantly higher prices for incremental me gabytes of capacity. Why would anyone opt to sell a  
megabyte for less when it could be sold for more? T he disk drive companies’ migration to the northeast  
was, as such, highly rational. 
Other scholars have found evidence in other industr ies that as companies leave their disruptive roots in 
search of greater profitability in the market tiers  above them, they gradually come to acquire the cos t 
structures required to compete in those upper marke t tiers. 1 This exacerbates their problem of 
downward immobility. 
 
 
RESOURCE ALLOCATION AND UPWARD MIGRATION   
 
Further insight into this asymmetric mobility acros s value networks comes from comparing two 
different descriptive models of how resources are a llocated. The first model describes resource 
allocation as a rational, top-down decision-making process in which senior managers weigh alternative 
proposals for investment in innovation and put mone y into those projects that they find to be consiste nt 
with firm strategy and to offer the highest return on investment. Proposals that don’t clear these hur dles 
are killed. 
The second model of resource allocation, first arti culated by Joseph Bower, 2 characterizes resource 
allocation decisions much differently. Bower notes that most proposals to innovate are generated from 
deep within the organization not from the top. As t hese ideas bubble up from the bottom, the  76  organization’s middle managers play a critical but invisible role in screening these projects. These 
managers can’t package and throw their weight behin d every idea that passes by; they need to decide 
which are the best, which are most likely to succee d, and which are most likely to be approved, given 
the corporate financial, competitive, and strategic  climate. 
In most organizations, managers’ careers receive a big boost when they play a key sponsorship role in 
very successful projects—and their careers can be p ermanently derailed if they have the bad judgment 
or misfortune to back projects that fail. Middle ma nagers aren’t penalized for all  failures, of course. 
Projects that fail because the technologists couldn ’t deliver, for example, often are not (necessarily ) 
regarded as failures at all, because a lot is learn ed from the effort and because technology developme nt 
is generally regarded as an unpredictable, probabil istic endeavor. But projects that fail because the 
market  wasn’t there have far more serious implications fo r managers’ careers. These tend to be much 
more expensive and public failures. They generally occur after the company has made full investments 
in product design, manufacturing, engineering, mark eting, and distribution. Hence, middle managers—
acting in both their own and the company’s interest —tend to back those projects for which market 
demand seems most assured. They then work to packag e the proposals for their chosen projects in ways 
geared to win senior management approval. As such, while senior managers may think  they’re making 
the resource allocation decisions, many of the real ly critical resource allocation decisions have actu ally 
been made long before senior management gets involv ed: Middle managers have made their decisions 
about which projects they’ll back and carry to seni or management—and which they will allow to 
languish. 
Consider the implications of this for a successful firm’s downward and upward mobility from its initia l 
value network in this hypothetical example. In the same week, two respected employees, one from 
marketing, the other from engineering, run two very  different ideas for new products past their 
common manager two levels above them in the organiz ation. The marketer comes first, with an idea for 
a higher-capacity, higher-speed model. The two-leve ls-up manager starts her interrogation: 
“Who’s going to buy it?” 
“Well, there’s a whole segment in the workstation i ndustry—they buy over $600 million in drives each 
year—that we’ve just never been able to reach becau se our capacity points just don’t reach that high. I 
think this product just might get us there.” 
“Have you run this idea past any potential customer s?” 
“Yeah, I was in California last week. They all said  they wanted prototypes as soon as they could get 
them. There’s a design window opening up in nine mo nths. They’ve been working with their current 
supplier [competitor X] to get something ready, but  someone we just hired from competitor X said 
they’re having lots of trouble meeting the specs. I  really think we can do it.” 
“But does engineering  think we can do it?” 
“They say it’ll be a stretch, but you know them. Th ey always say that.” 
“What kind of margins are we looking at up there?” 
“That’s what really excites me about this. If we ca n build it in our current factory, given the price per 
megabyte competitor X has been getting, I think we can get close to 35 percent.”  77  Compare that conversation to the manager’s intercha nge with the engineer whose idea is for a cheaper, 
smaller, slower, lower-capacity disruptive disk dri ve. 
“Who’s going to buy it?” 
“Well, I’m not sure, but there’s got  to be a market out there somewhere  for it. People are always 
wanting things smaller and less expensive. I could see them using it in fax machines, printers, maybe. ” 
“Have you run this idea past any potential customer s?” 
“Yeah, when I was at the last trade show I sketched  the idea out for one of our current customers. He 
said he was interested, but couldn’t see how they c ould really use it. Today you really need 270 MB to  
run everything, and there’s just no way we could ge t that kind of capacity on this thing—at least not for 
a while. His response doesn’t surprise me, really.”  
“How about the guys who make fax machines? What do they think?” 
“Well, they say they don’t know. Again, it’s an int riguing idea, but they already have their product 
plans pretty well set, and none of them use disk dr ives.” 
“You think we could make money on this project?” 
“Well, I think so, but that depends on how we could  price it, of course.” 
Which of the two projects will the two-levels-up ma nager back? In the tug-of-war for development 
resources, projects targeted at the explicit needs of current customers or at the needs of existing us ers 
that a supplier has not yet been able to reach will  always  win over proposals to develop products for 
markets that do not exist. This is because, in fact , the best resource allocation systems are designed  
precisely to weed out ideas that are unlikely to fi nd large, profitable, receptive markets. Any compan y 
that doesn’t  have a systematic way of targeting its development  resources toward customers’ needs, in 
fact, will fail. 3 
The most vexing managerial aspect of this problem o f asymmetry, where the easiest path to growth and 
profit is up, and the most deadly attacks come from  below, is that “good” management—working 
harder and smarter and being more visionary—doesn’t  solve the problem. The resource allocation 
process involves thousands of decisions, some subtl e and some explicit, made every day by hundreds 
of people, about how their time and the company’s m oney ought to be spent. Even when a senior 
manager decides to pursue a disruptive technology, the people in the organization are likely to ignore  it 
or, at best, cooperate reluctantly if it doesn’t fi t their  model of what it takes to succeed as an 
organization and as individuals within an organizat ion. Well-run companies are not populated by yes-
people who have been taught to carry out mindlessly  the directives of management. Rather, their 
employees have been trained to understand what is g ood for the company and what it takes to build a 
successful career within the company. Employees of great companies exercise initiative to serve 
customers and meet budgeted sales and profits. It i s very difficult for a manager to motivate competen t 
people to energetically and persistently pursue a c ourse of action that they think makes no sense. An 
example from the history of the disk drive industry  illustrates the impact of such employee behavior. 
 
 
THE CASE OF THE 1.8-INCH DISK DRIVE    78   
Managers in disk drive companies were very generous  in helping me conduct the research reported in 
this book, and, as the results began emerging in 19 92, I began feeding back the published papers that 
summarized what I was learning. I was particularly interested in whether the framework summarized in 
Figure 1.7 would have an impact on their decisions regarding the 1.8-inch drive, which was just then 
emerging as the industry’s most recent disruptive t echnology. For industry outsiders, of course, the 
conclusion was obvious: “How many times does this h ave to happen before these guys learn?! Of 
course they’ve got to do it.” The guys did, in fact , learn. By the end of 1993, each of the leading dr ive 
makers had developed 1.8-inch models and had them r eady for introduction if and when the market 
developed. 
In August 1994, I was visiting the CEO of one of th e largest disk drive companies and asked him what 
his firm was doing about the 1.8-inch drive. This c learly touched a hot button. He pointed to a shelf in 
his office where a sample 1.8-inch drive was perche d. “You see that?” he demanded. “That’s the fourth 
generation  of 1.8-inch drives we’ve developed—each one with m ore capacity than the last. But we 
haven’t sold any. We want to be ready when the mark et is there, but there just isn’t a market for them  
yet.” 
I countered by reminding him that Disk/Trend Report,  a highly regarded market research publication 
that was the source of much of the data used in my study, had measured the 1993 market at $40 
million, was projecting 1994 sales to be $80 millio n, and forecast 1995 volume at $140 million. 
“I know that’s what they think,” he responded. “But  they’re wrong. There isn’t a market. We’ve had 
that drive in our catalog for 18 months. Everyone k nows we’ve got it, but nobody wants it. The market 
just isn’t there. We just got way ahead of the mark et.” I had no other basis for pressing my point wit h 
this manager, who is one of the most astute manager s I’ve ever met. Our conversation moved to other 
issues. 
About a month later I was leading a case discussion  in the Harvard MBA program’s technology and 
operations management course about the development of a new engine at Honda. One of the students in 
the class had previously worked in Honda’s research  and development organization, so I asked him to 
take a few minutes to tell the class what it was li ke working there. It turned out that he had been 
working on dashboard mapping and navigation systems . I couldn’t resist interrupting his talk by asking , 
“How do you store all that data for the maps?” 
Said the student: “We found a little 1.8-inch disk drive and put it in there. It’s really neat—almost a 
solid-state device, with very few moving parts. Rea lly rugged.” 
“Who do you buy them from?” I pressed. 
“It’s kind of funny,” he replied. “You can’t buy th em from any of the big disk drive companies. We get  
them from a little startup company somewhere in Col orado—I can’t remember the name.” 
I have since reflected on why the head of this comp any would insist so stubbornly that there was no 
market for 1.8-inch drives, even while there was, a nd why my student would say the big drive makers 
didn’t sell these drives, even though they were try ing. The answer lies in the northeast-southeast 
problem, and in the role that the hundreds of well- trained decision makers in a good company play in 
funneling resources and energy into those projects they perceive will bring the company the greatest 
growth and profit. The CEO had decided that the com pany was going to catch this next disruptive wave 
early and had shepherded the project through to a s uccessful, economical design. But among the  79  employees, there was nothing about an $80 million, low-end market that solved the growth and profit 
problems of a multibillion dollar company—especiall y when capable competitors were doing all they 
could to steal away the customers providing those b illions. (The revenue figure is disguised.) And way  
at the other end of the company, there was nothing about supplying prototype quantities of 1.8-inch 
drives to an automaker that solved the problem of m eeting the 1994 quotas of salespeople whose 
contacts and expertise were based so solidly in the  computer industry. 
For an organization to accomplish a task as complex  as launching a new product, logic, energy, and 
impetus must all coalesce behind the effort. Hence,  it is not just the customers  of an established firm 
that hold it captive to their needs. Established fi rms are also captive to the financial structure and  
organizational culture inherent in the value networ k in which they compete—a captivity that can block 
any rationale for timely investment in the next wav e of disruptive technology. 
 
 
VALUE NETWORKS AND MARKET VISIBILITY   
 
The impetus to drift upmarket can be particularly p owerful when a firm’s customers themselves are 
migrating upmarket. In such circumstances, supplier s of an intermediate component such as a disk 
drive may not sense their northeasterly migration b ecause they are embedded among competitors and 
customers experiencing a similar drift. 
In this light, we can see how easy it would have be en for the leading 8-inch disk drive makers—Priam, 
Quantum, and Shugart—to miss the 5.25-inch generati on of drives. Not a single one of their core 
customers, for example, Digital Equipment, Prime Co mputer, Data General, Wang Laboratories, and 
Nixdorf, successfully introduced a desktop computer . Instead, each was moving upmarket itself  toward 
ever higher performance segments of their markets, trying to win the business of customers who 
historically had used mainframes. Similarly, not a single one of the customers of the 14-inch drive 
makers—mainframe makers such as Univac, Burroughs, NCR, ICL, Siemens, and Amdahl—ever made 
a bold enough move downmarket into minicomputers to  become a significant player there. 
Three factors—the promise of upmarket margins, the simultaneous upmarket movement of many of a 
company’s customers, and the difficulty of cutting costs to move downmarket profitably—together 
create powerful barriers to downward mobility. In t he internal debates about resource allocation for 
new product development, therefore, proposals to pu rsue disruptive technologies generally lose out to 
proposals to move upmarket. In fact, cultivating a systematic approach to weeding out new product 
development initiatives that would likely lower pro fits is one of the most important achievements of 
any well-managed company. 
An important strategic implication of this rational  pattern of upmarket movement is that it can create  
vacuum in low-end value networks that draws in entr ants with technologies and cost structures better 
suited to competition. One of these powerful downma rket voids occurred in the steel industry, for 
example, when entrant companies employing disruptiv e minimill process technology entered through 
low-end beachheads; they have attacked relentlessly  upmarket ever since. 
 
 
THE NORTHEASTERLY MIGRATION OF INTEGRATED STEEL    80   
Minimill steel making first became commercially via ble in the mid-1960s. Employing widely available 
and familiar technology and equipment, minimills me lt scrap steel in electric arc furnaces, continuous ly 
cast it into intermediate shapes called billets, an d then roll those into products such as bars, rods,  
beams, or sheets. They are called minimills  because the scale at which they produce cost-compe titive 
molten steel from scrap is less than one-tenth of t he scale required for an integrated mill to produce  
cost-competitive molten steel from iron ore in blas t and basic oxygen furnaces. (Integrated mills take  
their name from the integrated process of transform ing iron ore, coal, and limestone into final steel 
shapes.) Integrated mills and minimills look much t he same in their processes of continuous casting an d 
rolling operations. Scale is the only difference: T he output of efficiently sized blast furnaces requi res 
that integrated mills’ casting and rolling operatio ns must be much greater than those of the minimills . 
North America’s steel minimills are the most effici ent, lowest-cost steel makers in the world. In 1995 , 
the most efficient minimill required 0.6 labor-hour s per ton of steel produced; the best integrated mi ll 
required 2.3 labor-hours. In the product categories  in which they compete, the average minimill can 
make product of equivalent quality, on a fully cost ed basis, at about a 15 percent lower cost than the  
average integrated mill. In 1995, it cost about $40 0 million to build a cost-competitive steel minimil l 
and about $6 billion to build a cost-competitive in tegrated mill. 4 In terms of capital cost per ton of steel 
making capacity, integrated mills are more than fou r times as costly to build. 5 As a result, minimills’ 
share of the North American market has grown from n othing in 1965 to 19 percent in 1975, 32 percent 
in 1985, and 40 percent in 1995. Experts predict th ey will account for half of all steel production by  the 
turn of the century. 6 Minimills virtually dominate the North American ma rkets for rods, bars, and 
structural beams. 
Yet not a single one of the world’s major integrate d steel companies to date has built a mill employin g 
minimill technology. Why would none of them do some thing that makes so much sense? The 
explanation forwarded most frequently by the busine ss press, especially in the United States, is that the 
managers of the integrated companies are conservati ve, backward-looking, risk-averse, and 
incompetent. Consider these indictments. 
Last year, U.S. Steel Corp. closed fifteen of its f acilities, claiming they had become “noncompetitive .” 
Three years ago, Bethlehem Steel Corp. shuttered ma jor portions of its plants in Johnstown, PA, and 
Lackawanna, NY. . . . The closing of these major st eel complexes is the final dramatic concession from  
today’s chief executives that management has not be en doing its job. It represents decades of 
maximizing profits to look good for the short term.7 
If the U.S. steel industry were as productive in to ns per man-hour as it is in rhetoric per problem, i t 
would be a top-notch performer. 8 
Surely there is some credibility to such accusation s. But managerial incompetence cannot be a 
complete answer for the failure of North American i ntegrated mills to counter the conquest by 
minimills of vast portions of the steel industry. None  of what most experts regard as the best-managed 
and most successful of the world’s integrated steel  makers—including Nippon, Kawasaki, and NKK in 
Japan; British Steel and Hoogovens in Europe; and P ohang Steel in Korea—has invested in minimill 
technology even though it is demonstrably the lowes t-cost technology in the world. 
At the same time, in the last decade the management  teams at integrated mills have taken aggressive 
steps to increase mill efficiency. USX, for example , improved the efficiency of its steel making 
operations from more than nine labor-hours per ton of steel produced in 1980 to just under three hours  
per ton in 1991. It accomplished this by ferociousl y attacking the size of its workforce, paring it fr om  81  more than 93,000 in 1980 to fewer than 23,000 in 19 91, and by investing more than $2 billion in 
modernizing its plant and equipment. Yet all of thi s managerial aggressiveness was targeted at 
conventional ways of making steel. How can this be?  
Minimill steelmaking is a disruptive technology. Wh en it emerged in the 1960s, because it used scrap 
steel, it produced steel of marginal quality. The p roperties of its products varied according to the 
metallurgical composition and impurities of the scr ap. Hence, about the only market that minimill 
producers could address was that for steel reinforc ing bars (rebars)—right at the bottom of the market  
in terms of quality, cost, and margins. This market  was the least attractive of those served by 
established steel makers. And not only were margins  low, but customers were the least loyal: They 
would switch suppliers at will, dealing with whoeve r offered the lowest price. The integrated steel 
makers were almost relieved to be rid of the rebar business. 
The minimills, however, saw the rebar market quite differently. They had very different cost structure s 
than those of the integrated mills: little deprecia tion and no research and development costs, low sal es 
expenses (mostly telephone bills), and minimal gene ral managerial overhead. They could sell by 
telephone virtually all the steel they could make—a nd sell it profitably. 
Once they had established themselves in the rebar m arket, the most aggressive minimills, especially 
Nucor and Chaparral, developed a very different vie w of the overall steel market than the view that th e 
integrated mills held. Whereas the downmarket rebar  territory they seized had looked singularly 
unattractive to their integrated competitors, the m inimills’ view upmarket  showed that opportunities for 
greater profits and expanded sales were all above t hem. With such incentive, they worked to improve 
the metallurgical quality and consistency of their products and invested in equipment to make larger 
shapes. 
As the trajectory map in Figure 4.3 indicates, the minimills next attacked the markets for larger bars , 
rods, and angle irons immediately above them. By 19 80, they had captured 90 percent of the rebar 
market and held about 30 percent of the markets for  bars, rods, and angle irons. At the time of the 
minimills’ attack, the bar, rod, and angle iron sha pes brought the lowest margins in the integrated mi lls’ 
product lines. As a consequence, the integrated ste el makers were, again, almost relieved to be rid of  
the business, and by the mid-1980s this market belo nged to the minimills. 
 
  
Figure 4.3 The Progress of Disruptive Minimill Steel Technolog y  
  82  
 
 
 
  
Once their position in the market for bars, rods, a nd angle irons seemed secure, the minimills continu ed 
their march upmarket, this time toward structural b eams. Nucor did so from a new minimill plant in 
Arkansas, and Chaparral launched its attack from a new mill adjacent to its first one in Texas. The 
integrated mills were driven from this market by th e minimills as well. In 1992, USX closed its South 
Chicago structural steel mill, leaving Bethlehem as  the only integrated North American structural stee l 
maker. Bethlehem closed its last structural beam pl ant in 1995, leaving the field to the minimills. 
An important part of this story is that, throughout  the 1980s, as they were ceding the bar and beam 
business to the minimills, the integrated steel mak ers experienced dramatically improving profit. Not 
only were these firms attacking cost, they were for saking their lowest-margin products and focusing 
increasingly on high-quality rolled sheet steel, wh ere quality-sensitive manufacturers of cans, cars, and 
appliances paid premium prices for metallurgically consistent steel with defect-free surfaces. Indeed,  
the lion’s share of integrated mills’ investments i n the 1980s had been targeted at improving their 
ability to provide the most demanding customers in these three markets with the highest-quality 
product and to do so profitably. Sheet steel market s were an attractive haven for the integrated 
producers in part because they were protected from minimill competition. It cost about $2 billion to 
build a state-of-the-art, cost-competitive sheet st eel rolling mill, and this capital outlay simply ha d been 
too much for even the largest of the minimills. 
Targeting the premium end of the market pleased the  integrated mills’ investors: For example, 
Bethlehem Steel’s market value had leapt from $175 million in 1986 to $2.4 billion in 1989. This 
represented a very attractive return on the $1.3 bi llion the company invested in R&D and plant and 
equipment during this period. The business press ge nerously acknowledged these aggressive, well-
placed investments. 
Walter Williams (Bethlehem’s CEO) has worked wonder s. Over the past three years he mounted a 
highly personal campaign to improve the quality and  productivity of Bethlehem’s basic steel business. 
Bethlehem’s metamorphosis has outclassed even its m ajor U.S. competitors—which as a whole are 
now producing at lower costs than their Japanese ri vals and are fast closing the quality gap. Customer s 
notice the difference. “It’s nothing short of miraculous,” says a top purc haser of sheet steel  at 
Campbell Soup. [Italics added.] 9  83  Another analyst made similar observations. 
While almost no one was looking, a near miracle occ urred: Big Steel is making a quiet comeback. Gary 
Works (US Steel) is back in the black . . . pouring  out a glowing river of molten iron at the rate of 3 
million tons per year—a North American record. Unio n-management problem-solving teams are 
everywhere. Instead of making steel in all shapes a nd sizes, Gary has focused almost entirely on 
higher-value flat-rolled steel.  [Italics added.] 10  
Almost all of us would agree that these remarkable recoveries were the fruits of good management. But 
where will good management in this genre  lead these firms? 
 
 
MINIMILL THIN-SLAB CASTING FOR SHEET STEEL   
 
While integrated steel makers were busy engineering  their recoveries, more disruptive clouds began 
gathering on the horizon. In 1987, a German supplie r of equipment for the steel industry, Schloemann-
Siemag AG, announced that it had developed what it called “continuous thin-slab casting” 
technology—a way for steel to be continuously cast from its molten state into long, thin slabs that 
could be transported directly, without cooling, int o a rolling mill. Rolling the white-hot, already th in 
slab of steel to the final thickness of coiled shee t steel was much simpler than the traditional task 
mastered by the integrated mills of reheating and r olling sheet from thick ingots or slabs. Most 
important, a cost-competitive continuous thin-slab casting and rolling mill could be built for less th an 
$250 million—one-tenth the capital cost of a tradit ional sheet mill and a relatively manageable 
investment for a minimill steel maker. At this scal e, an electric arc furnace could easily supply the 
required quantity of molten steel. Moreover, thin-s lab casting promised at least a 20 percent reductio n 
in the total cost of making sheet steel. 
Because of its promise, thin-slab casting was caref ully evaluated by every major player in the steel 
industry. Some integrated mills, such as USX, worke d very hard to justify installation of a thin-slab 
facility. 11  In the end, however, it was minimill Nucor Steel, rather than the integrated mills, that made 
the bold move into thin-slab casting. Why? 
At the outset, thin-slab casting technology could n ot offer the smooth, defect-free surface finish 
required by the integrated mills’ mainstream custom ers (makers of cans, cars, and appliances). The 
only markets were those such as construction deckin g and corrugated steel for culverts, pipes, and 
Quonset huts, in which users were more sensitive to  price than to surface blemishes. Thin-slab casting  
was a disruptive technology. Furthermore, large, ca pable, and hungry integrated competitors were busy 
trying to rob each other’s most profitable business  with the large auto, appliance, and can companies.  It 
made no sense for them to target capital investment  at thin-slab casting, positioned as it was in the 
least-profitable, most price-competitive and commod ity-like end of their business. Indeed, after 
seriously considering between 1987 and 1988 whether  to invest in thin-slab casting at an amount then 
projected to be about $150 million, both Bethlehem and USX elected instead to invest in conventional 
thick-slab continuous casters at a cost of $250 mil lion to protect and enhance the profitability of th e 
business with their mainstream customers. 
Not surprisingly, Nucor saw the situation another w ay. Unencumbered by the demands of profitable 
customers in the sheet steel business and benefitin g from a cost structure forged at the bottom of the  
industry, Nucor fired up the world’s first continuo us thin-slab casting facility in Crawfordsville,  84  Indiana, in 1989, and constructed a second mill in Hickman, Arkansas, in 1992. It increased its capaci ty 
at both sites by 80 percent in 1995. Analysts estim ate that Nucor had captured 7 percent of the massiv e 
North American sheet market by 1996—hardly enough t o concern the integrated mills, because 
Nucor’s success has been limited to the commoditize d, least-profitable end of their product line. Of 
course, in its effort to win higher-margin business  with higher-quality products from these mills, Nuc or 
has already improved the surface quality of its she et steel substantially. 
Thus, the integrated steel companies’ march to the profitable northeast corner of the steel industry i s a 
story of aggressive investment, rational decision m aking, close attention to the needs of mainstream 
customers, and record profits. It is the same innov ator’s dilemma that confounded the leading provider s 
of disk drives and mechanical excavators: Sound man agerial decisions are at the very root of their 
impending fall from industry leadership. 
 
 
NOTES   
 
1.  This process of moving to higher tiers of the mark et and then adding the costs to support business at  
that level was described by Professor Malcom P. McN air, of the Harvard Business School, in a way 
that strikingly parallels the disk drive story. Wri ting in a history of retailing, McNair describes ho w 
successive waves of retailers entered the field wit h disruptive technologies (though he does not use t he 
term):  
 
The wheel always revolves, sometimes slowly, someti mes more rapidly, but it does not stand 
still. The cycle frequently begins with the bold ne w concept, the innovation. Somebody gets a 
bright new idea. There is a John Wanamaker, a Georg e Hartford (A&P), a Frank Woolworth, a 
W. T. Grant, a General Wood (Sears), a Michael Cull en (supermarkets), a Eugene Ferkauf. 
Such an innovator has an idea for a new kind of dis tributive enterprise. At the outset he is in bad 
odor, ridiculed, scorned, condemned as “illegitimat e.” Bankers and investors are leery of him. 
But he attracts the public on the basis of the pric e appeal made possible by the low operating 
costs inherent in his innovation. As he goes along he trades up, improves the quality of his 
merchandise, improves the appearance and standing o f his store, attains greater respectability. . . 
.  
During this process of growth the institution rapid ly becomes respectable in the eyes of both 
consumers and investors, but at the same time its c apital investment increases and its operating 
costs tend to rise. Then the institution enters the  stage of maturity. . . . The maturity phase soon 
tends to be followed by topheaviness . . . and even tual vulnerability. Vulnerability to what? 
Vulnerability to the next fellow who has a bright i dea and who starts his business on a low-cost 
basis, slipping in under the umbrella that the old- line institutions have hoisted. 
 
See Malcom P. McNair, “Significant Trends and Devel opments in the Post-War Period,” in Albert B. 
Smith, ed., Competitive Distribution in a Free High-Level Econo my and Its Implications for the 
University  (Pittsburgh: University of Pittsburgh Press, 1958)  17–18. In other words, the very costs 
required to become competitive in higher-end market s restrict downward mobility and create further 
incentive to move upmarket.  
2.  Joseph Bower, Managing the Resource Allocation Process  (Homewood, IL: Richard D. Irwin,  85  1970).  
3.  The use of the term systematic  in this sentence is important, because most resour ce allocation 
systems work in a systematic way—whether the system  is formal or informal. It will be shown later in 
this book that a key to managers’ ability to confro nt disruptive technology successfully is their abil ity 
to intervene and make resource allocation decisions  personally and persistently. Allocation systems ar e 
designed to weed out just such proposals as disrupt ive technologies. An excellent description of this 
dilemma can be found in Roger Martin, “Changing the  Mind of the Corporation,” Harvard Business 
Review, November–December 1993, 81–94.  
4.  Because of slow growth in steel demand in many of the world’s markets, fewer large integrated steel 
mills are being built in the 1990s. Those integrate d mills that are being built these days are in high -
growth, rapidly developing countries such as Korea,  Mexico, and Brazil.  
5.  Professor Thomas Eagar of the Department of Materi als Science at the Massachusetts Institute of 
Technology provided these estimates.  
6.  “The U.S. Steel Industry: An Historical Overview,”  Goldman Sachs U.S. Research Report,  1995.  
7.  “What Caused the Decline,” Business Week,  June 30, 1980, 74.  
8.  Donald B. Thompson, “Are Steel’s Woes Just Short-t erm,” Industry Week,  February 22, 1982, 31.  
9.  Gregory L. Miles, “Forging the New Bethlehem,” Business Week,  June 5, 1989, 108–110.  
10.  Seth Lubove and James R. Norman, “New Lease on Lif e,” Forbes,  May 9, 1994, 87.  
11.  The experience of the team at U.S. Steel charged w ith evaluating continuous thin-slab casting 
technology is chronicled in the Harvard Business Sc hool teaching case “Continuous Casting 
Investments at USX Corporation,” No. 697-020. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  86   
Part Two 
MANAGING DISRUPTIVE 
TECHNOLOGICAL CHANGE 
 
 
 
In the search for reasons why so many strong compan ies in three very different industries stumbled or 
failed, the research summarized in the preceding ch apters casts doubt on several conventional 
explanations other researchers have offered. It was n’t the case that the leading companies’ engineers 
tended to get stuck in a particular technological p aradigm or ignored innovations that were “not 
invented here.” The cause of failure could not be s olely attributed to established firms’ inadequate 
competence in new technological fields or their ina bility to stay atop their industry’s “technological  
mudslide.” Of course, these problems do afflict som e companies. But as a general rule, the evidence is  
very strong that as long as the new technology was required to address the needs of their customers, 
established firms were able to muster the expertise , capital, suppliers, energy, and rationale to deve lop 
and implement the requisite technology both competi tively and effectively. This has been true for 
incremental as well as radical advances; for projec ts that consumed months as well as those lasting 
more than a decade; in fast-paced disk drives, in t he slower-paced mechanical excavator industry, and 
in the process-intensive steel industry.  
Probably the most important outcome of this attempt  to define the problem is that it ruled out poor 
management as a root cause. Again, this is not to s ay that good and bad management aren’t key factors 
affecting the fortunes of firms. But as a general e xplanation, the managers of the companies studied 
here had a great track record in understanding cust omers’ future needs, identifying which technologies  
could best address those needs, and in investing to  develop and implement them. It was only when 
confronted with disruptive technology that they fai led. There had, therefore, to be a reason why good 
managers consistently made wrong decisions when fac ed with disruptive technological change. 
The reason is that good management itself  was the root cause. Managers played the game the w ay it 
was supposed to be played. The very decision-making  and resource-allocation processes that are key to 
the success of established companies are the very p rocesses that reject disruptive technologies: 
listening carefully to customers; tracking competit ors’ actions carefully; and investing resources to 
design and build higher-performance, higher-quality  products that will yield greater profit. These are  
the reasons why great firms stumbled or failed when  confronted with disruptive technological change. 
Successful companies want  their resources to be focused on activities that a ddress customers’ needs, 
that promise higher profits, that are technological ly feasible, and that help them play in substantial  
markets. Yet, to expect the processes that accompli sh these things also  to do something like nurturing 
disruptive technologies—to focus resources on propo sals that customers reject, that offer lower profit , 
that underperform existing technologies and can onl y be sold in insignificant markets—is akin to 
flapping one’s arms with wings strapped to them in an attempt to fly. Such expectations involve 
fighting some fundamental tendencies about the way successful organizations work and about how 
their performance is evaluated.  87  Part Two of this book is built upon detailed case s tudies of a few companies that succeeded, and many 
more that failed, when faced with disruptive techno logical change. Just as in our analogy to man’s 
finally learning to fly when aviators ultimately ca me to understand and either harness or accommodate 
some fundamental laws of nature, these case studies  show that those executives who succeeded tended 
to manage by a very different set of rules than tho se that failed. There were, in fact, five fundament al 
principles of organizational nature that managers i n the successful firms consistently recognized and 
harnessed. The firms that lost their battles with d isruptive technologies chose to ignore or fight the m. 
These principles are: 
 
1. Resource dependence: Customers effectively contr ol the patterns of resource allocation in 
well-run companies.  
2. Small markets don’t solve the growth needs of la rge companies.  
3. The ultimate uses or applications for disruptive  technologies are unknowable in advance. 
Failure is an intrinsic step toward success.  
4. Organizations have capabilities that exist indep endently of the capabilities of the people who 
work within them. Organizations’ capabilities resid e in their processes and their values—and 
the very processes and values that constitute their  core capabilities within the current business 
model also define their disabilities when confronte d with disruption.  
5. Technology supply may not equal market demand. T he attributes that make disruptive 
technologies unattractive in established markets of ten are the very ones that constitute their 
greatest value in emerging markets.  
How did the successful managers harness these princ iples to their advantage? 
 
1. They embedded projects to develop and commercial ize disruptive technologies within an 
organization whose customers needed them. When mana gers aligned a disruptive innovation 
with the “right” customers, customer demand increas ed the probability that the innovation 
would get the resources it needed.  
2. They placed projects to develop disruptive techn ologies in organizations small enough to get 
excited about small opportunities and small wins.  
3. They planned to fail early and inexpensively  in the search for the market for a disruptive 
technology. They found that their markets generally  coalesced through an iterative process of 
trial, learning, and trial again.  
4. They utilized some of the resources of the mainstream organization to address the disru ption, 
but they were careful not  to leverage its processes and values. They created  different ways of 
working within an organization whose values and cos t structure were turned to the disruptive 
task at hand.  
5. When commercializing disruptive technologies, th ey found or developed new markets  that 
valued the attributes of the disruptive products, r ather than search for a technological 
breakthrough so that the disruptive product could c ompete as a sustaining technology in 
mainstream markets.  
Chapters 5 through 9 in Part Two describe in more d etail how managers can address and harness these 
four principles. Each chapter starts by examining h ow harnessing or ignoring these principles affected  
the fortunes of disk drive companies when disruptiv e technologies were emerging. 1 Each chapter then 
branches into an industry with very different chara cteristics, to show how the same principles drove t he 
success and failure of firms confronted with disrup tive technologies there.  88  The sum of these studies is that while disruptive t echnology can change the dynamics of industries wit h 
widely varying characteristics, the drivers of succ ess or failure when confronted by such technology a re 
consistent across industries. 
Chapter 10  shows how these principles can be used by illustra ting how managers might apply them in a 
case study of a particularly vexing technology—the electric vehicle. Chapter 11  then reviews the 
principal findings of the book. 
 
 
NOTES   
 
1. The notion that we exercise power most effective ly when we understand the physical and 
psychological laws that define the way the world wo rks and then position or align ourselves in 
harmony with those laws, is of course not new to th is book. At a light-hearted level, Stanford 
Professor Robert Burgelman, whose work is extensive ly cited in ths book, once dropped his pen 
onto the floor in a lecture. He muttered as he stoo ped to pick it up, “I hate gravity.” Then, as he 
walked to the blackboard to continue his line of th ought, he added, “But do you know what? 
Gravity doesn’t care! It will always pull things do wn, and I may as well plan on it.” 
At a more serious level, the desirability of aligni ng our actions with the amore powerful laws of natu re, 
society, and psychology, in order to lead a product ive life, is a central theme in many works, 
particularly the ancient Chinese classic, Tao te Ching.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  89   CHAPTER FIVE  
Give Responsibility for  
Disruptive Technologies to  
Organizations Whose  
Customers Need Them 
 
 
Most executives would like to believe that they’re in charge of their organizations, that they make th e 
crucial decisions and that when they decide that so mething should be done everyone snaps to and 
executes. This chapter expands on the view already introduced: that in practice, it is a company’s 
customers  who effectively control what it can and cannot do.  As we have seen in the disk drive 
industry, companies were willing to bet enormous am ounts on technologically risky projects when it 
was clear that their customers needed the resulting  products. But they were unable to muster the 
wherewithal to execute much simpler disruptive proj ects if existing, profitable customers didn’t need 
the products.  
This observation supports a somewhat controversial theory called resource dependence,  propounded by 
a minority of management scholars, 1 which posits that companies’ freedom of action is limited to 
satisfying the needs of those entities outside the firm (customers and investors, primarily) that give  it 
the resources it needs to survive. Drawing heavily upon concepts from biological evolution, resource 
dependence theorists assert that organizations will  survive and prosper only if their staffs and syste ms 
serve the needs of customers and investors by provi ding them with the products, services, and profit 
they require. Organizations that do not will ultima tely die off, starved of the revenues they need to 
survive. 2 Hence, through this survival-of-the-fittest mechan ism, those firms that rise to prominence in 
their industries generally will be those whose peop le and processes are most keenly tuned to giving 
their customers what they want. The controversy wit h this theory arises when its proponents conclude 
that managers are powerless  to change the courses of their firms against the d ictates of their customers. 
Even if a manager has a bold vision to take her or his company in a very different direction, the powe r 
of the customer-focused people and processes in any  company well-adapted to survival in its 
competitive environment will reject the manager’s a ttempts to change direction. Therefore, because 
they provide the resources upon which the firm is d ependent, it is the customers, rather than the 
managers, who really determine what a firm will do.  It is forces outside the organization, rather than  
the managers within it, that dictate the company’s course. Resource dependence theorists conclude that  
the real role of managers in companies whose people  and systems are well-adapted to survival is, 
therefore, only a symbolic one. 
For those of us who have managed companies, consult ed for management, or taught future managers, 
this is a most disquieting thought. We are there to  manage, to make a difference, to formulate and 
implement strategy, to accelerate growth and improv e profits. Resource dependence violates our very 
reason for being. Nonetheless, the findings reporte d in this book provide rather stunning support for the 
theory of resource dependence—especially for the no tion that the customer-focused resource allocation 
and decision-making processes of successful compani es are far more powerful in directing investments 
than are executives’ decisions. 
Clearly, customers wield enormous power in directin g a firm’s investments. What, then, should 
managers do when faced with a disruptive technology  that the company’s customers explicitly do not  90  want? One option is to convince everyone in the fir m that the company should pursue it anyway, that it  
has long-term strategic importance despite rejectio n by the customers who pay the bills and despite 
lower profitability than the upmarket alternatives.  The other option would be to create an independent  
organization and embed it among emerging customers that do  need the technology. Which works best? 
Managers who choose the first option essentially ar e picking a fight with a powerful tendency of 
organizational nature—that customers, not managers,  essentially control the investment patterns of a 
company. By contrast, managers who choose the secon d option align themselves with this tendency, 
harnessing rather than fighting its power. The case s presented in this chapter provide strong evidence  
that the second option offers far higher probabilit ies of success than the first. 
 
 
INNOVATION AND RESOURCE ALLOCATION   
 
The mechanism through which customers control the i nvestments of a firm is the resource allocation 
process—the process that determines which initiativ es get staff and money and which don’t. Resource 
allocation and innovation are two sides of the same  coin: Only those new product development projects 
that do get adequate funding, staffing, and managem ent attention have a chance to succeed; those that 
are starved of resources will languish. Hence, the patterns of innovation in a company will mirror qui te 
closely the patterns in which resources are allocat ed. 
Good resource allocation processes are designed to weed out proposals that customers don’t want. 
When these decision-making processes work well, if customers don’t want a product, it won’t get 
funded; if they do want it, it will. This is how th ings must  work in great companies. They must  invest in 
things customers want—and the better they become at  doing this, the more successful they will be. 
As we saw in chapter 4, resource allocation is not simply a matter of top-down decision making 
followed by implementation. Typically, senior manag ers are asked to decide whether to fund a project 
only after many others at lower levels in the organ ization have already decided which types of project  
proposals they want to package and send on to senio r management for approval and which they don’t 
think are worth the effort. Senior managers typical ly see only a well-screened subset of the innovativ e 
ideas generated. 3 
And even after senior management has endorsed fundi ng for a particular project, it is rarely a “done 
deal.” Many crucial resource allocation decisions a re made after project approval—indeed, after 
product launch—by mid-level managers who set priori ties when multiple projects and products 
compete for the time of the same people, equipment,  and vendors. As management scholar Chester 
Barnard has noted: 
From the point of view of the relative importance o f specific decisions, those of executives properly 
call for first attention. But from the point of vie w of aggregate importance, it is not decisions of 
executives, but of non-executive participants  in organizations which should enlist major interes t. 
[Italics added.] 4 
So how do non-executive participants make their  resource allocation decisions? They decide which 
projects they will propose to senior management and  which they will give priority to, based upon their  
understanding of what types of customers and produc ts are most profitable to the company. Tightly 
coupled with this is their view of how their sponso rship of different proposals will affect their own  91  career trajectories within the company, a view that  is formed heavily by their understanding of what 
customers want and what types of products the compa ny needs to sell more of in order to be more 
profitable. Individuals’ career trajectories can so ar when they sponsor highly profitable innovation 
programs. It is through these mechanisms of seeking  corporate profit and personal success, therefore, 
that customers exert a profound influence on the pr ocess of resource allocation, and hence on the 
patterns of innovation, in most companies. 
 
 
SUCCESS IN DISRUPTIVE DISK DRIVE TECHNOLOGY   
 
It is possible to break out of this system of custo mer control, however. Three cases in the history of  the 
disk drive industry demonstrate how managers can de velop strong market positions in a disruptive 
technology. In two cases, managers harnessed, rathe r than fought, the forces of resource dependence: 
They spun out independent companies to commercializ e the disruptive technology. In the third, the 
manager chose to fight these forces, and survived t he project, exhausted. 
 
 
Quantum and Plus Development   
 
As we have seen, Quantum Corporation, a leading mak er of 8-inch drives sold in the minicomputer 
market in the early 1980s, completely missed the ad vent of 5.25-inch drives: It introduced its first 
versions nearly four years after those drives first  appeared in the market. As the 5.25-inch pioneers 
began to invade the minicomputer market from below,  for all the reasons already described, Quantum’s 
sales began to sag. 
In 1984 several Quantum employees saw a potential m arket for a thin 3.5-inch drive plugged into an 
expansion slot in IBM XT- and AT-class desktop comp uters—drives that would be sold to personal 
computer users rather than the OEM minicomputer man ufacturers that had accounted for all of 
Quantum’s revenue. They determined to leave Quantum  and start a new firm to commercialize their 
idea. 
Rather than let them leave unencumbered, however, Q uantum’s executives financed and retained 80 
percent ownership of this spinoff venture, called P lus Development Corporation, and set the company 
up in different facilities. It was a completely sel f-sufficient organization, with its own executive s taff 
and all of the functional capabilities required in an independent company. Plus was extremely 
successful. It designed and marketed its drives but  had them manufactured under contract by 
Matsushita Kotobuki Electronics (MKE) in Japan. 
As sales of Quantum’s line of 8-inch drives began t o evaporate in the mid-1980s, they were offset by 
Plus’s growing “Hardcard” revenues. By 1987, sales of Quantum’s 8- and 5.25-inch products had 
largely disappeared. Quantum then purchased the rem aining 20 percent of Plus, essentially closed 
down the old corporation, and installed Plus’s exec utives in Quantum’s most senior positions. They 
then reconfigured Plus’s 3.5-inch products to appea l to OEM desktop computer makers, such as Apple, 
just as the capacity vector for 3.5-inch drives was  invading the desktop market, as shown in the disk 
drive trajectory map in Figure 1.7. Quantum, thus r econstituted as a 3.5-inch drive maker, has 
aggressively adopted sustaining component technolog y innovations, moving upmarket toward  92  engineering workstations, and has also successfully  negotiated the sustaining architectural innovation  
into 2.5-inch drives. By 1994 the new Quantum had b ecome the largest unit-volume producer of disk 
drives in the world. 5 
 
 
Control Data in Oklahoma   
 
Control Data Corporation (CDC) effected the same se lf-reconstitution—once. CDC was the dominant 
manufacturer of 14-inch drives sold into the OEM ma rket between 1965 and 1982; its market share 
fluctuated between 55 and 62 percent. When the 8-in ch architecture emerged in the late 1970s, 
however, CDC missed it—by three years. The company never captured more than a fraction of the 8-
inch market, and those 8-inch drives that it did se ll were sold almost exclusively to defend its 
established customer base of mainframe computer man ufacturers. The reason was resources and 
managerial emphasis: Engineers and marketers at the  company’s principal Minneapolis facility kept 
getting pulled off the 8-inch program to resolve pr oblems in the launch of next-generation 14-inch 
products for CDC’s mainstream customers. 
CDC launched its first 5.25-inch model two years af ter Seagate’s pioneering product appeared in 1980. 
This time, however, CDC located its 5.25-inch effor t in Oklahoma City. This was done, according to 
one manager, “not to escape CDC’s Minneapolis engin eering culture, but to isolate the [5.25-inch 
product] group from the company’s mainstream custom ers.” Although it was late in the market and 
never regained its former dominant position, CDC’s foray into 5.25-inch drives was profitable, and at 
times the firm commanded a 20 percent share of high er-capacity 5.25-inch drives. 
 
 
Micropolis: Transition by Managerial Force   
 
Micropolis Corporation, an early disk drive leader founded in 1978 to make 8-inch drives, was the only  
other industry player to successfully make the tran sition to a disruptive platform. It did not use the  spin-
out strategy that had worked for Quantum and Contro l Data, however, choosing instead to manage the 
change from within the mainstream company. But even  this exception supports the rule that customers 
exert exceptionally powerful influence over the inv estments that firms can undertake successfully. 
Micropolis began to change in 1982, when founder an d CEO Stuart Mabon intuitively perceived the 
trajectories of market demand and technology supply  mapped in Figure 1.7 and decided that the firm 
should become primarily a maker of 5.25-inch drives . While initially hoping to keep adequate 
resources focused on developing its next generation  of 8-inch drives so that Micropolis could straddle  
both markets, 6 he assigned the company’s premier engineers to the  5.25-inch program. Mabon recalls 
that it took “100 percent of my time and energy for  eighteen months” to keep adequate resources 
focused on the 5.25-inch program, because the organ ization’s own mechanisms allocated resources to 
where the customers were—8-inch drives. 
By 1984, Micropolis had failed to keep pace with co mpetition in the minicomputer market for disk 
drives and withdrew its remaining 8-inch models. Wi th Herculean effort, however, it did succeed in its  
5.25-inch programs. Figure 5.1 shows why this strug gle occurred: In making the transition, Micropolis 
assumed a position on a very different technologica l trajectory. It had to walk away from every one of   93  its major customers and replace the lost revenues w ith sales of the new product line to an entirely 
different group of desktop computer makers. Mabon r emembers the experience as the most exhausting 
of his life. 
Micropolis finally introduced a 3.5-inch product in  1993. That was the point at which the product had 
progressed to pack more than 1 gigabyte in the 3.5- inch platform. At that level, Micropolis could sell  
the 3.5-inch drive to its existing customers. 
 
  
Figure 5.1 Technology Transition and Market Position at Microp olis Corporation  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
  
 
DISRUPTIVE TECHNOLOGIES AND THE THEORY OF RESOURCE DEPENDENCE  
 
The struggles recounted earlier of Seagate Technolo gy’s attempts to sell 3.5-inch drives and of Bucyru s 
Erie’s failed attempt to sell its early Hydrohoe on ly to its mainstream customers illustrate how the 
theory of resource dependence can be applied to cas es of disruptive technologies. In both instances, 
Seagate and Bucyrus were among the first in their i ndustries to develop these disruptive products. But  
despite senior managers’ decisions to introduce the m, the impetus or organizational energy required to  
launch the products aggressively into the appropria te value networks simply did not coalesce—until 
customers needed them. 
Should we then accept the corollary stipulated by r esource-dependence theorists that managers are 
merely powerless individuals? Hardly. In the Introd uction, exploring the image of how people learned 
to fly, I noted that all attempts had ended in fail ure as long as they consisted of fighting fundament al 
laws of nature. But once laws such as gravity, Bern oulli’s principle, and the notions of lift, drag an d 
resistance began to be understood, and flying machi nes were designed that accounted for or harnessed  94  those laws, people flew quite successfully. By anal ogy, this is what Quantum and Control Data did. By 
embedding independent organizations within an entir ely different value network, where they were 
dependent upon the appropriate set of customers for  survival, those managers harnessed the powerful 
forces of resource dependence. The CEO of Micropoli s fought them, but he won a rare and costly 
victory. 
Disruptive technologies have had deadly impact in m any industries besides disk drives, mechanical 
excavators, and steel. 7 The following pages summarize the effect of disrup tive technologies in three 
other industries—computers, retailing, and printers —to highlight how the only companies in those 
industries that established strong market positions  in the disruptive technologies were those which, l ike 
Quantum and Control Data, harnessed rather than fou ght the forces of resource dependence. 
 
 
DEC, IBM, AND THE PERSONAL COMPUTER   
 
Quite naturally, the computer industry and the disk  drive industry have parallel histories, because va lue 
networks of the latter are embedded in those of the  former. In fact, if the axes and intersecting 
trajectories depicted on the disk drive trajectory map in Figure 1.7  were relabeled with computer-
relevant terms, it would summarize equally well the  failure of leading computer industry firms. IBM, 
the industry’s first leader, sold its mainframe com puters to the central accounting and data processin g 
departments of large organizations. The emergence o f the minicomputer represented a disruptive 
technology to IBM and its competitors. Their custom ers had no use for it; it promised lower, not 
higher, margins; and the market initially was signi ficantly smaller. As a result, the makers of 
mainframes ignored the minicomputer for years, allo wing a set of entrants—Digital Equipment, Data 
General, Prime, Wang, and Nixdorf—to create and dom inate that market. IBM ultimately introduced 
its own line of minicomputers, but it did so primar ily as a defensive measure, when the capabilities o f 
minicomputers had advanced to the point that they w ere performance-competitive with the computing 
needs of some of IBM’s customers. 
Similarly, none of the makers of minicomputers beca me a significant factor in the desktop personal 
computer market, because to them the desktop comput er was a disruptive technology. The PC market 
was created by another set of entrants, including A pple, Commodore, Tandy, and IBM. The 
minicomputer makers were exceptionally prosperous a nd highly regarded by investors, the business 
press, and students of good management—until the la te 1980s, when the technological trajectory of the 
desktop computer intersected with the performance d emanded by those who had previously bought 
minicomputers. The missile-like attack of the deskt op computer from below severely wounded every 
minicomputer maker. Several of them failed. None es tablished a viable position in the desktop personal  
computer value network. 
A similar sequence of events characterized the emer gence of the portable computer, where the market 
was created and dominated by a set of entrants like  Toshiba, Sharp, and Zenith. Apple and IBM, the 
leading desktop makers, did not introduce portable models until the portables’ performance trajectory 
intersected with the computing needs of their custo mers. 
Probably none of these firms has been so deeply wou nded by disruptive technology as Digital 
Equipment. DEC fell from fortune to folly in just a  few years, as stand-alone workstations and 
networked desktop computers obviated most customers ’ needs for minicomputers almost overnight.  95  DEC didn’t stumble for lack of trying, of course. F our times between 1983 and 1995 it introduced lines  
of personal computers targeted at consumers, produc ts that were technologically much simpler than 
DEC’s minicomputers. But four times it failed to bu ild businesses in this value network that were 
perceived within the company as profitable. Four ti mes it withdrew from the personal computer market. 
Why? DEC launched all four forays from within the m ainstream company. 8 For all of the reasons so far 
recounted, even though executive-level decisions la y behind the move into the PC business, those who 
made the day-to-day resource allocation decisions i n the company never saw the sense in investing the 
necessary money, time, and energy in low-margin pro ducts that their customers didn’t want. Higher-
performance initiatives that promised upscale margi ns, such as DEC’s super-fast Alpha microprocessor 
and its adventure into mainframe computers, capture d the resources instead. 
In trying to enter the desktop personal computing b usiness from within its mainstream organization, 
DEC was forced to straddle the two different cost s tructures intrinsic to two different value networks . It 
simply couldn’t hack away enough overhead cost to b e competitive in low-end personal computers 
because it needed those costs to remain competitive  in its higher-performance products. 
Yet IBM’s success in the first five years of the pe rsonal computing industry stands in stark contrast to 
the failure of the other leading mainframe and mini computer makers to catch the disruptive desktop 
computing wave. How did IBM do it? It created an au tonomous organization in Florida, far away from 
its New York state headquarters, that was free to p rocure components from any source, to sell through 
its own channels, and to forge a cost structure app ropriate to the technological and competitive 
requirements of the personal computing market. The organization was free to succeed along metrics of 
success that were relevant to the personal computin g market. In fact, some have argued that IBM’s 
subsequent decision to link its personal computer d ivision much more closely to its mainstream 
organization was an important factor in IBM’s diffi culties in maintaining its profitability and market  
share in the personal computer industry. It seems t o be very difficult to manage the peaceful, 
unambiguous coexistence of two cost structures, and  two models for how to make money, within a 
single company. 
The conclusion that a single organization might sim ply be incapable of competently pursuing 
disruptive technology, while remaining competitive in mainstream markets, bothers some “can-do” 
managers—and, in fact, most managers try to do exac tly what Micropolis and DEC did: maintain their 
competitive intensity in the mainstream, while simu ltaneously trying to pursue disruptive technology. 
The evidence is strong that such efforts rarely suc ceed; position in one market will suffer unless two  
separate organizations, embedded within the appropr iate value networks, pursue their separate 
customers. 
 
 
KRESGE, WOOLWORTH, AND DISCOUNT RETAILING   
 
In few industries has the impact of disruptive tech nology been felt so pervasively as in retailing, wh ere 
discounters seized dominance from traditional depar tment and variety stores. The technology of 
discount retailing was disruptive to traditional op erations because the quality of service and selecti on 
offered by discounters played havoc with the accust omed metrics of quality retailing. Moreover, the 
cost structure required to compete profitably in di scount retailing was fundamentally different than t hat 
which department stores had developed to compete wi thin their value networks.  96  The first discount store was Korvette’s, which bega n operating a number of outlets in New York in the 
mid-1950s. Korvette’s and its imitators operated at  the very low end of retailing’s product line, sell ing 
nationally known brands of standard hard goods at 2 0 to 40 percent below department store prices. 
They focused on products that “sold themselves” bec ause customers already knew how to use them. 
Relying on national brand image to establish the va lue and quality of their products, these discounter s 
eliminated the need for knowledgeable salespeople; they also focused on the group of customers least 
attractive to mainstream retailers: “young wives of  blue collar workers with young children.” 9 This was 
counter to the upscale formulas department stores h istorically had used to define quality retailing an d to 
improve profits. 
Discounters didn’t accept lower profits than those of traditional retailers, however; they just earned  
their profits through a different formula. In the s implest terms, retailers cover their costs through the 
gross margin, or markup, they charge over the cost of the merchandise they sell. Traditional departmen t 
stores historically marked merchandise up by 40 per cent and turned their inventory over four times in a 
year—that is, they earned 40 percent on the amount they invested in inventory, four times during the 
year, for a total return on inventory investment of  160 percent. Variety stores earned somewhat lower 
profits through a formula similar to that used by t he department stores. Discount retailers earned a 
return on inventory investment similar to that of d epartment stores, but through a different model: lo w 
gross margins and high inventory turns. Table 5.1 s ummarizes the three positions. 
 
  
Table 5.1 Different Pathways to Profits  
 
 
 
Calculated as Margins x Turns, in other words, the total of the margins earned through successive 
turnovers each year.  
Source: Annual corporate reports of many companies in each category for various years.  
 
  
The history of discount retailing vividly recalls t he history of minimill steel making. Just like the 
minimills, discounters took advantage of their cost  structure to move upmarket and seize share from 
competing traditional retailers at a stunning rate:  first at the low end, in brand-name hard goods suc h as 
hardware, small appliances, and luggage, and later in territory further to the northeast such as home 
furnishings and clothing. Figure 5.2 illustrates ho w stunning the discounters’ invasion was: Their sha re 
of retailing revenues in the categories of goods th ey sold rose from 10 percent in 1960 to nearly 40 
percent a scant six years later. 
 
   97  Figure 5.2  Gains in Discount Retailers’ Market Share, 1960-19 66  
 
 
 
Source: Data are from various issues of Discount Merchandiser.   
Just as in disk drives and excavators, a few of the  leading traditional retailers—notably S. S. Kresge , F. 
W. Woolworth, and Dayton Hudson—saw the disruptive approach coming and invested early. None of 
the other major retail chains, including Sears, Mon tgomery Ward, J. C. Penney, and R. H. Macy, made 
a significant attempt to create a business in disco unt retailing. Kresge (with its Kmart chain) and 
Dayton Hudson (with the Target chain) succeeded. 10  They both created focused discount retailing 
organizations that were independent from their trad itional business. They recognized and harnessed the  
forces of resource dependence. By contrast, Woolwor th failed in its venture (Woolco), trying to launch  
it from within the F. W. Woolworth variety store co mpany. A detailed comparison of the approaches of 
Kresge and Woolworth, which started from very simil ar positions, lends additional insight into why 
establishing independent organizations to pursue di sruptive technology seems to be a necessary 
condition for success. 
S. S. Kresge, then the world’s second largest varie ty store chain, began studying discount retailing i n 
1957, while discounting was still in its infancy. B y 1961, both Kresge and its rival F. W. Woolworth 
(the world’s largest variety store operator) had an nounced initiatives to enter discount retailing. Bo th 
firms opened stores in 1962, within three months of  each other. The performance of the Woolco and 
Kmart ventures they launched, however, subsequently  differed dramatically. A decade later, Kmart’s 
sales approached $3.5 billion while Woolco’s sales were languishing unprofitably at $0.9 billion. 11  
In making its commitment to discount retailing, Kre sge decided to exit the variety store business 
entirely: In 1959 it hired a new CEO, Harry Cunning ham, whose sole mission was to convert Kresge 
into a discounting powerhouse. Cunningham, in turn,  brought in an entirely new management team, so 
that by 1961 there “was not a single operating vice  president, regional manager, assistant regional 
manager, or regional merchandise manager who was no t new on the job.” 12  In 1961 Cunningham 
stopped opening any new variety stores, embarking i nstead on a program of closing about 10 percent of 
Kresge’s existing variety operations each year. Thi s represented a wholesale refocusing of the company  
on discount retailing. 
Woolworth, on the other hand, attempted to support a program of sustaining improvements in 
technology, capacity, and facilities in its core va riety store businesses while simultaneously investi ng in 
disruptive discounting. The managers charged with i mproving the performance of Woolworth’s variety 
stores were also charged with building “the largest  chain of discount houses in America.” CEO Robert 
Kirkwood asserted that Woolco “would not conflict w ith the company’s plans for growth and  98  expansion in the regular variety store operations,”  and that no existing stores would be converted to a 
discount format. 13  Indeed, as discount retailing hit its most frenzie d expansion phase in the 1960s, 
Woolworth was opening new variety stores at the pac e it had set in the 1950s. 
Unfortunately (but predictably), Woolworth proved u nable to sustain within a single organization the 
two different cultures, and two different models of  how to make a profit, that were required to be 
successful in variety and discount retailing. By 19 67 it had dropped the term “discount” from all 
Woolco advertising, adopting the term “promotional department store” instead. Although initially 
Woolworth had set up a separate administrative staf f for its Woolco operation, by 1971 more rational, 
cost-conscious heads had prevailed. 
In a move designed to increase sales per square foo t in both Woolco and Woolworth divisions, the two 
subsidiaries have been consolidated operationally o n a regional basis. Company officials say the 
consolidation—which involves buying offices, distri bution facilities and management personnel at the 
regional level—will help both to develop better mer chandise and more efficient stores. Woolco will 
gain the benefits of Woolworth’s buying resources, distribution facilities and additional expertise in  
developing specialty departments. In return, Woolwo rth will gain Woolco’s knowhow in locating, 
designing, promoting and operating large stores ove r 100,000 sq. ft. 14  
What was the impact of this cost-saving consolidati on? It provided more evidence that two models for 
how to make money cannot peacefully coexist within a single organization. Within a year of this 
consolidation, Woolco had increased its markups suc h that its gross margins were the highest in the 
discount industry—about 33 percent. In the process,  its inventory turns fell from the 7x it originally  had 
achieved to 4x. The formula for profit that had lon g sustained F. W. Woolworth (35 percent margins 
for four inventory turns or 140 percent return on i nventory investment) was ultimately demanded of 
Woolco as well. (See Figure 5.3.) Woolco was no lon ger a discounter—in name or in fact. Not 
surprisingly, Woolworth’s venture into discount ret ailing failed: It closed its last Woolco store in 1 982. 
  
Figure 5.3 Impact of the Integration of Woolco, and F. W. Wool worth on the Way Woolco Attempted 
to Make Money  
 
 
 
Source: Data are from various annual reports of F. W. Woolw orth Company and from various issues of 
Discount Merchandiser.   
 
   99  Woolworth’s organizational strategy for succeeding in disruptive discount retailing was the same as 
Digital Equipment’s strategy for launching its pers onal computer business. Both founded new ventures 
within the mainstream organization that had to earn  money by mainstream rules, and neither could 
achieve the cost structure and profit model require d to succeed in the mainstream value network. 
 
 
SURVIVAL BY SUICIDE: HEWLETT-PACKARD’S LASER JET AN D INK-JET PRINTERS   
 
Hewlett-Packard’s experience in the personal comput er printer business illustrates how a company’s 
pursuit of a disruptive technology by spinning out an independent organization might entail, in the en d, 
killing another of its business units. 
Hewlett-Packard’s storied success in manufacturing printers for personal computers becomes even 
more remarkable when one considers its management o f the emergence of bubble-jet or ink-jet 
technology. Beginning in the mid-1980s, HP began bu ilding a huge and successful business around 
laser jet printing technology. The laser jet was a discontinuous improvement over dot-matrix printing,  
the previously dominant personal computer printing technology, and HP built a commanding market 
lead. 
When an alternative way of translating digital sign als into images on paper (ink-jet technology) first  
appeared, there were vigorous debates about whether  laser jet or ink jet would emerge as the dominant 
design in personal printing. Experts lined up on bo th sides of the question, offering HP extensive 
advice on which technology would ultimately become the printer of choice on the world’s desktops. 15  
Although it was never framed as such in the debates  of the time, ink-jet printing was a disruptive 
technology. It was slower than the laser jet, its r esolution was worse, and its cost per printed page was 
higher. But the printer itself was smaller and pote ntially much less expensive than the laser jet. At these 
lower prices, it promised lower gross margin dollar s per unit than the laser jet. Thus, the ink-jet pr inter 
was a classic disruptive product, relative to the l aser jet business. 
Rather than place its bet exclusively with one or t he other, and rather than attempt to commercialize the 
disruptive ink-jet from within the existing printer  division in Boise, Idaho, HP created a completely 
autonomous organizational unit, located in Vancouve r, Washington, with responsibility for making the 
ink-jet printer a success. It then let the two busi nesses compete against each other. Each has behaved  
classically. As shown in Figure 5.4, the laser jet division has moved sharply upmarket, in a strategy 
reminiscent of 14-inch drives, mainframe computers,  and integrated steel mills. HP’s laser jet printer s 
can print at high speeds with exceptional resolutio n; handle hundreds of fonts and complicated 
graphics; print on two sides of the page; and serve  multiple users on a network. They have also gotten  
larger physically. 
The ink-jet printer isn’t as good as the laser jet and may never be. But the critical question is whet her 
the ink jet could ever be as good a printer as the personal desktop computing market demands. The 
answer appears to be yes. The resolution and speed of ink-jet printers, while still inferior to those of 
laser jets, are now clearly good enough for many st udents, professionals, and other un-networked users  
of desktop computers. 
HP’s ink-jet printer business is now capturing many  of those who would formerly have been laser jet 
users. Ultimately, the number of users at the highe st-performance end of the market, toward which the  100  laser jet division is headed, will probably become small. One of HP’s businesses may, in the end, have  
killed another. But had HP not set up its ink-jet b usiness as a separate organization, the ink-jet 
technology would probably have languished within th e mainstream laser jet business, leaving one of 
the other companies now actively competing in the i nk-jet printer business, such as Canon, as a seriou s 
threat to HP’s printer business. And by staying in the laser business, as well, HP has joined IBM’s 
mainframe business and the integrated steel compani es in making a lot  of money while executing an 
upmarket retreat. 16  
 
  
Figure 5.4 Speed Improvements in InkJet and LaserJet Printers  
 
 
 
Source: Hewlett-Packard product brochures, various years.  
 
 
NOTES   
 
1.  The theory of resource dependence has been most th oroughly argued by Jeffrey Pfeffer and Gerald 
R. Salancik in The External Control of Organizations: A Resource D ependence Perspective  (New 
York: Harper & Row, 1978).  
2.  This implies that, in managing business under both  normal conditions and conditions of assault by a 
disruptive technology, the choice of which customer s the firm will serve has enormous strategic 
consequences.  
3.  Joseph L. Bower, in Managing the Resource Allocation Process  (Homewood, IL: Richard D. Irwin, 
1972), presents an elegant and compelling picture o f the resource allocation process.  
4.  Chester Barnard, The Functions of the Executive  (Cambridge, MA: Harvard University Press, 1938), 
190–191.  
5.  Quantum’s spin-out of the Hardcard effort and its subsequent strategic reorientation is an example of  
the processes of strategy change described by Rober t Burgelman, in “Intraorganizational Ecology of 
Strategy-Making and Organizational Adaptation: Theo ry and Field Research,” Organization Science   101  (2), 1991, 239–262, as essentially a process of nat ural selection through which suboptimal strategic 
initiatives lose out to optimal ones in the interna l competition for corporate resources.  
6.  The failure of Micropolis to maintain simultaneous  competitive commitments to both its established 
technology and the new 5.25-inch technology is cons istent with the technological histories recounted 
by James Utterback, in Mastering the Dynamics of Innovation  (Boston: Harvard Business School Press, 
1994). Utterback found that firms that attempted to  develop radically new technology almost always 
tried to maintain simultaneous commitment to the ol d and that they almost always failed.  
7.  A set of industries in which disruptive technologi es are believed to have played a role in toppling 
leading firms is presented by Richard S. Rosenbloom  and Clayton M. Christensen in “Technological 
Discontinuities, Organizational Capabilities, and S trategic Commitments,” Industrial and Corporate 
Change  (3), 1994, 655–685.  
8.  In the 1990s, DEC finally set up a Personal Comput er Division in its attempt to build a significant 
personal computer business. It was not as autonomou s from DEC’s mainstream business; however, the 
Quantum and Control Data spin-outs were. Although D EC set up specific performance metrics for the 
PC division, it was still held, de facto,  to corporate standards for gross margins and reven ue growth.  
9.  “Harvard Study on Discount Shoppers,” Discount Merchandiser,  September, 1963, 71.  
10.  When this book was being written, Kmart was a crip pled company, having been beaten in a game 
of strategy and operational excellence by WalMart. Nonetheless, during the preceding two decades, 
Kmart had been a highly successful retailer, creati ng extraordinary value for Kresge shareholders. 
Kmart’s present competitive struggles are unrelated  to Kresge’s strategy in meeting the original 
disruptive threat of discounting.  
11.  A detailed contrast between the Woolworth and Kres ge approaches to discount retailing can be 
found in the Harvard Business School teaching case.  “The Discount Retailing Revolution in America,” 
No. 695-081.  
12.  See Robert Drew-Bear, “S. S. Kresge’s Kmarts,” Mass Merchandising: Revolution and Evolution  
(New York: Fairchild Publications, 1970), 218.  
13.  F. W. Woolworth Company Annual Report, 1981, p. 8.   
14.  “Woolco Gets Lion’s Share of New Space,” Chain Store Age,  November, 1972, E27. This was an 
extraordinarily elegant, rational argument for the consolidation, clearly crafted by a corporate spin-
doctor extraordinaire. Never mind that no Woolworth  stores approached 100,000 square feet in size!  
15.  See, for example, “The Desktop Printer Industry in  1990,” Harvard Business School, Case No. 9-
390-173.  
16.  Business historian Richard Tedlow noted that the s ame dilemma had confronted A&P’s executives 
as they deliberated whether to adopt the disruptive  supermarket retailing format:  
The supermarket entrepreneurs competed against A&P not by doing better what A&P was the best 
company in the world at doing, but by doing somethi ng that A&P did not want to do at all. The greatest  
entrepreneurial failure in this story is Kroger. Th is company was second in the market, and one of its  
own employees (who left to found the world’s first supermarket) knew how to make it first. Kroger 
executives did not listen. Perhaps it was lack of i magination or perhaps, like the executives at A&P, 
those at Kroger also had too much invested in the s tandard way of doing business. If the executives at  
A&P endorsed the supermarket revolution, they were ruining their own distribution system. That is 
why they sat by paralyzed, unable to act until it w as almost too late. In the end, A&P had little choi ce. 
The company could ruin its own system, or see other s do it. 
 
See Richard Tedlow, New and Improved: The Story of Mass Marketing in Am erica  (Boston: Harvard 
Business School Press, 1996). 
  102   
CHAPTER SIX  
Match the Size of the  
Organization to the Size  
of the Market 
 
 
Managers who confront disruptive technological chan ge must be leaders, not followers, in 
commercializing disruptive technologies. Doing so r equires implanting the projects that are to develop  
such technologies in commercial organizations that match in size the market they are to address. These  
assertions are based on two key findings of this st udy: that leadership is more crucial in coping with  
disruptive technologies than with sustaining ones, and that small, emerging markets cannot solve the 
near-term growth and profit requirements of large c ompanies.  
The evidence from the disk drive industry shows tha t creating new markets is significantly less  risky 
and more  rewarding than entering established markets agains t entrenched competition. But as 
companies become larger and more successful, it bec omes even more difficult to enter emerging 
markets early enough. Because growing companies nee d to add increasingly large chunks of new 
revenue each year just to maintain their desired ra te of growth, it becomes less and less possible tha t 
small markets can be viable as vehicles through whi ch to find these chunks of revenue. As we shall see , 
the most straightforward way of confronting this di fficulty is to implant projects aimed at 
commercializing disruptive technologies in organiza tions small enough to get excited about small-
market opportunities, and to do so on a regular bas is even while the mainstream company is growing. 
 
 
ARE THE PIONEERS REALLY  THE ONES WITH ARROWS IN THEIR BACKS?   
 
A crucial strategic decision in the management of i nnovation is whether it is important to be a leader  or 
acceptable to be a follower. Volumes have been writ ten on first-mover advantages, and an offsetting 
amount on the wisdom of waiting until the innovatio n’s major risks have been resolved by the 
pioneering firms. “You can always tell who the pion eers were,” an old management adage goes. 
“They’re the ones with the arrows in their backs.” As with most disagreements in management theory, 
neither position is always right. Indeed, some find ings from the study of the disk drive industry give  
some insight into when leadership is critical and w hen followership makes better sense. 
 
 
Leadership in Sustaining Technologies May Not Be Es sential   
 
One of the watershed technologies affecting the pac e at which disk drive makers have increased the 
recording density of their drives was the thin-film  read/write head. We saw in chapter 1 that despite the 
radically different, competence-destroying characte r of the technology, the $100 million and five-to- 103  fifteen year expense of developing it, the firms th at led in this technology were the leading, establi shed 
disk drive manufacturers. 
Because of the risk involved in the technology’s de velopment and its potential importance to the 
industry, the trade press began speculating in the late 1970s about which competitor would lead with 
thin-film heads. How far might conventional ferrite  head technology be pushed? Would any drive 
makers get squeezed out of the industry race becaus e they placed a late or wrong bet on the new head 
technology? Yet, it turned out, whether a firm led or followed in this innovation did not  make a 
substantial difference in its competitive position.  This is illustrated in Figures 6.1 and 6.2. 
Figure 6.1 shows when each of the leading firms int roduced its first model employing thin-film head 
technology. The vertical axis measures the recordin g density of the drive. The bottom end of the line 
for each firm denotes the maximum recording density  it had achieved before it introduced a model with 
a thin-film head. The top end of each line indicate s the density of the first model each company 
introduced with a thin-film head. Notice the wide d isparity in the points at which the firms felt it w as 
important to introduce the new technology. IBM led the industry, introducing its new head when it had 
achieved 3 megabits (Mb) per square inch. Memorex a nd Storage Technology similarly took a 
leadership posture with respect to this technology.  At the other end, Fujitsu and Hitachi pushed the 
performance of conventional ferrite heads nearly te n times beyond the point where IBM first 
introduced the technology, choosing to be followers , rather than leaders, in thin-film technology. 
 
  
Figure 6.1 Points at Which Thin-Film Technology Was Adopted by  Leading Manufacturers, Relative 
to the Capabilities of Ferrite/Oxide Technology at the Time of the Switch  
 
 
 
Source:  Data are from various issues of Disk/Trend Report.    104   
  
What benefit, if any, did leadership in this techno logy give to the pioneers? There is no evidence tha t 
the leaders gained any significant competitive adva ntage over the followers; none of the firms that 
pioneered thin-film technology gained significant m arket share on that account. In addition, pioneerin g 
firms appear not to have developed any sort of lear ning advantage enabling them to leverage their earl y 
lead to attain higher levels of density than did fo llowers. Evidence of this is displayed in Figure 6. 2. 
The horizontal axis shows the order in which the fi rms adopted thin-film heads. Hence, IBM was the 
first, Memorex, the second, and Fujitsu the fifteen th. The vertical axis gives the rank ordering of th e 
recording density of the most advanced model market ed by each firm in 1989. If the early adopters of 
thin-film heads enjoyed some sort of experience-bas ed advantage over the late adopters, then we would 
expect the points in the chart to slope generally f rom the upper left toward the lower right. The char t 
shows instead that there is no relationship between  leadership and followership in thin-film heads and  
any subsequent technological edge. 1 
Each of the other sustaining technologies in the in dustry’s history present a similar picture. There i s no 
evidence that any of the leaders in developing and adopting sustaining technologies developed a 
discernible competitive advantage over the follower s. 2 
 
 
Leadership in Disruptive Technologies Creates Enorm ous Value   
 
In contrast to the evidence that leadership in sust aining technologies has historically conferred litt le 
advantage on the pioneering disk drive firms, there  is strong evidence that leadership in disruptive 
technology has been very  important. The companies that entered the new valu e networks enabled by 
disruptive generations of disk drives within the fi rst two years after those drives appeared were six 
times more likely to succeed than those that entere d later. 
Eighty-three companies entered the U.S. disk drive industry between 1976 and 1993. Thirty-five of 
these were diversified concerns, such as Memorex, A mpex, 3M, and Xerox, that made other computer 
peripheral equipment or other magnetic recording pr oducts. Forty-eight were independent startup 
companies, many being financed by venture capital a nd headed by people who previously had worked 
for other firms in the industry. These numbers repr esent the complete census of all firms that ever we re 
incorporated and/or were known to have announced th e design of a hard drive, whether or not they 
actually sold any. It is not a statistical sample o f firms that might be biased in favor or against an y type 
of firm. 
 
  
Figure 6.2  Relationship between Order of Adoption of Thin-Fil m Technology and Areal Density of 
Highest-Performance 1989 Model  
  105  
 
 
Source: Clayton M. Christensen, “Exploring the Limits of th e Technology S-Curve. Part I: Component 
Technologies,” Production and Operations Management 1, no. 4 (Fall 1992): 347. Reprinted by 
permission.  
 
  
The entry strategies employed by each of these firm s can be characterized along the two axes in Table 
6.1. The vertical axis describes technology  strategies, with firms at the bottom using only pr oven 
technologies in their initial products and those at  the top using one or more new component 
technologies. 3 The horizontal axis charts market  strategies, with firms at the left having entered already 
established value networks and those at the right h aving entered emerging value networks. 4 Another 
way to characterize this matrix is to note that com panies that were agressive at entry in developing a nd 
adopting sustaining innovations appear in the two t op boxes, left and right, while companies that led at 
entry in creating new value networks appear in the two right-hand boxes, top and bottom. The 
companies in the right boxes include all  companies that attempted to create new value netwo rks, even 
those networks that did not materialize into substa ntial markets (such as removable hard drives).  
 
  
Table 6.1 Disk Drive Companies Achieving $100 Milli on in Annual Revenues in at Least One Year 
Between 1976 and 1984  
  106  
 
 
Source: Data are from various issues of Disk/Trend Report.   
Note:  S indicates success, F indicates failure, N indica tes no, T indicates total.  
 
  
Each quadrant displays the number of companies that  entered using the strategy represented. Under the 
S (for “success”) are the number of firms that succe ssfully generated $100 million in revenues in at 
least one year, even if the firm subsequently faile d; F (for “failure”) shows the number of firms that 
failed ever to reach the $100 million revenue thres hold and that have subsequently exited the industry ; 
N (for “no”) indicates the number of firms for which  there is as yet no verdict because, while still 
operating in 1994, they had not yet reached $100 mi llion in sales; and T (for “total”) lists the total 
number of firms that entered in each category. 5 The column labeled “% Success” indicates the 
percentage of the total number of firms that reache d $100 million in sales. Finally, beneath the matri x 
are the sums of the data in the two quadrants above . 
The numbers beneath the matrix show that only three  of the fifty-one firms (6 percent) that entered 
established markets ever reached the $100 million r evenue benchmark. In contrast, 37 percent of the 
firms that led in disruptive technological innovati on—those entering markets that were less than two 
years old—surpassed the $100 million level, as show n on the right side of Table 6.1. Whether a firm  107  was a start-up or a diversified firm had little imp act on its success rate. What mattered appears not to 
have been its organizational form, but whether it w as a leader in introducing disruptive products and 
creating the markets in which they were sold. 6 
Only 13 percent of the firms that entered attemptin g to lead in sustaining component technologies (the  
top half of the matrix) succeeded, while 20 percent  of the firms that followed were successful. Clearl y, 
the lower-right quadrant offered the most fertile g round for success. 
The cumulative sales numbers in the right-most colu mns in each quadrant show the total, cumulative 
revenues logged by all firms pursuing each of the s trategies; these are summarized below the matrix. 
The result is quite stunning. The firms that led in  launching disruptive products together logged a 
cumulative total of $62 billion dollars in revenues  between 1976 and 1994. 7 Those that followed into 
the markets later, after those markets had become e stablished, logged only $3.3 billion in total reven ue. 
It is, indeed, an innovator’s dilemma. Firms that s ought growth by entering small, emerging markets 
logged twenty times  the revenues of the firms pursuing growth in large r markets. The difference in 
revenues per firm is even more striking: The firms that followed late into the markets enabled by 
disruptive technology, on the left half of the matr ix, generated an average cumulative total of $64.5 
million per firm. The average  company that led in disruptive technology generate d $1.9 billion  in 
revenues. The firms on the left side seem to have m ade a sour bargain. They exchanged a market risk,  
the risk that an emerging market for the disruptive  technology might not develop after all, for a 
competitive risk,  the risk of entering markets against entrenched co mpetition. 8 
 
 
COMPANY SIZE AND LEADERSHIP IN DISRUPTIVE TECHNOLOG IES   
 
Despite evidence that leadership in disruptive inno vation pays such huge dividends, established firms,  
as shown in the first four chapters of this book, o ften fail to take the lead. Customers of establishe d 
firms can hold the organizations captive, working t hrough rational, well-functioning resource allocati on 
processes to keep them from commercializing disrupt ive technologies. One cruel  additional  disabling 
factor that afflicts established firms as they work  to maintain their growth rate is that the larger a nd 
more successful they become, the more difficult it is to muster the rationale for entering an emerging  
market in its early stages, when the evidence above  shows that entry is so crucial. 
Good managers are driven to keep their organization s growing for many reasons. One is that growth 
rates have a strong effect on share prices. To the extent that a company’s stock price represents the 
discounted present value of some consensus forecast  of its future earnings stream, then the level  of the 
stock price—whether it goes up or down—is driven by  changes in the projected rate of growth  in 
earnings. 9 In other words, if a company’s current share price  is predicated on a consensus growth 
forecast of 20 percent, and the market’s consensus for growth is subsequently revised downward to 15 
percent growth, then the company’s share price will  likely fall —even though its revenues and earnings 
will still be growing at a healthy rate. A strong a nd increasing stock price, of course, gives a compa ny 
access to capital on favorable terms; happy investo rs are a great asset to a company. 
Rising share prices make stock option plans an inex pensive way to provide incentive to and to reward 
valuable employees. When share prices stagnate or f all, options lose their value. In addition, company  
growth creates room at the top for high-performing employees to expand the scope of their 
responsibilities. When companies stop growing, they  begin losing many of their most promising future 
leaders, who see less opportunity for advancement.  108  Finally, there is substantial evidence that growing  companies find it much easier to justify investmen ts 
in new product and process technologies than do com panies whose growth has stopped. 10  
Unfortunately, companies that become large and succ essful find that maintaining growth becomes 
progressively more difficult. The math is simple: A  $40 million company that needs to grow profitably 
at 20 percent to sustain its stock price and organi zational vitality needs an additional $8 million in  
revenues the first year, $9.6 million the following  year, and so on; a $400 million company with a 20 
percent targeted growth rate needs new business wor th $80 million in the first year, $96 million in th e 
next, and so on; and a $4 billion company with a 20  percent goal needs to find $800 million, $960 
million, and so on, in each successive year. 
This problem is particularly vexing for big compani es confronting disruptive technologies. Disruptive 
technologies facilitate the emergence of new market s, and there are no $800 million emerging markets. 
But it is precisely when emerging markets are small —when they are least  attractive to large companies 
in search of big chunks of new revenue—that entry i nto them is so critical. 
How can a manager of a large, successful company de al with these realities of size and growth when 
confronted by disruptive change? I have observed th ree approaches in my study of this problem: 
 
1. Try to affect the growth rate of the emerging ma rket, so that it becomes big enough, fast 
enough, to make a meaningful dent on the trajectory  of profit and revenue growth of a large 
company.  
2. Wait until the market has emerged and become bet ter defined, and then enter after it “has 
become large enough to be interesting.”  
3. Place responsibility to commercialize disruptive  technologies in organizations small enough 
that their performance will be meaningfully affecte d by the revenues, profits, and small orders 
flowing from the disruptive business in its earlies t years.  
As the following case studies show, the first two a pproaches are fraught with problems. The third has 
its share of drawbacks too, but offers more evidenc e of promise. 
 
 
CASE STUDY: PUSHING THE GROWTH RATE OF AN EMERGING MARKET   
 
The history of Apple Computer’s early entry into th e hand-held computer, or personal digital assistant  
(PDA), market helps to clarify the difficulties con fronting large companies in small markets. 
Apple Computer introduced its Apple I in 1976. It w as at best a preliminary product with limited 
functionality, and the company sold a total of 200 units at $666 each before withdrawing it from the 
market. But the Apple I wasn’t a financial disaster . Apple had spent modestly on its development, and 
both Apple and its customers learned a lot about ho w desktop personal computers might be used. Apple 
incorporated this learning into its Apple II comput er, introduced in 1977, which was highly successful . 
Apple sold 43,000 Apple II computers in the first t wo years they were on the market, 11  and the 
product’s success positioned the company as the lea der in the personal computer industry. On the basis  
of the Apple II’s success Apple went public in 1980 .  109  A decade after the release of the Apple II, Apple C omputer had grown into a $5 billion company, and 
like all large and successful companies, it found i tself having to add large chunks of revenue each ye ar 
to preserve its equity value and organizational vit ality. In the early 1990s, the emerging market for 
hand-held PDAs presented itself as a potential vehi cle for achieving that needed growth. In many ways,  
this opportunity, analogous to that in 1978 when th e Apple II computer helped shape its industry, was a 
great fit for Apple. Apple’s distinctive design exp ertise was in user-friendly products, and user-
friendliness and convenience were the basis of the PDA concept. 
How did Apple approach this opportunity? Aggressive ly. It invested scores of millions of dollars to 
develop its product, dubbed the “Newton.” The Newto n’s features were defined through one of the 
most thoroughly executed market research efforts in  corporate history; focus groups and surveys of 
every type were used to determine what features con sumers would want. The PDA had many of the 
characteristics of a disruptive computing technolog y, and recognizing the potential problems, Apple 
CEO John Sculley made the Newton’s development a pe rsonal priority, promoting the product widely, 
and ensuring that the effort got the technical and financial resources it needed. 
Apple sold 140,000 Newtons in 1993 and 1994, its fi rst two years on the market. Most observers, of 
course, viewed the Newton as a big flop. Technicall y, its handwriting recognition capabilities were 
disappointing, and its wireless communications tech nologies had made it expensive. But what was 
most damning was that while Sculley had publicly po sitioned the Newton as a key product to sustain 
the company’s growth, its first-year sales amounted  to about 1 percent of Apple’s revenues. Despite al l 
the effort, the Newton made hardly a dent in Apple’ s need for new growth. 
But was the Newton a failure? The timing of Newton’ s entry into the handheld market was akin to the 
timing of the Apple II into the desktop market. It was a market-creating, disruptive product targeted at 
an undefinable set of users whose needs were unknow n to either themselves or Apple. On that basis, 
Newton’s sales should have been a pleasant surprise  to Apple’s executives: It outsold the Apple II in its 
first two years by a factor of more than three to o ne. But while selling 43,000 units was viewed as an  
IPO-qualifying triumph in the smaller Apple of 1979 , selling 140,000 Newtons was viewed as a failure 
in the giant Apple of 1994. 
As chapter 7 will show, disruptive technologies oft en enable something to be done that previously had 
been deemed impossible. Because of this, when they initially emerge, neither manufacturers nor 
customers know how or why the products will be used , and hence do not know what specific features 
of the product will and will not ultimately be valu ed. Building such markets entails a process of mutu al 
discovery by customers and manufacturers—and this s imply takes time. In Apple’s development of the 
desktop computer, for example, the Apple I failed, the first Apple II was lackluster, and the Apple II+ 
succeeded. The Apple III was a market failure becau se of quality problems, and the Lisa was a failure.  
The first two generations of the Macintosh computer  also stumbled. It wasn’t until the third iteration  of 
the Macintosh that Apple and its customers finally found “it”: the standard for convenient, user-
friendly computing to which the rest of the industr y ultimately had to conform. 12  
In launching the Newton, however, Apple was despera te to short-circuit this coalescent process for 
defining the ultimate product and market. It assume d that its customers knew what they wanted and 
spent very aggressively to find out what this was. (As the next chapter will show, this is impossible. ) 
Then to give customers what they thought they wante d, Apple had to assume the precarious role of a 
sustaining technology leader in an emerging industr y. It spent enormous sums to push mobile data 
communications and handwriting recognition technolo gies beyond the state of the art. And finally, it 
spent aggressively to convince people to buy what i t had designed.  110  Because emerging markets are small by definition, t he organizations competing in them must be able to 
become profitable at small scale. This is crucial b ecause organizations or projects that are perceived  as 
being profitable and successful can continue to att ract financial and human resources both from their 
corporate parents and from capital markets. Initiat ives perceived as failures have a difficult time 
attracting either. Unfortunately, the scale of the investments Apple made in its Newton in order to 
hasten the emergence of the PDA market made it very  difficult to earn an attractive return. Hence, the  
Newton came to be broadly viewed as a flop. 
As with most business disappointments, hindsight re veals the faults in Apple’s Newton project. But I 
believe that the root cause of Apple’s struggle was  not  inappropriate management. The executives’ 
actions were a symptom of a deeper problem: Small m arkets cannot satisfy the near-term growth 
requirements of big organizations. 
 
 
CASE STUDY: WAITING UNTIL A MARKET IS LARGE ENOUGH TO BE INTERESTING   
 
A second way that many large companies have respond ed to the disruptive technology trap is to wait 
for emerging markets to “get large enough to be int eresting” before they enter. Sometimes this works, 
as IBM’s well-timed 1981 entry into the desktop PC business demonstrated. But it is a seductive logic 
that can backfire, because the firms creating new m arkets often forge capabilities that are closely 
attuned to the requirements of those markets and th at later entrants find difficult to replicate. Two 
examples from the disk drive industry illustrate th is problem. 
Priam Corporation, which ascended to leadership of the market for 8-inch drives sold to minicomputer 
makers after its entry in 1978, had built the capab ility in that market to develop its drives on a two -year 
rhythm. This pace of new product introduction was c onsistent with the rhythm by which its customers, 
minicomputer makers, introduced their new products into the market. 
Seagate’s first 5.25-inch drive, introduced to the emerging desktop market in 1980, was disruptively 
slow compared to the performance of Priam’s drives in the minicomputer market. But by 1983, Seagate 
and the other firms that led in implementing the di sruptive 5.25-inch technology had developed a one-
year  product introduction rhythm in their market. Becau se Seagate and Priam achieved similar 
percentage improvements in speed with each new prod uct generation, Seagate, by introducing new 
generations on a one-year rhythm, quickly began to converge on Priam’s performance advantage. 
Priam introduced its first 5.25-inch drive in 1982.  But the rhythm by which it introduced its subseque nt 
5.25-inch models was the two-year capability it had  honed in the minicomputer market—not the one-
year cycle required to compete in the desktop marke tplace. As a consequence, it was never able to 
secure a single  major OEM order from a desktop computer manufactur er: It just couldn’t hit their 
design windows with its new products. And Seagate, by taking many more steps forward than did 
Priam, was able to close the performance gap betwee n them. Priam closed its doors in 1990. 
The second example occurred in the next disruptive generation. Seagate Technology was the second in 
the industry to develop a 3.5-inch drive in 1984. A nalysts at one point had speculated that Seagate 
might ship 3.5-inch drives as early as 1985; and in deed, Seagate showed a 10 MB model at the fall 
1985 Comdex Show. When Seagate still had not shippe d a 3.5-inch drive by late 1986, CEO Al 
Shugart explained, “So far, there just isn’t a big enough market for it, as yet.” 13  In 1987, when the 3.5-
inch market at $1.6 billion had gotten “big enough to be interesting,” Seagate finally launched its  111  offering. By 1991, however, even though Seagate had  by then built substantial volume in 3.5-inch 
drives, it had not yet succeeded in selling a singl e drive to a maker of portable computers: Its model s 
were all sold into the desktop market, defensively cannibalizing its sales of 5.25-inch drives. Why? 
One likely reason for this phenomenon is that Conne r Peripherals, which pioneered and maintained the 
lead in selling 3.5-inch drives to portable compute r makers, fundamentally changed the way drive 
makers had to approach the portables market. As one  Conner executive described it, 
From the beginning of the OEM disk drive industry, product development had proceeded in three 
sequential steps. First you designed the drive; the n you made it; and then you sold it. We changed all  
that. We first sell  the drives; then we design them; and then we build  them. 14  
In other words, Conner set a pattern whereby drives  for the portable computer market were custom-
designed for major customers. And it refined a set of capabilities in its marketing, engineering, and 
manufacturing processes that were tailored to that pattern. 15  Said another Conner executive, “Seagate 
was never able to figure out how to sell drives in the portable market. They just never got it.” 16  
 
 
CASE STUDY: GIVING SMALL OPPORTUNITIES TO SMALL ORG ANIZATIONS   
 
Every innovation is difficult. That difficulty is c ompounded immeasurably, however, when a project is 
embedded in an organization in which most people ar e continually questioning why the project is being 
done at all. Projects make sense to people if they address the needs of important customers, if they 
positively impact the organization’s needs for prof it and growth, and if participating in the project 
enhances the career opportunities of talented emplo yees. When a project doesn’t have these 
characteristics, its manager spends much time and e nergy justifying why it merits resources and cannot  
manage the project as effectively. Frequently in su ch circumstances, the best people do not want to be  
associated with the project—and when things get tig ht, projects viewed as nonessential are the first t o 
be canceled or postponed. 
Executives can give an enormous boost to a project’ s probability of success, therefore, when they 
ensure that it is being executed in an environment in which everyone involved views the endeavor as 
crucial to the organization’s future growth and pro fitability. Under these conditions, when the 
inevitable disappointments, unforeseen problems, an d schedule slippages occur, the organization will 
be more likely to find ways to muster whatever is r equired to solve the problem. 
As we have seen, a project to commercialize a disru ptive technology in a small, emerging market is 
very unlikely to be considered essential to success  in a large company; small markets don’t solve the 
growth problems of big companies. Rather than conti nually working to convince and remind everyone 
that the small, disruptive technology might someday  be significant or that it is at least strategicall y 
important, large companies should seek to embed the  project in an organization that is small enough to  
be motivated by the opportunity offered by a disrup tive technology in its early years. This can be don e 
either by spinning out an independent organization or by acquiring an appropriately small company. 
Expecting achievement-driven employees in a large o rganization to devote a critical mass of resources,  
attention, and energy to a disruptive project targe ted at a small and poorly defined market is equival ent 
to flapping one’s arms in an effort to fly: It deni es an important tendency in the way organizations 
work. 17   112  There are many success stories to the credit of thi s approach. Control Data, for example, which had 
essentially missed the 8-inch disk drive generation , sent a group to Oklahoma City to commercialize it s 
5.25-inch drive. In addition to CDC’s need to escap e the power of its mainstream customers, the firm 
explicitly wanted to create an organization whose s ize matched the opportunity. “We needed an 
organization,” reflected one manager, “that could g et excited about a $50,000 order. In Minneapolis 
[which derived nearly $1 billion from the sale of 1 4-inch drives in the mainframe market] you needed a  
million-dollar order just to turn anyone’s head.” C DC’s Oklahoma City venture proved to be a 
significant success. 
Another way of matching the size of an organization  to the size of the opportunity is to acquire a sma ll 
company within which to incubate the disruptive tec hnology. This is how Allen Bradley negotiated its 
very successful disruptive transition from mechanic al to electronic motor controls. 
For decades the Allen Bradley Company (AB) in Milwa ukee has been the undisputed leader in the 
motor controls industry, making heavy-duty, sophist icated switches that turn large electric motors off  
and on and protect them from overloads and surges i n current. AB’s customers were makers of 
machine tools and cranes as well as contractors who  installed fans and pumps for industrial and 
commercial heating, ventilating, and air conditioni ng (HVAC) systems. Motor controls were 
electromechanical devices that operated on the same  principle as residential light switches, although on 
a larger scale. In sophisticated machine tools and HVAC systems, electric motors and their controls 
were often linked, through systems of electromechan ical relay switches, to turn on and off in particul ar 
sequences and under particular conditions. Because of the value of the equipment they controlled and 
the high cost of equipment downtime, controls were required to be rugged, capable of turning on and 
off millions of times and of withstanding the vibra tions and dirt that characterized the environments in 
which they were used. 
In 1968, a startup company, Modicon, began selling electronic programmable motor controls—a 
disruptive technology from the point of view of mai nstream users of electromechanical controls. Texas 
Instruments (TI) entered the fray shortly thereafte r with its own electronic controller. Because early  
electronic controllers lacked the real and perceive d ruggedness and robustness for harsh environments 
of the hefty AB-type controllers, Modicon and TI we re unable to sell their products to mainstream 
machine tool makers and HVAC contractors. As perfor mance was measured in the mainstream 
markets, electronic products underperformed convent ional controllers, and few mainstream customers 
needed the programmable flexibility offered by elec tronic controllers. 
As a consequence, Modicon and TI were forced to cul tivate an emerging market for programmable 
controllers: the market for factory automation. Cus tomers in this emerging market were not equipment 
manufacturers, but equipment users,  such as Ford and General Motors, who were just beg inning their 
attempt to integrate pieces of automatic manufactur ing equipment. 
Of the five leading manufacturers of electromechani cal motor controls—Allen Bradley, Square D, 
Cutler Hammer, General Electric, and Westinghouse—o nly Allen Bradley retained a strong market 
position as programmable electronic controls improv ed in ruggedness and began to invade the core 
motor control markets. Allen Bradley entered the el ectronic controller market just two years after 
Modicon and built a market-leading position in the new technology within a few years, even as it kept 
its strength in its old electromechanical products.  It subsequently transformed itself into a major 
supplier of electronic controllers for factory auto mation. The other four companies, by contrast, 
introduced electronic controllers much later and su bsequently either exited the controller business or  
were reduced to weak positions. From a capabilities  perspective this is a surprising outcome, because  113  General Electric and Westinghouse had much deeper e xpertise in microelectronics technologies at that 
time than did Allen Bradley, which had no instituti onal experience in the technology. 
What did Allen Bradley do differently? In 1969, jus t one year after Modicon entered the market, AB 
executives bought a 25 percent interest in Informat ion Instruments, Inc., a fledgling programmable 
controller start-up based in Ann Arbor, Michigan. T he following year it purchased outright a nascent 
division of Bunker Ramo, which was focused on progr ammable electronic controls and their emerging 
markets. AB combined these acquisitions into a sing le unit and maintained it as a business separate 
from its mainstream electromechanical products oper ation in Milwaukee. Over time, the electronics 
products have significantly eaten into the electrom echanical controller business, as one AB division 
attacked the other. 18  By contrast, each of the other four companies trie d to manage its electronic 
controller businesses from within its mainstream el ectromechanical divisions, whose customers did not 
initially need or want electronic controls. Each fa iled to develop a viable position in the new 
technology. 
Johnson & Johnson has with great success followed a  strategy similar to Allen Bradley’s in dealing 
with disruptive technologies such as endoscopic sur gical equipment and disposable contact lenses. 
Though its total revenues amount to more than $20 b illion, J&J comprises 160 autonomously operating 
companies, which range from its huge MacNeil and Ja nssen pharmaceuticals companies to small 
companies with annual revenues of less than $20 mil lion. Johnson & Johnson’s strategy is to launch 
products of disruptive technologies through very sm all companies acquired for that purpose. 
 
 
SUMMARY   
 
It is not crucial for managers pursuing growth and competitive advantage to be leaders in every elemen t 
of their business. In sustaining technologies, in f act, evidence strongly suggests that companies whic h 
focus on extending the performance of conventional technologies, and choose to be followers in 
adopting new ones, can remain strong and competitiv e. This is not the case with disruptive 
technologies, however. There are enormous returns a nd significant first-mover advantages associated 
with early entry into the emerging markets in which  disruptive technologies are initially used. Disk 
drive manufacturers that led in commercializing dis ruptive technology grew at vastly greater rates tha n 
did companies that were disruptive technology follo wers. 
Despite the evidence that leadership in commerciali zing disruptive technologies is crucial, large, 
successful innovators encounter a significant dilem ma in the pursuit of such leadership. In addition t o 
dealing with the power of present customers as disc ussed in the last chapter, large, growth-oriented 
companies face the problem that small markets don’t  solve the near-term growth needs of large 
companies. The markets whose emergence is enabled b y disruptive technologies all began as small 
ones. The first orders that the pioneering companie s received in those markets were small ones. And 
the companies that cultivated those markets had to develop cost structures enabling them to become 
profitable at small scale. Each of these factors ar gues for a policy of implanting projects to 
commercialize disruptive innovations in small organ izations that will view the projects as being on 
their critical path to growth and success, rather t han as being distractions from the main business of  the 
company.  114  This recommendation is not new, of course; a host o f other management scholars have also argued that 
smallness and independence confer certain advantage s in innovation. It is my hope that chapters 5 and 
6 provide deeper insight about why and under what c ircumstances this strategy is appropriate. 
 
 
NOTES   
 
1.  The benefits of persistently pursuing incremental improvements versus taking big strategic leaps 
have been capably argued by Robert Hayes in “Strate gic Planning: Forward in Reverse?” Harvard 
Business Review,  November–December, 1985, 190–197.  
I believe that there are some specific situations i n which leadership in sustaining technology is cruc ial, 
however. In a private conversation, Professor Kim C lark characterized these situations as those 
affecting knife-edge  businesses, that is, businesses in which the basis  of competition is simple and 
unidimensional and there is little room for error. An example of such a knife-edge industry is the 
photolithographic aligner (PLA) industry, studied b y Rebecca M. Henderson and Kim B. Clark, in 
“Architectural Innovation: The Reconfiguration of E xisting Systems and the Failure of Established 
Firms,” Administrative Science Quarterly  (35), March, 1990, 9–30. In this case, aligner man ufacturers 
failed when they fell behind technologically in the  face of sustaining architectural changes. This is 
because the basis of competition in the PLA industr y was quite straightforward even though the 
products themselves were very complex: products eit her made the narrowest line width on silicon 
wafers of any in the industry or no one bought them . This is because PLA customers, makers of 
integrated circuits, simply had to have the fastest  and most capable photolithographic alignment 
equipment or they could not remain competitive in t heir own markets. The knife-edge existed because 
product functionality was the only basis of competi tion: PLA manufacturers would either fall off one 
side to rapid success or off the other side to fail ure. Clearly, such knife-edge situations make leade rship 
in sustaining technology very important. 
In most other sustaining situations, however, leade rship is not  crucial. This far more common situation 
is the subject of Richard S. Rosenbloom’s study of the transition by National Cash Register from 
electro-mechanical to electronic technology. (See R ichard S. Rosenbloom, “From Gears to Chips: The 
Transformation of NCR and Harris in the Digital Era ,” Working paper, Harvard Business School 
Business History Seminar, 1988). In this case, NCR was very late in its industry in developing and 
launching a line of electronic cash registers. So l ate was NCR with this technology, in fact, that its  sales 
of new cash registers dropped essentially to zero f or an entire year in the early 1980s. Nonetheless, the 
company had such a strong field service capability that it survived by serving its installed base for the 
year it took to develop and launch its electronic c ash registers. NCR then leveraged the strength of i ts 
brand name and field sales presence to quickly reca pture its share of the market. 
Even though a cash register is a simpler machine th an a photolithographic aligner, I would characteriz e 
its market as complex, in that there are multiple b ases of competition, and hence multiple ways to 
survive. As a general rule, the more complex a mark et, the less important is leadership in sustaining 
technological innovations. It is in dealing with kn ife-edge markets or with disruptive technologies th at 
leadership appears to be crucial. I am indebted to Professors Kim B. Clark and Robert Hayes for their 
contributions to my thinking on this topic. 
 
2.  This is not to say that firms whose product perfor mance or product cost consistently lagged behind  115  the competition were able to prosper. I assert that  there is no evidence that leadership in sustaining  
technological innovation confers a discernible and enduring competitive advantage over companies that 
have adopted a follower strategy because there are numerous ways to “skin the cat” in improving the 
performance of a complex product such as a disk dri ve. Developing and adopting new component 
technologies, such as thin-film and magneto-resisti ve heads, is one way to improve performance, but 
there are innumerable other avenues for extending t he performance of conventional technologies while 
waiting for new approaches to become better underst ood and more reliable. This argument is presented 
more fully in Clayton M. Christensen, “Exploring th e Limits of the Technology S-Curve,” Production 
and Operations Management  (1), 1992, 334–366.  
3.  For the purposes of this analysis, a technology wa s classed as “new or unproven” if less than two 
years had elapsed from the time it had first appear ed in a product that was manufactured and sold by a  
company somewhere in the world or if, even though i t had been in the market for more than two years, 
less than 20 percent of the disk drive makers had u sed the technology in one of their products.  
4.  In this analysis, emerging markets  or value networks were those in which two years or  less had 
elapsed since the first rigid disk drive had been u sed with that class of computers; established markets  
or value networks were those in which more than two  years had elapsed since the first drive was used.  
5.  Entry by acquisition was a rare route of entry in the disk drive industry. Xerox followed this strate gy, 
acquiring Diablo, Century Data, and Shugart Associa tes. The performance of these companies after 
acquisition was so poor that few other companies fo llowed Xerox’s lead. The only other example of 
entry by acquisition was the acquisition of Tandon by Western Digital, a manufacturer of controllers. 
In the case of Xerox and Western Digital, the entry  strategy of the firms they acquired  is recorded in 
Table 6.1. Similarly, the start-up of Plus Developm ent Corporation, a spin-out of Quantum, appears in 
Table 6.1 as a separate company.  
6.  The evidence summarized in this matrix may be of s ome use to venture capital investors, as a general 
way to frame the riskiness of proposed investments.  It suggests that start-ups which propose to 
commercialize a breakthrough technology that is ess entially sustaining in character have a far lower 
likelihood of success than start-ups whose vision i s to use proven technology to disrupt an establishe d 
industry with something that is simpler, more relia ble, and more convenient. The established firms in 
an industry have every incentive to catch up with a  supposed sustaining technological breakthrough, 
while they have strong disincentives to pursue disr uptive initiatives.  
7.  Not all of the small, emerging markets actually be came large ones. The market for removable drive 
modules, for example, remained a small niche for mo re than a decade, only beginning to grow to 
significant size in the mid-1990s. The conclusion i n the text that emerging markets offer a higher 
probability for success reflects the average, not a n invariant result.  
8.  The notions that one ought not accept the risks of  innovating simultaneously along both market and 
technology dimensions are often discussed among ven ture capitalists. It is also a focus of chapter 5 i n 
Lowell W. Steele, Managing Technology  (New York: McGraw Hill, 1989). The study reported here of 
the posterior probabilities of success for differen t innovation strategies builds upon the concepts of  
Steele and Lyle Ochs (whom Steele cites). I was als o stimulated by ideas presented in Allan N. Afuah 
and Nik Bahram, “The Hypercube of Innovation,” Research Policy  (21), 1992.  
9.  The simplest equation used by financial analysts t o determine share price is P = D/(C-G),  where P = 
price per share, D = dividends per share, C = the company’s cost of capital, and G = projected long-
term growth rate.  
10.  This evidence is summarized by Clayton M. Christen sen in “Is Growth an Enabler  of Good 
Management, or the Result  of It?” Harvard Business School working paper, 199 6.  
11.  Scott Lewis, “Apple Computer, Inc.,” in Adele Hast , ed., International Directory of Company 
Histories  (Chicago: St. James Press, 1991), 115–116.  
12.  An insightful history of the emergence of the pers onal computer industry appears in Paul Frieberger 
and Michael Swaine, Fire in the Valley: The Making of the Personal Comp uter  (Berkeley, CA: 
Osborne-McGraw Hill, 1984).   116  13.  “Can 3.5" Drives Displace 5.25s in Personal Comput ing?” Electronic Business,  1 August, 1986, 
81–84.  
14.  Personal interview with Mr. William Schroeder, Vic e Chairman, Conner Peripherals Corporation, 
November 19, 1991.  
15.  An insightful study on the linkage among a company ’s historical experience, its capabilities, and 
what it consequently can and cannot do, appears in Dorothy Leonard-Barton, “Core Capabilities and 
Core Rigidities: A Paradox in Managing New Product Development,” Strategic Management Journal  
(13), 1992, 111–125.  
16.  Personal interview with Mr. John Squires, cofounde r and Executive Vice President, Conner 
Peripherals Corporation, April 27, 1992.  
17.  See, for example, George Gilder, “The Revitalizati on of Everything: The Law of the Microcosm,” 
Harvard Business Review,  March-April, 1988, 49–62.  
18.  Much of this information about Allen Bradley has b een taken from John Gurda, The Bradley 
Legacy  (Milwaukee: The Lynde and Harry Bradley Foundation , 1992). 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  117   
CHAPTER SEVEN  
Discovering New and  
Emerging Markets 
 
 
Markets that do not exist cannot be analyzed: Suppl iers and customers must discover them together. 
Not only are the market applications for disruptive  technologies unknown  at the time of their 
development, they are unknowable.  The strategies and plans that managers formulate f or confronting 
disruptive technological change, therefore, should be plans for learning and discovery rather than pla ns 
for execution. This is an important point to unders tand, because managers who believe they know a 
market’s future will plan and invest very different ly from those who recognize the uncertainties of a 
developing market.  
Most managers learn about innovation in a sustaining technology context  because most technologies 
developed by established companies are sustaining i n character. Such innovations are, by definition, 
targeted at known markets in which customer needs a re understood. In this environment, a planned, 
researched approach to evaluating, developing, and marketing innovative products is not only possible,  
it is critical to success. 
What this means, however, is that much of what the best executives in successful companies have 
learned about managing innovation is not relevant t o disruptive technologies. Most marketers, for 
example, have been schooled extensively, at univers ities and on the job, in the important art of liste ning 
to their customers, but few have any theoretical or  practical training in how to discover markets that  do 
not yet exist. The problem with this lopsided exper ience base is that when the same analytical and 
decision-making processes learned in the school of sustaining innovation are applied to enabling or 
disruptive technologies, the effect on the company can be paralyzing. These processes demand crisply 
quantified information when none exists, accurate e stimates of financial returns when neither revenues  
nor costs can be known, and management according to  detailed plans and budgets that cannot be 
formulated. Applying inappropriate marketing, inves tment, and management processes can render good 
companies incapable of creating the new markets in which enabling or disruptive technologies are first  
used. 
In this chapter we shall see how experts in the dis k drive industry were able to forecast the markets for 
sustaining technologies with stunning accuracy but had great difficulty in spotting the advent and 
predicting the size of new markets for disruptive i nnovations. Additional case histories in the 
motorcycle and microprocessor industries further de monstrate the uncertainty about emerging market 
applications for disruptive or enabling technologie s, even those that, in retrospect, appear obvious. 
 
 
FORECASTING MARKETS FOR SUSTAINING VERSUS DISRUPTIV E TECHNOLOGIES   
 
An unusual amount of market information has been av ailable about the disk drive industry from its 
earliest days—a major reason why studying it has yi elded such rich insights. The primary source of  118  data, Disk/Trend Report,  published annually by Disk/Trend, Inc., of Mountai n View, California, lists 
every model of disk drive that has ever been offere d for sale by any company in the world, for each of  
the years from 1975 to the present. It shows the mo nth and year in which each model was first shipped,  
lists the performance specifications of the drive, and details the component technologies used. In 
addition, every manufacturer in the world shares wi th Disk/Trend  its sales by product type, with 
information about what types of customers bought wh ich drive. Editors at Disk/Trend  then aggregate 
this data to derive the size of each narrowly defin ed market segment and publish a listing of the majo r 
competitors’ shares, carefully guarding all proprie tary data. Manufacturers in the industry find the 
reports so valuable that they all continue to share  their proprietary data with Disk/Trend.  
In each edition, Disk/Trend  publishes the actual unit volumes and dollar sales  in each market segment 
for the year just past and offers its forecasts for  each of the next four years in each category. Give n its 
unparalleled access to industry data spanning two d ecades, this publication offers an unusual chance t o 
test through unfolding market history the accuracy of past predictions. Over all, Disk/Trend  has a 
remarkable track record in forecasting the future o f established markets, but it has struggled to esti mate 
accurately the size of new markets enabled by disru ptive disk drive technologies. 
 
  
Figure 7.1 The Four Years after the First Commercial Shipments : Sustaining versus Disruptive 
Technologies  
 
 
 
Source:  Data are from various issues of Disk/Trend Report.   
 
  
The evidence is summarized in Figure 7.1, which com pares the total unit volumes that Disk/Trend 
Report  had forecast would be shipped in the first four ye ars after commercial shipments of each new 
disk drive architecture began, to the total volumes  that were actually shipped over that four-year per iod. 
To facilitate comparison, the heights of the bars m easuring forecast shipments were normalized to a 
value of 100, and the volumes actually shipped were  scaled as a percentage of the forecast. Of the fiv e  119  new architectures for which Disk/Trend ’s forecasts were available, the 14-inch Winchester  and the 2.5-
inch generation were sustaining innovations, which were sold into the same value networks as the 
preceding generation of drives. The other three, 5. 25-, 3.5-, and 1.8-inch drives, were disruptive 
innovations that facilitated the emergence of new v alue networks. ( Disk/Trend  did not publish separate 
forecasts for 8-inch drives.) 
Notice that Disk/Trend ’s forecasts for the sustaining 2.5-inch and 14-inc h Winchester technologies 
were within 8 percent and 7 percent, respectively, of what the industry actually shipped. But its 
estimates were off by 265 percent for 5.25-inch dri ves, 35 percent for 3.5-inch drives (really quite 
close), and 550 percent for 1.8-inch drives. Notabl y, the 1.8-inch drive, the forecast of which 
Disk/Trend  missed so badly, was the first generation of drive s with a primarily non-computer market. 
The Disk/Trend  staff used the same methods to generate the foreca sts for sustaining architectures as 
they did for disruptive ones: interviewing leading customers and industry experts, trend analysis, 
economic modeling, and so on. The techniques that w orked so extraordinarily well when applied to 
sustaining technologies, however, clearly failed ba dly when applied to markets or applications that di d 
not yet exist. 
 
 
IDENTIFYING THE MARKET FOR THE HP 1.3-INCH KITTYHAW K DRIVE   
 
Differences in the forecastablity of sustaining ver sus disruptive technologies profoundly affected 
Hewlett-Packard’s efforts to forge a market for its  revolutionary, disruptive 1.3-inch Kittyhawk disk 
drive. 1 In 1991, Hewlett-Packard’s Disk Memory Division (D MD), based in Boise, Idaho, generated 
about $600 million in disk drive revenues for its $ 20 billion parent company. That year a group of 
DMD employees conceived of a tiny, 1.3-inch 20 MB d rive, which they code-named Kittyhawk. This 
was indeed a radical program for HP: The smallest d rive previously made by DMD had been 3.5-
inches, and DMD had been one of the last in the ind ustry to introduce one. The 1.3-inch Kittyhawk 
represented a significant leapfrog for the company— and, most notably, was HP’s first attempt to lead 
in a disruptive technology. 
For the project to make sense in a large organizati on with ambitious growth plans, HP executives 
mandated that Kittyhawk’s revenues had to ramp to $ 150 million within three years. Fortunately for 
Kittyhawk’s proponents, however, a significant mark et for this tiny drive loomed on the horizon: hand-
held palm-top computers, or personal digital assist ants (PDAs). Kittyhawk’s sponsors, after studying 
projections for this market, decided that they coul d scale the revenue ramp that had been set for them . 
They consulted a market research firm, which confir med HP’s belief that the market for Kittyhawk 
would indeed be substantial. 
HP’s marketers developed deep relationships with se nior executives at major companies in the 
computer industry, for example, Motorola, ATT, IBM,  Apple, Microsoft, Intel, NCR, and Hewlett-
Packard itself, as well as at a host of lesser-know n startup companies. All had placed substantial 
product development bets on the PDA market. Many of  their products were designed with Kittyhawk’s 
features in mind, and Kittyhawk’s design in turn re flected these customers’ well-researched needs. 
The Kittyhawk team concluded that developing a driv e that met these customers’ requirements would 
be a demanding but feasible technological stretch, and they launched an aggressive twelve-month effort  
to develop the tiny device. The result, shown in Fi gure 7.2, was impressive. The first version packed 20  120  MB, and a second model, introduced a year later, st ored 40 MB. To meet the ruggedness demanded in 
its target market of PDAs and electronic notebooks,  Kittyhawk was equipped with an impact sensor 
similar to those used in automobile airbag crash se nsors and could withstand a three-foot drop onto 
concrete without data loss. It was designed to sell  initially at $250 per unit. 
Although Kittyhawk’s technical development went acc ording to plan, the development of applications 
for it did not. The PDA market failed to materializ e substantially, as sales of Apple’s Newton and 
competing devices fell far short of aspirations. Th is surprised many of the computer industry experts 
whose opinions HP’s marketers had worked so hard to  synthesize. During its first two years on the 
market, Kittyhawk logged just a fraction of the sal es that had been forecast. The sales achieved might  
have initially satisfied startup companies and vent ure capitalists, but for HP’s management, the 
volumes were far below expectations and far too sma ll to satisfy DMD’s need to grow and gain overall 
market share. Even more surprising, the application s that contributed most significantly to Kittyhawk’ s 
sales were not in computers at all. They were Japan ese-language portable word processors, miniature 
cash registers, electronic cameras, and industrial scanners, none of which had figured in Kittyhawk’s 
original marketing plans. 
 
  
Figure 7.2  Hewlett-Packard Kittyhawk Drive  
 
 
 
Source:  Hewlett Packard Company. Used by permission.  
 
  
Even more frustrating, as the second anniversary of  Kittyhawk’s launch approached, were the inquiries 
received by HP marketers from companies making mass -market video game systems to buy very large 
volumes of Kittyhawk—if HP could make a version ava ilable at a lower price point. These companies 
had been aware of Kittyhawk for two years, but they  reported that it had taken some time for them to 
see what could be done with a storage device so sma ll. 
To a significant extent, HP had designed Kittyhawk to be a sustaining technology for mobile 
computing. Along many of the metrics of value in th at application—small size, low weight and power 
consumption, and ruggedness—Kittyhawk constituted a  discontinuous sustaining improvement relative  121  to 2.5- and 1.8-inch drives. Only in capacity (whic h HP had pushed as far as possible) was Kittyhawk 
deficient. The large inquiries and orders that fina lly began arriving for the Kittyhawk, however, were  
for a truly  disruptive product: something priced at $50 per un it and with limited functionality. For these 
applications, a capacity of 10 MB would have been p erfectly adequate. 
Unfortunately, because HP had positioned the drive with the expensive features needed for the PDA 
market rather than designing it as a truly disrupti ve product, it simply could not meet the price requ ired 
by home video game manufacturers. Having invested s o aggressively to hit its original targets as 
defined by the PDA application, management had litt le patience and no money to redesign a simpler, 
defeatured 1.3-inch drive that fit the market appli cations that had finally become clear. HP withdrew 
Kittyhawk from the market in late 1994. 
The HP project managers concede in retrospect that their most serious mistake in managing the 
Kittyhawk initiative was to act as if their forecas ts about the market were right, rather than as if t hey 
were wrong. They had invested aggressively in manuf acturing capacity for producing the volumes 
forecast for the PDA market and had incorporated de sign features, such as the shock sensor, that were 
crucial to acceptance in the PDA market they had so  carefully researched. Such planning and 
investment is crucial to success in a sustaining te chnology, but, the managers reflected, it was not r ight 
for a disruptive product like Kittyhawk. If they ha d the opportunity to launch Kittyhawk all over agai n, 
they would assume that neither they nor anyone else  knew for sure what kinds of customers would 
want it or in what volumes. This would lead them to ward a much more exploratory, flexible approach 
toward product design and investment in manufacturi ng capacity; they would, given another chance, 
feel their way into the market, leaving enough reso urces to redirect their program if necessary and 
building upon what they learned on the way. 
Hewlett-Packard’s disk drive makers are not the onl y ones, of course, who behaved as if they knew 
what the market for a disruptive technology would b e. They are in stellar company, as the following 
case histories show. 
 
 
HONDA’S INVASION OF THE NORTH AMERICAN MOTORCYCLE I NDUSTRY   
 
Honda’s success in attacking and dominating the Nor th American and European motorcycle markets 
has been cited as a superb example of clear strateg ic thinking coupled with aggressive and coherent 
execution. According to these accounts, Honda emplo yed a deliberate manufacturing strategy based on 
an experience curve in which it cut prices, built v olume, aggressively reduced costs, cut prices some 
more, reduced costs further, and built an unassaila ble volume-based low-cost manufacturing position in  
the motorcycle market. Honda then used that base to  move upmarket and ultimately blew all 
established motorcycle manufacturers out of the mar ket except for Harley-Davidson and BMW, which 
barely survived. 2 Honda combined this manufacturing triumph with a c lever product design, catchy 
advertising, and a convenient, broad-based distribu tor/retailer network tailored to the informal cycli sts 
who constituted Honda’s core customer base. Told in  this manner, Honda’s history is a tale of strategi c 
brilliance and operational excellence that all mana gers dream will be told about them someday. The 
reality of Honda’s achievement, as recounted by the  Honda employees who were managing the 
business at the time, however, is quite different. 3  
During Japan’s years of post-war reconstruction and  poverty, Honda had emerged as a supplier of 
small, rugged motorized bicycles that were used by distributors and retailers in congested urban areas   122  to make small deliveries to local customers. Honda developed considerable expertise in designing 
small, efficient engines for these bikes. Its Japan ese market sales grew from an initial annual volume  of 
1,200 units in 1949 to 285,000 units in 1959. 
Honda’s executives were eager to exploit the compan y’s low labor costs to export motorbikes to North 
America, but there was no equivalent market there f or its popular Japanese “Supercub” delivery bike. 
Honda’s research showed that Americans used motorcy les primarily for over-the-road distance driving 
in which size, power, and speed were the most highl y valued product attributes. Accordingly, Honda 
engineers designed a fast, powerful motorcycle spec ifically for the American market, and in 1959 
Honda dispatched three employees to Los Angeles to begin marketing efforts. To save living expenses, 
the three shared an apartment, and each brought wit h him a Supercub bike to provide cheap 
transportation around the city. 
The venture was a frustrating experience from the b eginning. Honda’s products offered no advantage to 
prospective customers other than cost, and most mot orcycle dealers refused to accept the unproven 
product line. When the team finally succeeded in fi nding some dealers and selling a few hundred units,  
the results were disastrous. Honda’s understanding of engine design turned out not to be transferable to 
highway applications, in which bikes were driven at  high speeds for extended periods: The engines 
sprung oil leaks and the clutches wore out. Honda’s  expenses in air-freighting the warrantied 
replacement motorcycles between Japan and Los Angel es nearly sunk the company. 
Meanwhile, one Saturday, Kihachiro Kawashima, the H onda executive in charge of the North 
American venture, decided to vent his frustrations by taking his Supercub into the hills east of Los 
Angeles. It helped: He felt better after zipping ar ound in the dirt. A few weeks later he sought relie f 
dirt-biking again. Eventually he invited his two co lleagues to join him on their Supercubs. Their 
neighbors and others who saw them zipping around th e hills began inquiring where they could buy 
those cute little bikes, and the trio obliged by sp ecial-ordering Supercub models for them from Japan.  
This private use of what became known as off-road d irt bikes continued for a couple of years. At one 
point a Sears buyer tried to order Supercubs for th e company’s outdoor power equipment departments, 
but Honda ignored the opportunity, preferring to fo cus on selling large, powerful, over-the-road cycle s, 
a strategy that continued to be unsuccessful. 
Finally, as more and more people clamored for their  own little Honda Supercubs to join their dirt-
biking friends, the potential for a very different market dawned on Honda’s U.S. team: Maybe there 
was an undeveloped off-the-road recreational motorb ike market in North America for which—quite by 
accident—the company’s little 50cc Supercub was nic ely suited. Although it took much arguing and 
arm-twisting, the Los Angeles team ultimately convi nced corporate management in Japan that while 
the company’s large bike strategy was doomed to fai lure, another quite different opportunity to create  a 
totally new market segment merited pursuit. 
Once the small-bike strategy was formally adopted, the team found that securing dealers for the 
Supercub was an even more vexing challenge than it had been for its big bikes. There just weren’t any 
retailers selling that class of product. Ultimately , Honda persuaded a few sporting goods dealers to t ake 
on its line of motorbikes, and as they began to pro mote the bikes successfully, Honda’s innovative 
distribution strategy was born. 
Honda had no money for a sophisticated advertising campaign. But a UCLA student who had gone 
dirt-biking with his friends came up with the adver tising slogan, “You meet the nicest people on a 
Honda,” for a paper he wrote in an advertising cour se. Encouraged by his teacher, he sold the idea to an 
advertising agency, which then convinced Honda to u se it in what became an award-winning  123  advertising campaign. These serendipitous events we re, of course, followed by truly world-class design  
engineering and manufacturing execution, which enab led Honda to repeatedly lower its prices as it 
improved its product quality and increased its prod uction volumes. 
Honda’s 50cc motorbike was a disruptive technology in the North American market. The rank-ordering 
of product attributes that Honda’s customers employ ed in their product decision making defined for 
Honda a very different value network than the estab lished network in which Harley-Davidson, BMW, 
and other traditional motorcycle makers had compete d. 
From its low-cost manufacturing base for reliable m otorbikes, using a strategy reminiscent of the 
upmarket invasions described earlier in disk drives , steel, excavators, and retailing, Honda turned it s 
sights upmarket, introducing between 1970 and 1988 a series of bikes with progressively more 
powerful engines. 
For a time in the late 1960s and early 1970s, Harle y attempted to compete head-on with Honda and to 
capitalize on the expanding low-end market by produ cing a line of small-engine (150 to 300 cc) bikes 
acquired from the Italian motorcycle maker Aeromecc hania. Harley attempted to sell the bikes through 
its North American dealer network. Although Honda’s  manufacturing prowess clearly disadvantaged 
Harley in this effort, a primary cause of Harley’s failure to establish a strong presence in the small -bike 
value network was the opposition of its dealer netw ork. Their profit margins were far greater on high-
end bikes, and many of them felt the small machines  compromised Harley-Davidson’s image with their 
core customers. 
Recall from chapter 2 the finding that within a giv en value network, the disk drive companies and thei r 
computer-manufacturing customers had developed very  similar economic models or cost structures, 
which determined the sorts of business that appeare d profitable to them. We see the same phenomenon 
here. Within their value network, the economics of Harley’s dealers drove them to favor the same type 
of business that Harley had come to favor. Their co existence within the value network made it difficul t 
for either Harley or its dealers to exit the networ k through its bottom. In the late 1970s Harley gave  in 
and repositioned itself at the very high end of the  motorcycle market—a strategy reminiscent of 
Seagate’s repositioning in disk drives, and of the upmarket retreats of the cable excavator companies 
and the integrated steel mills. 
Interestingly, Honda proved just as inaccurate in e stimating how large  the potential North American 
motorcycle market was as it had been in understandi ng what  it was. Its initial aspirations upon entry in 
1959 had been to capture 10 percent of a market est imated at 550,000 units per year with annual 
growth of 5 percent. By 1975 the market had grown 1 6 percent per year to 5,000,000 annual units—
units that came largely from an application that Ho nda could not have foreseen. 4 
 
 
INTEL’S DISCOVERY OF THE MICROPROCESSOR MARKET   
 
Intel Corporation, whose founders launched the comp any in 1969 based on their pioneering 
development of metal-on-silicon (MOS) technology to  produce the world’s first dynamic random 
access memory (DRAM) integrated circuits, had becom e by 1995 one of the world’s most profitable 
major companies. Its storied success is even more r emarkable because, when its initial leadership 
position in the DRAM market began crumbling between  1978 and 1986 under the onslaught of  124  Japanese semiconductor manufacturers, Intel transfo rmed itself from a second-tier DRAM company 
into the world’s dominant microprocessor manufactur er. How did Intel do it? 
Intel developed the original microprocessor under a  contract development arrangement with a Japanese 
calculator manufacturer. When the project was over,  Intel’s engineering team persuaded company 
executives to purchase the microprocessor patent fr om the calculator maker, which owned it under the 
terms of its contract with Intel. Intel had no expl icit strategy for building a market for this new 
microprocessor; the company simply sold the chip to  whoever seemed to be able to use it. 
Mainstream as they seem today, microprocessors were  disruptive technologies when they first 
emerged. They were capable only of limited function ality, compared to the complex logic circuits that 
constituted the central processing units of large c omputers in the 1960s. But they were small and 
simple, and they enabled affordable logic and compu tation in applications where this previously had 
not been feasible. 
Through the 1970s, as competition in the DRAM marke t intensified, margins began to decline on 
Intel’s DRAM revenues while margins on its micropro cessor product line, where there was less 
competition, stayed robust. Intel’s system for allo cating production capacity operated according to a 
formula whereby capacity was committed in proportio n to the gross margins earned by each product 
line. The system therefore imperceptibly began dive rting investment capital and manufacturing 
capacity away from the DRAM business and into micro processors—without an explicit management 
decision to do so. 5 In fact, Intel senior management continued to focu s most of its own attention and 
energy on DRAM, even while the company’s resource a llocation processes were gradually 
implementing an exit from that business. 
This de facto strategy shift, driven by Intel’s aut onomously operating resource allocation process, wa s 
fortuitous. Because so little was known of the micr oprocessor market at that time, explicit analysis 
would have provided little justification for a bold  move into microprocessors. Gordon Moore, Intel co-
founder and chairman, for example, recalled that IB M’s choice of the Intel 8088 microprocessor as the 
“brain” of its new personal computer was viewed wit hin Intel as a “small design win.” 6 Even after 
IBM’s stunning success with its personal computers,  Intel’s internal forecast of the potential 
applications for the company’s next-generation 286 chip did not include personal computers in its list  
of the fifty highest-volume applications. 7 
In retrospect, the application of microprocessors t o personal computers is an obvious match. But in th e 
heat of the battle, of the many applications in whi ch microprocessors might have been used, even a 
management team as astute as Intel’s could not know  which would emerge as the most important and 
what volumes and profits it would yield. 
 
 
UNPREDICTABILITY AND DOWNWARD IMMOBILITY IN ESTABLI SHED FIRMS   
 
The reaction of some managers to the difficulty of correctly planning the markets for disruptive 
technologies is to work harder and plan smarter. Wh ile this approach works for sustaining innovations,  
it denies the evidence about the nature of disrupti ve ones. Amid all the uncertainty surrounding 
disruptive technologies, managers can always count on one anchor: Experts’ forecasts will always be 
wrong.  It is simply impossible to predict with any useful  degree of precision how disruptive products 
will be used or how large their markets will be. An  important corollary is that, because markets for  125  disruptive technologies are unpredictable, companie s’ initial strategies for entering these markets wi ll 
generally be wrong. 
How does this statement square with the findings pr esented in Table 6.1, which showed a stunning 
difference in the posterior probabilities of succes s between firms that entered new, emerging value 
networks (37 percent) and those that entered existi ng value networks (6 percent)? If markets cannot be  
predicted in advance, how can firms that target the m be more successful? Indeed, when I have shown 
the matrix in Table 6.1 to managerial audiences, th ey are quite astonished by the differences in the 
magnitudes and probabilities of success. But it is clear that the managers don’t believe that the resu lts 
can be generalized to their own situations. The fin dings violate their intuitive sense that creating n ew 
markets is a genuinely risky business. 8 
 
 
Failed Ideas versus Failed Businesses   
 
The case studies reviewed in this chapter suggest a  resolution to this puzzle. There is a big differen ce 
between the failure of an idea  and the failure of a firm.  Many of the ideas prevailing at Intel about 
where the disruptive microprocessor could be used w ere wrong; fortunately, Intel had not expended all 
of its resources implementing wrong-headed marketin g plans while the right market direction was still 
unknowable. As a company, Intel survived many false  starts in its search for the major market for 
microprocessors. Similarly, Honda’s idea about how to enter the North American motorcycle market 
was wrong, but the company didn’t deplete its resou rces pursuing its big-bike strategy and was able to  
invest aggressively in the winning strategy after i t had emerged. Hewlett-Packard’s Kittyhawk team 
was not as fortunate. Believing they had identified  the winning strategy, its managers spent their bud get 
on a product design and the manufacturing capacity for a market application that never emerged. When 
the ultimate applications for the tiny drive ultima tely began to coalesce, the Kittyhawk team had no 
resources left to pursue them. 
Research has shown, in fact, that the vast majority  of successful new business ventures abandoned thei r 
original business strategies when they began implem enting their initial plans and learned what would 
and would not work in the market. 9 The dominant difference between successful venture s and failed 
ones, generally, is not the astuteness of their ori ginal strategy. Guessing the right strategy at the outset 
isn’t nearly as important to success as conserving enough resources (or having the relationships with 
trusting backers or investors) so that new business  initiatives get a second or third stab at getting it 
right. Those that run out of resources or credibili ty before they can iterate toward a viable strategy  are 
the ones that fail. 
 
 
Failed Ideas and Failed Managers   
 
In most companies, however, individual managers don ’t have the luxury of surviving a string of trials 
and errors in pursuit of the strategy that works. R ightly or wrongly, individual managers in most 
organizations believe that they cannot  fail: If they champion a project that fails becaus e the initial 
marketing plan was wrong, it will constitute a blot ch on their track record, blocking their rise throu gh 
the organization. Because failure is intrinsic to t he process of finding new markets for disruptive 
technologies, the inability or unwillingness of ind ividual managers to put their careers at risk acts as a  126  powerful deterrent to the movement of established f irms into the value networks created by those 
technologies. As Joseph Bower observed in his class ic study of the resource allocation process at a 
major chemical company, “Pressure from the market r educes both the probability and the cost of being 
wrong.” 10   
Bower’s observation is consistent with the findings  in this book about the disk drive industry. When 
demand for an innovation was assured, as was the ca se with sustaining technologies, the industry’s 
established leaders were capable of placing huge, l ong, and risky bets to develop whatever technology 
was required. When demand was not assured, as was t he case in disruptive technologies, the 
established firms could not even make the technolog ically straightforward bets required to 
commercialize such innovations. That is why 65 perc ent of the companies entering the disk drive 
industry attempted to do so in an established, rath er than emerging market. Discovering markets for 
emerging technologies inherently involves failure, and most individual decision makers find it very 
difficult to risk backing a project that might fail  because the market is not there. 
 
 
Plans to Learn versus Plans to Execute   
 
Because failure is intrinsic to the search for init ial market applications for disruptive technologies , 
managers need an approach very different from what they would take toward a sustaining technology. 
In general, for sustaining technologies, plans must  be made before action is taken, forecasts can be 
accurate, and customer inputs can be reasonably rel iable. Careful planning, followed by aggressive 
execution, is the right formula for success in sust aining technology. 
But in disruptive situations, action must be taken before careful plans are made. Because much less ca n 
be known about what markets need or how large they can become, plans must serve a very different 
purpose: They must be plans for learning  rather than plans for implementation. By approachi ng a 
disruptive business with the mindset that they can’ t know where the market is, managers would identify  
what critical information about new markets is most  necessary and in what sequence that information is  
needed. Project and business plans would mirror tho se priorities, so that key pieces of information 
would be created, or important uncertainties resolv ed, before expensive commitments of capital, time, 
and money were required. 
Discovery-driven planning,  which requires managers to identify the assumption s upon which their 
business plans or aspirations are based, 11  works well in addressing disruptive technologies. In the case 
of Hewlett-Packard’s Kittyhawk disk drive, for exam ple, HP invested significant sums with its 
manufacturing partner, the Citizen Watch Company, i n building and tooling a highly automated 
production line. This commitment was based on an as sumption that the volumes forecast for the drive, 
built around forecasts by HP customers of PDA sales , were accurate. Had HP’s managers instead 
assumed that nobody knew in what volume PDAs would sell, they might have built small modules of 
production capacity rather than a single, high-volu me line. They could then have held to capacity or 
added or reduced capacity as key events confirmed o r disproved their assumptions. 
Similarly, the Kittyhawk product development plan w as based on an assumption that the dominant 
application for the little drive was in PDAs, which  demanded high ruggedness. Based on this 
assumption, the Kittyhawk team committed to compone nts and a product architecture that made the 
product too expensive to be sold to the price-sensi tive video game makers at the emerging low end of 
the market. Discovery-driven planning would have fo rced the team to test its market assumptions  127  before  making commitments that were expensive to reverse— in this case, possibly by creating a 
modularized design that easily could be reconfigure d or defeatured to address different markets and 
price points, as events in the marketplace clarifie d the validity of their assumptions. 
Philosophies such as management by objective  and management by exception  often impede the 
discovery of new markets because of where they focu s management attention. Typically, when 
performance falls short of plan, these systems enco urage management to close the gap between what 
was planned and what happened. That is, they focus on unanticipated failures. But as Honda’s 
experience in the North American motorcycle market illustrates, markets for disruptive technologies 
often emerge from unanticipated successes, on which  many planning systems do not focus the attention 
of senior management. 12  Such discoveries often come by watching how people  use products, rather 
than by listening to what they say. 
I have come to call this approach to discovering th e emerging markets for disruptive technologies 
agnostic marketing,  by which I mean marketing under an explicit assump tion that no one —not us, not 
our customers—can know whether, how, or in what qua ntities a disruptive product can or will be used 
before they have experience using it. Some managers , faced with such uncertainty, prefer to wait until  
others have defined the market. Given the powerful first-mover advantages at stake, however, 
managers confronting disruptive technologies need t o get out of their laboratories and focus groups an d 
directly create knowledge about new customers and n ew applications through discovery-driven 
expeditions into the marketplace. 
 
 
NOTES   
 
1.  What follows is a summary of the fuller history re counted in “Hewlett-Packard: The Flight of the 
Kittyhawk,” Harvard Business School, Case No. 9-697 -060, 1996.  
2.  Examples of such histories of Honda’s success incl ude the Harvard Business School case study, “A 
Note on the Motorcycle Industry—1975,” No. 9-578-21 0, and a report published by The Boston 
Consulting Group, “Strategy Alternatives for the Br itish Motorcycle Industry,” 1975.  
3.  Richard Pascale and E. Tatum Christiansen, “Honda (A),” Harvard Business School Teaching, Case 
No. 9-384-049, 1984, and “Honda (B),” Harvard Busin ess School, Teaching Case No. 9-384-050, 
1984.  
4.  Statistical Abstract of the United States  (Washington, D.C.: United States Bureau of the Cen sus, 
1980), 648.  
5.  Intel’s exit from the DRAM business and entry into  microprocessors has been chronicled by Robert 
A. Burgelman in “Fading Memories: A Process Theory of Strategic Business Exit in Dynamic 
Environments,” Administrative Science Quarterly  (39), 1994, 24–56. This thoroughly researched and 
compellingly written study of the process of strate gy evolution is well worth reading.  
6.  George W. Cogan and Robert A. Burgelman, “Intel Co rporation (A): The DRAM Decision,” 
Stanford Business School, Case PS-BP-256.  
7.  Robert A. Burgelman, “Fading Memories: A Process T heory of Strategic Business Exit in Dynamic 
Environments,” Administrative Science Quarterly  (39) 1994.  
8.  Studies of how managers define and perceive risk c an shed significant light on this puzzle. Amos 
Tversky and Daniel Kahneman, for example, have show n that people tend to regard propositions that 
they do not understand as more risky, regardless of  their intrinsic risk, and to regard things they do  
understand as less  risky, again without regard to intrinsic risk. (Am os Tversky and Daniel Kahneman, 
“Judgment Under Uncertainty: Heuristics and Biases, ” Science  [185], 1974, 1124–1131.) Managers,  128  therefore, may view creation of new markets as risk y propositions, in the face of contrary evidence, 
because they do not understand non-existent markets ; similarly, they may regard investment in 
sustaining technologies, even those with high intri nsic risk, as safe because they understand the mark et 
need.  
9.  Among the excellent studies in this tradition are Myra M. Hart, Founding Resource Choices: 
Influences and Effects,  DBA thesis, Harvard University Graduate School of Business Administration, 
1995; Amar Bhide, “How Entrepreneurs Craft Strategi es that Work,” Harvard Business Review,  
March–April, 1994, 150–163; Amar Bhide, “Bootstrap Finance: The Art of Start-Ups,” Harvard 
Business Review,  November–December 1992, 109–118; “Hewlett-Packard’ s Kittyhawk,” Harvard 
Business School, Case No. 9-697-060; and “Vallourec ’s Venture into Metal Injection Molding,” 
Harvard Business School, Case No. 9-697-001.  
10.  Joseph Bower, Managing the Resource Allocation Process  (Homewood, IL: Richard D. Irwin, 
1970), 254.  
11.  Rita G. McGrath and Ian C. MacMillan, “Discovery-D riven Planning,” Harvard Business Review,  
July–August, 1995, 4–12.  
12.  This point is persuasively argued in Peter F. Druc ker, Innovation and Entrepreneurship  (New 
York: Harper & Row, 1985). Below, in chapter 9, I r ecount how software maker Intuit discovered that 
many of the people buying its Quicken  personal financial management software were, in fa ct, using it 
to keep the books of their small businesses. Intuit  had not anticipated this application, but it 
consequently adapted the product more closely to sm all business needs and launched Quickbooks,  
which captured more than 70 percent of the small bu siness accounting software market within two 
years.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  129    
CHAPTER EIGHT  
How to Appraise Your  
Organization’s Capabilities  
and Disabilities 
 
 
When managers assign employees to tackle a critical  innovation, they instinctively work to match the 
requirements of the job with the capabilities of th e individuals whom they charge to do it. In evaluat ing 
whether an employee is capable of successfully exec uting a job, managers will assess whether he or she  
has the requisite knowledge, judgment, skill, persp ective, and energy. Managers will also assess the 
employee’s values —the criteria by which he or she tends to decide wha t should and shouldn’t be done. 
Indeed, the hallmark of a great manager is the abil ity to identify the right person for the right job,  and 
to train his or her employees so that they have the  capabilities to succeed at the jobs they are given .  
Unfortunately, some managers don’t think as rigorou sly about whether their organizations  have the 
capability to successfully execute jobs that may be  given to them. Frequently, they assume that if the  
people working on a project individually have the r equisite capabilities to get the job done well, the n 
the organization in which they work will also have the same capability to succeed. This often is not t he 
case. One could take two sets of identically capabl e people and put them to work in two different 
organizations, and what they accomplish would likel y be significantly different. This is because 
organizations themselves, independent of the people  and other resources in them, have capabilities. To  
succeed consistently, good managers need to be skil led not just in choosing, training, and motivating 
the right people for the right job, but in choosing , building, and preparing the right organization  for the 
job as well. 
The purpose of this chapter is to describe the theo ry that lies behind the empirical observations made  in 
chapters 5, 6, and 7—in particular, the observation  that the only companies that succeeded in 
addressing disruptive technology were those that cr eated independent organizations whose size 
matched the size of the opportunity. The notion tha t organizations have “core competencies” has been a  
popular one for much of the last decade. 1 In practice, however, most managers have found tha t the 
concept is sufficiently vague that some supposed “c ompetence” can be cited in support of a 
bewildering variety of innovation proposals. This c hapter brings greater precision to the core 
competence concept, by presenting a framework to he lp managers understand, when they are 
confronted with a necessary change, whether the org anizations over which they preside are competent 
or incompetent of tackling the challenges that lie ahead. 
 
 
AN ORGANIZATIONAL CAPABILITIES FRAMEWORK   
 
Three classes of factors affect what an organizatio n can and cannot do: its resources, its processes, and 
its values. When asking what sorts of innovations t heir organizations are and are not likely to be abl e to 
implement successfully, managers can learn a lot ab out capabilities by disaggregating their answers 
into these three categories. 2   130   
 
Resources   
 
Resources are the most visible of the factors that contribute to what an organization can and cannot d o. 
Resources include people, equipment, technology, pr oduct designs, brands, information, cash, and 
relationships with suppliers, distributors, and cus tomers. Resources are usually things,  or assets —they 
can be hired and fired, bought and sold, depreciate d or enhanced. They often can be transferred across  
the boundaries of organizations much more readily t han can processes and values. Without doubt, 
access to abundant and high-quality resources enhan ces an organization’s chances of coping with 
change. 
Resources are the things that managers most instinc tively identify when assessing whether their 
organizations can successfully implement changes th at confront them. Yet resource analysis clearly 
does not tell a sufficient story about capabilities . Indeed, we could deal identical sets of resources  to 
two different organizations, and what they created from those resources would likely be very 
different—because the capabilities to transform inp uts into goods and services of greater value reside  in 
the organization’s processes and values. 
 
 
Processes   
 
Organizations create value as employees transform i nputs of resources—people, equipment, 
technology, product designs, brands, information, e nergy, and cash—into products and services of 
greater worth. The patterns of interaction, coordin ation, communication, and decision-making through 
which they accomplish these transformations are processes .3 Processes include not just manufacturing 
processes, but those by which product development, procurement, market research, budgeting, 
planning, employee development and compensation, an d resource allocation are accomplished. 
Processes differ not only in their purpose, but als o in their visibility. Some processes are “formal,”  in 
the sense that they are explicitly defined, visibly  documented, and consciously followed. Other 
processes are “informal,” in that they are habitual  routines or ways of working that have evolved over  
time, which people follow simply because they work— or because “That’s the way we do things around 
here.” Still other methods of working and interacti ng have proven so effective for so long that people  
unconsciously follow them—they constitute the cultu re of the organization. Whether they are formal, 
informal, or cultural, however, processes define ho w an organization transforms the sorts of inputs 
listed above into things of greater value. 
Processes are defined or evolve de facto  to address specific tasks. This means that when ma nagers use a 
process to execute the tasks for which it was desig ned, it is likely to perform efficiently. But when the 
same, seemingly efficient process is employed to ta ckle a very different task, it is likely to seem sl ow, 
bureaucratic, and inefficient. In other words, a pr ocess that defines a capability  in executing a certain 
task concurrently defines disabilities  in executing other tasks. 4 The reason good managers strive for 
focus in their organizations is that processes and tasks can be readily aligned. 5 
One of the dilemmas of management is that, by their  very nature, processes are established so that 
employees perform recurrent tasks in a consistent w ay, time after time. To ensure consistency, they ar e  131  meant not  to change—or if they must change, to change throug h tightly controlled procedures. This 
means that the very mechanisms through which organi zations create value are intrinsically inimical to 
change . 
Some of the most crucial processes to examine as ca pabilities or disabilities aren’t the obvious value -
adding processes involved in logistics, development , manufacturing, and customer service. Rather, they  
are the enabling or background processes that suppo rt investment decision-making. As we saw in 
chapter 7, the processes that render good companies  incapable of responding to change are often those 
that define how market research is habitually done;  how such analysis is translated into financial 
projections; how plans and budgets are negotiated a nd how those numbers are delivered; and so on. 
These typically inflexible processes are where many  organizations’ most serious disabilities in coping  
with change reside. 
 
 
Values   
 
The third class of factors that affect what an orga nization can or cannot accomplish is its values. Th e 
values of an organization are the criteria by which  decisions about priorities are made. Some corporat e 
values are ethical in tone, such as those that guid e decisions to ensure patient well-being at Johnson  & 
Johnson or that guide decisions about plant safety at Alcoa. But within the Resources-Processes-Values  
(RPV) framework, values have a broader meaning. An organization’s values are the standards by which 
employees make prioritization decisions—by which th ey judge whether an order is attractive or 
unattractive; whether a customer is more important or less important; whether an idea for a new 
product is attractive or marginal; and so on. Prior itization decisions are made by employees at every 
level. At the executive tiers, they often take the form of decisions to invest or not invest in new 
products, services, and processes. Among salespeopl e, they consist of on-the-spot, day-to-day decision s 
about which products to push with customers and whi ch not to emphasize. 
The larger and more complex a company becomes, the more important it is for senior managers to train 
employees at every level to make independent decisi ons about priorities that are consistent with the 
strategic direction and the business model of the c ompany. A key metric of good management, in fact, 
is whether such clear and consistent values have pe rmeated the organization. 6 
Clear, consistent, and broadly understood values, h owever, also define what an organization cannot do.  
A company’s values, by necessity, must reflect its cost structure or its business model, because these  
define the rules its employees must follow in order  for the company to make money. If, for example, 
the structure of a company’s overhead costs require s it to achieve gross profit margins of 40 percent,  a 
powerful value or decision rule will have evolved t hat encourages middle managers to kill ideas that 
promise gross margins below 40 percent. This means that such an organization would be incapable  of 
successfully commercializing projects targeting low -margin markets. At the same time, another 
organization’s values, driven by a very different c ost structure, might enable or facilitate the succe ss of 
the very same project. 
The values of successful firms tend to evolve in a predictable fashion in at least two dimensions. The  
first relates to acceptable gross margins. As compa nies add features and functionality to their produc ts 
and services in order to capture more attractive cu stomers in premium tiers of their markets, they oft en 
add overhead cost. As a result, gross margins that at one point were quite attractive, at a later poin t 
seem unattractive. Their values change. For example , Toyota entered the North American market with  132  its Corona model—a product targeting the lowest-pri ced tiers of the market. As the entry tier of the 
market became crowded with look-alike models from N issan, Honda, and Mazda, competition among 
equally low-cost competitors drove down profit marg ins. Toyota developed more sophisticated cars 
targeted at higher tiers of the market in order to improve its margins. Its Corolla, Camry, Previa, 
Avalon, and Lexus families of cars have been introd uced in response to the same competitive 
pressures—it kept its margins healthy by migrating up-market. In the process, Toyota has had to add 
costs to its operation to design, build, and suppor t cars of this caliber. It progressively deemphasiz ed 
the entry-level tiers of the market, having found t he margins it could earn there to be unattractive, given 
its changed cost structure. 
Nucor Steel, the leading minimill that led the up-m arket charge against the integrated mills that was 
recounted in chapter 4, likewise has experienced a change in values. As it has managed the center of 
gravity in its product line up-market from re-bar t o angle iron to structural beams and finally to she et 
steel, it has begun to decidedly deemphasize re-bar —the product that had been its bread and butter in 
its earlier years. 
The second dimension along which values predictably  change relates to how big a business has to be in 
order to be interesting. Because a company’s stock price represents the discounted present value of it s 
projected earnings stream, most managers typically feel compelled not just to maintain growth, but to 
maintain a constant rate  of growth. In order for a $40 million company to g row 25 percent, it needs to 
find $10 million in new business the next year. For  a $40 billion  company to grow 25 percent, it needs 
to find $10 billion in new business the next year. The size of market opportunity that will solve each  of 
these companies’ needs for growth is very different . As noted in chapter 6, an opportunity that excite s a 
small organization isn’t big enough to be interesti ng to a very large one. One of the bittersweet rewa rds 
of success is, in fact, that as companies become la rge, they literally lose the capability to enter sm all 
emerging markets. This disability is not because of  a change in the resources within the companies—
their resources typically are vast. Rather, it is b ecause their values change. 
Executives and Wall Street financiers who engineer megamergers among already huge companies in 
order to achieve cost savings need to account for t he impact of these actions on the resultant 
companies’ values. Although their merged organizati ons might have more resources to throw at 
innovation problems, their commercial organizations  tend to lose their appetites for all but the bigge st 
blockbuster opportunities. Huge size constitutes a very real disability  in managing innovation. In many 
ways, Hewlett-Packard’s recent decision to split it self into two companies is rooted in its recognitio n of 
this problem. 
 
 
THE RELATIONSHIP BETWEEN PROCESSES AND VALUES, AND SUCCESS IN 
ADDRESSING SUSTAINING VS. DISRUPTIVE TECHNOLOGIES   
 
The resources-processes-values (RPV) framework has been a useful tool for me to understand the 
findings from my research relating to the differenc es in companies’ track records in sustaining and 
disruptive technologies. Recall that we identified 116 new technologies that were introduced in the 
industry’s history. Of these, 111 were sustaining t echnologies, in that their impact was to improve th e 
performance of disk drives. Some of these were incr emental improvements while others, such as 
magneto-resistive heads, represented discontinuous leaps forward in performance. In all 111 cases of 
sustaining technology, the companies that led in de veloping and introducing the new technology were  133  the companies that had led in the old technology. T he success rate of the established firms in 
developing and adopting sustaining technologies was  100 percent. 
The other five of these 116 technologies were disru ptive innovations—in each case, smaller disk drives  
that were slower and had lower capacity than those used in the mainstream market. There was no new 
technology involved in these disruptive products. Y et none  of the industry’s leading companies 
remained atop the industry after these disruptive i nnovations entered the market—their batting average  
was zero . 
Why such markedly different batting averages when p laying the sustaining versus disruptive games? 
The answer lies in the RPV framework of organizatio nal capabilities. The industry leaders developed 
and introduced sustaining technologies over and ove r again. Month after month, year after year, as the y 
introduced new and improved products in order to ga in an edge over the competition, the leading 
companies developed processes for evaluating the te chnological potential and assessing their 
customers’ needs for alternative sustaining technol ogies. In the parlance of this chapter, the 
organizations developed a capability  for doing these things, which resided in their pro cesses. 
Sustaining technology investments also fit the valu es of the leading companies, in that they promised 
higher margins from better products sold to their l eading-edge customers. 
On the other hand, the disruptive innovations occur red so intermittently that no company had a 
routinized process for handling them. Furthermore, because the disruptive products promised lower 
profit margins per unit sold and could not be used by their best customers, these innovations were 
inconsistent with the leading companies’ values. Th e leading disk drive companies had the resources —
the people, money, and technology—required to succe ed at both sustaining and disruptive 
technologies. But their processes and values consti tuted disabilities in their efforts to succeed at 
disruptive technologies. 
Large companies often surrender emerging growth mar kets because smaller, disruptive companies are 
actually more capable  of pursuing them. Though start-ups lack resources,  it doesn’t matter. Their 
values can embrace small markets, and their cost st ructures can accommodate lower margins. Their 
market research and resource allocation processes a llow managers to proceed intuitively rather than 
having to be backed up by careful research and anal ysis, presented in PowerPoint. All of these 
advantages add up to enormous opportunity or loomin g disaster—depending upon your perspective. 
Managers who face the need to change or innovate, t herefore, need to do more than assign the right 
resources to the problem. They need to be sure that  the organization in which those resources will be 
working is itself capable of succeeding—and in maki ng that assessment, managers must scrutinize 
whether the organization’s processes and values fit  the problem. 
 
 
THE MIGRATION OF CAPABILITIES   
 
In the start-up stages of an organization, much of what gets done is attributable to its resources —its 
people. The addition or departure of a few key peop le can have a profound influence on its success. 
Over time, however, the locus of the organization’s  capabilities shifts toward its processes and value s. 
As people work together successfully to address rec urrent tasks, processes become defined. And as the 
business model takes shape and it becomes clear whi ch types of business need to be accorded highest 
priority, values coalesce. In fact, one reason that  many soaring young companies flame out after they  134  go public based upon a hot initial product is that whereas their initial success was grounded in 
resources—the founding group of engineers—they fail  to create processes that can create a sequence  of 
hot products. 
An example of such flame out is the story of Avid T echnology, a producer of digital editing systems 
for television. Avid’s technology removed tedium fr om the video editing process. Customers loved it, 
and on the back of its star product, Avid stock ros e from $16 at its 1993 IPO to $49 in mid-1995. 
However, the strains of being a one-trick pony soon  surfaced as Avid was faced with a saturated 
market, rising inventories and receivables, and inc reased competition. Customers loved the product, bu t 
Avid’s lack of effective processes to consistently develop new products and to control quality, delive ry, 
and service ultimately tripped the company and sent  its stock back down. 
In contrast, at highly successful firms such as McK insey and Company, the processes and values have 
become so powerful that it almost doesn’t matter wh ich people get assigned to which project teams. 
Hundreds of new MBAs join the firm every year, and almost as many leave. But the company is able to 
crank out high-quality work year after year because  its core capabilities are rooted in its processes and 
values rather than in its resources. I sense, howev er, that these capabilities of McKinsey also consti tute 
its disabilities. The rigorously analytical, data-d riven processes that help it create value for its c lients in 
existing, relatively stable markets render it much less capable of building a strong client base among  
the rapidly growing companies in dynamic technology  markets. 
In the formative stages of a company’s processes an d values, the actions and attitudes of the company’ s 
founder have a profound impact. The founder often h as strong opinions about the way employees ought 
to work together to reach decisions and get things done. Founders similarly impose their views of what  
the organization’s priorities need to be. If the fo under’s methods are flawed, of course, the company 
will likely fail. But if those methods are useful, employees will collectively experience for themselv es 
the validity of the founder’s problem-solving metho dologies and criteria for decision-making. As they 
successfully use those methods of working together to address recurrent tasks, processes become 
defined. Likewise, if the company becomes financial ly successful by prioritizing various uses of its 
resources according to criteria that reflect the fo under’s priorities, the company’s values begin to 
coalesce. 
As successful companies mature, employees gradually  come to assume that the priorities they have 
learned to accept, and the ways of doing things and  methods of making decisions that they have 
employed so successfully, are the right way to work . Once members of the organization begin to adopt 
ways of working and criteria for making decisions b y assumption, rather than by conscious decision, 
then those processes and values come to constitute the organization’s culture .7 As companies grow 
from a few employees to hundreds and thousands, the  challenge of getting all employees to agree on 
what needs to be done and how it should be done so that the right jobs are done repeatedly and 
consistently can be daunting for even the best mana gers. Culture is a powerful management tool in 
these situations. Culture enables employees to act autonomously and causes them to act consistently. 
Hence, the location of the most powerful factors th at define the capabilities and disabilities of 
organizations migrates over time —from resources to ward visible, conscious processes and values, and 
then toward culture. As long as the organization co ntinues to face the same sorts of problems that its  
processes and values were designed to address, mana ging the organization is relatively straightforward . 
But because these factors also define what an organ ization cannot  do, they constitute disabilities when 
the problems facing the company change. When the or ganization’s capabilities reside primarily in its 
people, changing to address new problems is relativ ely simple. But when the capabilities have come to  135  reside in processes and values and especially  when they have become embedded in culture, change can 
become extraordinarily difficult. 
 
 
A case in point: Did Digital Equipment have the cap ability to succeed in personal computers?   
 
Digital Equipment Corporation (DEC) was a spectacul arly successful maker of minicomputers from the 
1960s through the 1980s. One might have been tempte d to assert, when the personal computer market 
began to coalesce in the early 1980s, that DEC’s “c ore competence” was in building computers. But if 
computers were DEC’s competence, why did the compan y stumble? 
Clearly, DEC had the resources  to succeed in personal computers. Its engineers we re routinely 
designing far more sophisticated computers than PCs . DEC had plenty of cash, a great brand, and 
strong technology. But did DEC have the processes  to succeed in the personal computer business? No. 
The processes for designing and manufacturing minic omputers involved designing many of the key 
components of the computer internally and then inte grating the components into proprietary 
configurations. The design process itself consumed two to three years for a new product model. DEC’s 
manufacturing processes entailed making most compon ents and assembling them in a batch mode. It 
sold direct to corporate engineering organizations.  These processes worked extremely well in the 
minicomputer business. 
The personal computer business, in contrast, requir ed processes through which the most cost-effective 
components were outsourced from the best suppliers around the globe. New computer designs, 
comprised of modular components, had to be complete d in six- to twelve-month cycles. The computers 
were manufactured in high-volume assembly lines, an d sold through retailers to consumers and 
businesses. None of these processes required to com pete successfully in the personal computer business  
existed within DEC. In other words, although the people  working at DEC, as individuals, had the 
abilities to design, build, and sell personal compu ters profitably, they were working in an organizati on 
that was incapable of doing this because its proces ses had been designed and had evolved to do other  
tasks well. The very processes that made the compan y capable of succeeding in one business rendered 
it incapable of succeeding in another. 
And what about DEC’s values ? Because of the overhead costs that were required to succeed in the 
minicomputer business, DEC had to adopt a set of va lues that essentially dictated, “If it generates 50  
percent gross margins or more, it’s good business. If it generates less than 40 percent margins, it’s not 
worth doing.” Management had to ensure that all emp loyees prioritized projects according to this 
criterion, or the company couldn’t make money. Beca use personal computers generated lower margins, 
they did not “fit” with DEC’s values. The company’s  criteria for prioritization placed higher-
performance minicomputers ahead of personal compute rs in the resource allocation process. And any 
attempts that the company made to enter the persona l computer business had to target the highest-
margin tiers of that market—because the financial r esults that might be earned in those tiers were the  
only ones that the company’s values would tolerate.  But because of the patterns noted in chapter 4—the  
strong tendency for competitors with low-overhead b usiness models to migrate up-market—Digital’s 
values rendered it incapable of pursuing a winning strategy. 
As we saw in chapter 5, Digital Equipment could hav e owned another  organization whose processes 
and values were tailored to those required to play in the personal computer game. But the particular 
organization in Maynard, Massachusetts, whose extra ordinary capabilities had carried the company to  136  such success in the minicomputer business, was simp ly incapable of succeeding in the personal 
computer world. 
 
 
CREATING CAPABILITIES TO COPE WITH CHANGE   
 
If a manager determined that an employee was incapa ble of succeeding at a task, he or she would either  
find someone else to do the job or carefully train the employee to be able to succeed. Training often 
works, because individuals can become skilled at mu ltiple tasks. 
Despite beliefs spawned by popular change-managemen t and reengineering programs, processes are 
not nearly as flexible or “trainable” as are resour ces—and values are even less so. The processes that  
make an organization good at outsourcing components  cannot simultaneously make it good at 
developing and manufacturing components in-house. V alues that focus an organization’s priorities on 
high-margin products cannot simultaneously focus pr iorities on low-margin products. This is why 
focused organizations perform so much better than u nfocused ones: their processes and values are 
matched carefully with the set of tasks that need t o be done. 
For these reasons, managers who determine that an o rganization’s capabilities aren’t suited for a new 
task, are faced with three options through which to  create new capabilities. They can: 
 
• Acquire a different organization whose processes an d values are a close match with the new 
task  
• Try to change the processes and values of the curre nt organization  
• Separate out an independent organization and develo p within it the new processes and values 
that are required to solve the new problem  
 
 
Creating Capabilities Through Acquisitions   
 
Managers often sense that acquiring rather than dev eloping a set of capabilities makes competitive and  
financial sense. The RPV model can be a useful way to frame the challenge of integrating acquired 
organizations. Acquiring managers need to begin by asking, “What is it that really created the value 
that I just paid so dearly for? Did I justify the p rice because of its resources—its people, products,  
technology, market position, and so on? Or, was a s ubstantial portion of its worth created by processe s 
and values—unique ways of working and decision-maki ng that have enabled the company to 
understand and satisfy customers, and develop, make , and deliver new products and services in a timely  
way? 
If the acquired company’s processes and values are the real driver of its success, then the last thing  the 
acquiring manager wants to do is to integrate the c ompany into the new parent organization. Integratio n 
will vaporize many of the processes and values of t he acquired firm as its managers are required to 
adopt the buyer’s way of doing business and have th eir proposals to innovate evaluated according to 
the decision criteria of the acquiring company. If the acquiree’s processes and values were the reason   137  for its historical success, a better strategy is to  let the business stand alone, and for the parent t o infuse 
its resources into the acquired firm’s processes an d values. This strategy, in essence, truly constitu tes 
the acquisition of new capabilities. 
If, on the other hand, the company’s resources  were the primary rationale for the acquisition, th en 
integrating the firm into the parent can make a lot  of sense—essentially plugging the acquired people,  
products, technology, and customers into the parent ’s processes, as a way of leveraging the parent’s 
existing capabilities. 
The perils of the DaimlerChrysler merger that began  in the late 1990s, for example, can be better 
understood through the RPV model. Chrysler had few resources that could be considered unique in 
comparison to its competitors. Its success in the m arket of the 1990s was rooted in its processes—
particularly in its rapid, creative product design processes, and in its processes of integrating the efforts 
of its subsystem suppliers. What would be the best way for Daimler to leverage the capabilities that 
Chrysler brought to the table? Wall Street exerted nearly inexorable pressure on management to 
consolidate the two organizations in order to cut c osts. However, integrating the two companies would 
likely vaporize the key processes that made Chrysle r such an attractive acquisition in the first place . 
This situation is reminiscent of IBM’s 1984 acquisi tion of Rolm. There wasn’t anything in Rolm’s pool 
of resources that IBM didn’t already have. It was R olm’s processes for developing PBX products and 
for finding new markets for them that was really re sponsible for its success. In 1987 IBM decided to 
fully integrate the company into its corporate stru cture. Trying to push Rolm’s resources—its products  
and its customers—through the same processes that w ere honed in its large computer business, caused 
the Rolm business to stumble badly. And inviting ex ecutives of a computer company whose values had 
been whetted on operating profit margins of 18 perc ent to get excited about prioritizing products with  
operating margins below 10 percent was impossible. IBM’s decision to integrate Rolm actually 
destroyed the very source of the original worth of the deal. As this chapter is being written in Febru ary 
2000, DaimlerChrysler, bowing to the investment com munity’s drumbeat for efficiency savings, now 
stands on the edge of the same precipice. 
Often, it seems, financial analysts have a better i ntuition for the value of resources than for proces ses. 
In contrast, Cisco Systems’ acquisitions process ha s worked well—because its managers seem to have 
kept resources, processes, and values in the right perspective. Between 1993 and 1997 it acquired 
primarily small companies that were less than two y ears old: early-stage organizations whose market 
value was built primarily upon their resources—part icularly engineers and products. Cisco has a well-
defined, deliberate process by which it essentially  plugs these resources into the parent’s processes and 
systems, and it has a carefully cultivated method o f keeping the engineers of the acquired company 
happily on the Cisco payroll. In the process of int egration, Cisco throws away whatever nascent 
processes and values came with the acquisition—beca use those weren’t what Cisco paid for. On a 
couple of occasions when the company acquired a lar ger, more mature organization—notably its 1996 
acquisition of StrataCom—Cisco did not  integrate. Rather, it let StrataCom stand alone, a nd infused its 
substantial resources into the organization to help  it grow at a more rapid rate. 8 
On at least three occasions, Johnson & Johnson has used acquisitions to establish a position in an 
important wave of disruptive technology. Its busine sses in disposable contact lenses, endoscopic 
surgery, and diabetes blood glucose meters were all  acquired when they were small, were allowed to 
stand alone, and were infused with resources. Each has become a billion-dollar business. Lucent 
Technologies and Nortel followed a similar strategy  for catching the wave of routers, based upon 
packet-switching technology, that were disrupting t heir traditional circuit-switching equipment. But  138  they made these acquisitions late and the firms the y acquired, Ascend Communications and Bay 
Networks, respectively, were extraordinarily expens ive because they had already created the new 
market application, data networks, along with the m uch larger Cisco Systems—and they were right on 
the verge of attacking the voice network. 
 
 
Creating New Capabilities Internally   
 
Companies that have tried to develop new capabiliti es within established organizational units also hav e 
a spotty track record, unfortunately. Assembling a beefed-up set of resources as a means of changing 
what an existing organization can do is relatively straightforward. People with new skills can be hire d, 
technology can be licensed, capital can be raised, and product lines, brands, and information can be 
acquired. Too often, however, resources such as the se are then plugged into fundamentally unchanged 
processes—and little change results. For example, t hrough the 1970s and 1980s Toyota upended the 
world automobile industry through its innovation in  development, manufacturing, and supply-chain 
processes —without investing aggressively in resources such a s advanced manufacturing or 
information-processing technology. General Motors r esponded by investing nearly $60 billion in 
manufacturing resources —computer-automated equipment that was designed to reduce cost and 
improve quality. Using state-of-the-art resources i n antiquated processes, however, made little 
difference in General Motors’ performance, because it is in its processes and values that the 
organization’s most fundamental capabilities lie. P rocesses and values define how resources—many of 
which can be bought and sold, hired and fired—are c ombined to create value. 
Unfortunately, processes are very hard to change—fo r two reasons. The first is that organizational 
boundaries are often drawn to facilitate the operat ion of present processes. Those boundaries can 
impede the creation of new processes that cut acros s those boundaries. When new challenges require 
different people or groups to interact differently than they habitually have done—addressing different  
challenges with different timing than historically had been required—managers need to pull the 
relevant people out of the existing organization an d draw a new boundary around a new group. New 
team boundaries enable or facilitate new patterns o f working together that ultimately can coalesce as 
new processes—new capabilities for transforming inp uts into outputs. Professors Steven C. 
Wheelwright and Kim B. Clark have called these stru ctures heavyweight teams. 9 
The second reason new process capabilities are hard  to develop is that, in some cases, managers don’t 
want  to throw the existing processes out—the methods wo rk perfectly well in doing what they were 
designed to do. As noted above, while resources ten d to be flexible and can be used in a variety of 
situations, processes and values are by their very nature inflexible . Their very raison d’être  is to cause 
the same thing to be done consistently, over and ov er again. Processes are meant not  to change. 
When disruptive change appears on the horizon, mana gers need to assemble the capabilities to confront 
the change before  it has affected the mainstream business. In other words, they need an organization 
that is geared toward the new challenge before the old one, whose processes are tuned to the existing 
business model, has reached a crisis that demands f undamental change. 
Because of its task-specific nature, it is impossib le to ask one process to do two fundamentally diffe rent 
things. Consider the examples presented in chapter 7, for instance. The market research and planning 
processes that are appropriate for the launch of ne w products into existing markets simply aren’t 
capable of guiding a company into emerging, poorly defined markets. And the processes by which a  139  company would experimentally and intuitively feel i ts way into emerging markets would constitute 
suicide if employed in a well-defined existing busi ness. If a company needs to do both types of tasks 
simultaneously, then it needs two very different pr ocesses. And it is very difficult for a single 
organizational unit to employ fundamentally differe nt, opposing processes. As shown below, this is 
why managers need to create different teams, within  which different processes to address new 
problems can be defined and refined. 
 
 
Creating Capabilities Through a Spin-out Organizati on   
 
The third mechanism for new capability creation—spa wning them within spin-out ventures—is 
currently en vogue  among many managers as they wrestle with how to ad dress the Internet. When are 
spin-outs a crucial step in building new capabiliti es to exploit change, and what are the guidelines b y 
which they should be managed? A separate organizati on is required when the mainstream 
organization’s values  would render it incapable of focusing resources on  the innovation project. Large 
organizations cannot be expected to allocate freely  the critical financial and human resources needed to 
build a strong position in small, emerging markets.  And it is very difficult for a company whose cost 
structure is tailored to compete in high-end market s to be profitable in low-end markets as well. When  
a threatening disruptive technology requires a diff erent cost structure in order to be profitable and 
competitive, or when the current size of the opport unity is insignificant relative to the growth needs  of 
the mainstream organization, then—and only then—is a spin-out organization a required part of the 
solution. 
How separate does the effort need to be? The primar y requirement is that the project cannot be forced 
to compete with projects in the mainstream organiza tion for resources. Because values are the criteria  
by which prioritization decisions are made, project s that are inconsistent with a company’s mainstream  
values will naturally be accorded lowest priority. Whether the independent organization is physically 
separate is less important than is its independence  from the normal resource allocation process. 
In our studies of this challenge, we have never see n a company succeed in addressing a change that 
disrupts its mainstream values absent the personal,  attentive oversight of the CEO—precisely because 
of the power of processes and values and particular ly the logic of the normal resource allocation 
process. Only the CEO can ensure that the new organ ization gets the required resources and is free to 
create processes and values that are appropriate to  the new challenge. CEOs who view spin-outs as a 
tool to get disruptive threats off of their persona l agendas are almost certain to meet with failure. We 
have seen no exceptions to this rule. 
The framework summarized in Figure 8.1 can help man agers exploit the capabilities that reside in their  
current processes and values when that is possible,  and to create new ones, when the present 
organization is incapable. The left axis in Figure 8.1 measures the extent to which the existing 
processes—the patterns of interaction, communicatio n, coordination, and decision-making currently 
used in the organization—are the ones that will get  the new job done effectively. If the answer is yes  
(toward the lower end of the scale), the project ma nager can exploit the organization’s existing 
processes and organizational structure to succeed. As depicted in the corresponding position on the 
right axis, functional or lightweight teams, as des cribed by Clark and Wheelwright, 10  are useful 
structures for exploiting existing capabilities. In  such teams, the role of the project manager is to 
facilitate and coordinate work that is largely done  within functional organizations.  140   
  
Figure 8.1  Fitting an Innovation’s Requirements with the Orga nization’s Capabilities  
 
 
 
Note: The left and bottom axes reflect the question s the manager needs to ask about the existing 
situations. The notes at the right side represents the appropriate response to the situation on the le ft 
axis. The notes at the top represent the appropriat e response to the manager’s answer to the bottom 
axis.  
 
  
On the other hand, if the ways of getting work done  and of decision-making in the mainstream business 
would impede rather than facilitate the work of the  new team—because different people need to 
interact with different people about different subj ects and with different timing than has habitually been 
necessary—then a heavyweight team structure is nece ssary. Heavyweight teams are tools to create new 
processes—new ways of working together that constit ute new capabilities. In these teams, members do 
not simply represent the interests and skills of th eir function. They are charged to act like general 
managers, and reach decisions and make trade-offs f or the good of the project . They typically are 
dedicated and colocated. 
The horizontal axis of Figure 8.1 asks managers to assess whether the organization’s values will 
allocate to the new initiative the resources it wil l need in order to become successful. If there is a  poor, 
disruptive fit, then the mainstream organization’s values will accord low priority to the project. 
Therefore, setting up an autonomous organization wi thin which development and commercialization 
can occur will be absolutely essential to success. At the other extreme, however, if there is a strong , 
sustaining fit, then the manager can expect that th e energy and resources of the mainstream 
organization will coalesce behind it. There is no r eason for a skunk works or a spin-out in such cases . 
Region A in Figure 8.1 depicts a situation in which  a manager is faced with a breakthrough but 
sustaining technological change—it fits the organiz ation’s values. But it presents the organization wi th 
different types of problems to solve and therefore requires new types of interaction and coordination 
among groups and individuals. The manager needs a h eavyweight development team to tackle the new 
task, but the project can be executed within the ma instream company. This is how Chrysler, Eli Lilly, 
and Medtronic accelerated their product development  cycles so dramatically. 11  Heavyweight teams are 
the organizational mechanism that the managers of I BM’s disk drive division used to learn how to  141  integrate components more effectively in their prod uct designs, in order to wring 50 percent higher 
performance out of the components they used. Micros oft’s project to develop and launch its Internet 
browser was located in the Region A corner of this framework. It represented an extraordinary, difficu lt 
managerial achievement that required different peop le to work together in patterns different than any 
ever used before within Microsoft. But it was a sustaining  technology to the company. Its customers 
wanted the product, and it strengthened the company ’s integral business model. There was, therefore, 
no need to spin the project out into a completely d ifferent organization. 
When in Region B, where the project fits the compan y’s processes and values, a lightweight 
development team can be successful. In such teams c oordination across functional boundaries occurs 
within the mainstream organization. 
Region C denotes an area in which a manager is face d with a disruptive technological change that 
doesn’t fit the organization’s existing processes a nd values. To ensure success in such instances, 
managers should create an autonomous organization a nd commission a heavyweight development team 
to tackle the challenge. In addition to the example s cited in chapters 5, 6, and 7, many companies’ 
efforts to address the distribution channel conflic ts created by the Internet should be managed in thi s 
manner. In 1999 Compaq Computer, for example, launc hed a business to market its computers direct to 
customers over the Internet, so that it could compe te more effectively with Dell Computer. Within a 
few weeks its retailers had protested so loudly tha t Compaq had to back away from the strategy. This 
was very  disruptive to the values, or profit model, of the company and its retailers. The only way it 
could manage this conflict would be to launch the d irect business through an independent company. It 
might even need a different brand in order to manag e the tension. 
Some have suggested that Wal-Mart’s strategy of man aging its on-line retailing operation through an 
independent organization in Silicon Valley is foolh ardy, because the spin-out organization can’t 
leverage Wal-Mart’s extraordinary logistics managem ent processes and infrastructure. I believe the 
spin-out was wise, however, based upon Figure 8.1. The on-line venture actually needs very different 
logistics processes than those of its bricks-and-mo rtar operations. Those operations transport goods b y 
the truckload. On-line retailers need to pick indiv idual items from inventory and ship small packages to 
diverse locations. The venture is not only disrupti ve to Wal-Mart’s values, but it needs to create its  own 
logistics processes as well. It needed to be spun o ut separately. 
Region D typifies projects in which products or ser vices similar to those in the mainstream need to be  
sold within a fundamentally lower overhead cost bus iness model. Wal-Mart’s Sam’s Clubs would fit in 
this region. These, in fact, can leverage similar l ogistics management processes as the main company; 
but budgeting, management, and P&L responsibility n eeds to be different. 
Functional and lightweight teams are appropriate ve hicles for exploiting established capabilities, 
whereas heavyweight teams are tools for creating ne w ones. Spin-out organizations, similarly, are tool s 
for forging new values. Unfortunately, most compani es employ a one-size-fits-all organizing strategy, 
using lightweight teams for programs of every size and character. Among those few firms that have 
accepted the “heavyweight gospel,” many have attemp ted to organize all of their development teams in 
a heavyweight fashion. Ideally, each company should  tailor the team structure and organizational 
location to the process and values required by each  project. 
In many ways, the disruptive technologies model is a theory of relativity, because what is disruptive to 
one company might have a sustaining impact on anoth er. For example, Dell Computer began by selling 
computers over the telephone. For Dell, the initiat ive to begin selling and accepting orders over the 
Internet was a sustaining  innovation. It helped it make more money in the wa y it was already  142  structured. For Compaq, Hewlett-Packard, and IBM, h owever, marketing direct to customers over the 
Internet would have a powerfully disruptive impact.  The same is true in stock brokerage. For discount 
brokers such as Ameritrade and Charles Schwab, whic h accepted most of their orders by telephone, 
trading securities on-line simply helped them disco unt more cost-effectively—and even offer enhanced 
service relative to their former capabilities. For full-service firms with commissioned brokers such a s 
Merrill Lynch, however, on-line trading represents a powerful disruptive threat. 
 
 
SUMMARY   
 
Managers whose organizations are confronting change  must first determine that they have the resources 
required to succeed. They then need to ask a separa te question: does the organization have the 
processes and values to succeed? Asking this second  question is not as instinctive for most managers 
because the processes by which work is done and the  values by which employees make their decisions 
have served them well. What I hope this framework a dds to managers’ thinking, however, is that the 
very capabilities of their organizations also defin e their disabilities. A little time spent soul-sear ching 
for honest answers to this issue will pay off hands omely. Are the processes by which work habitually 
gets done in the organization appropriate for this new problem? And will the values of the organizatio n 
cause this initiative to get high priority, or to l anguish? 
If the answer to these questions is no, it’s okay. Understanding problems is the most crucial step in 
solving them. Wishful thinking about this issue can  set teams charged with developing and 
implementing an innovation on a course fraught with  roadblocks, second-guessing, and frustration. The 
reasons why innovation often seems to be so difficu lt for established firms is that they employ highly  
capable people, and then set them to work within pr ocesses and values that weren’t designed to 
facilitate success with the task at hand. Ensuring that capable people are ensconced in capable 
organizations is a major management responsibility in an age such as ours, when the ability to cope 
with accelerating change has become so critical. 
 
 
NOTES   
 
1.  See C. K. Prahalad, and Gary Hamel, “The Core Comp etence of the Corporation,” Harvard Business 
Review, 1990.  
2.  Many of these ideas emerged from wonderful, stimul ating discussions with doctoral students in the 
Business Policy seminar at the Harvard Business Sch ool between 1993 and 1999. I wish to thank all of 
those students, but in particular Don Sull, Tom Eis enmann, Tomoyoshi Noda, Michael Raynor, 
Michael Roberto, Deborah Sole, Clark Gilbert, and M ichael Overdorf for their contributions to these 
ideas.  
3.  The most logical, comprehensive characterization o f processes that we have seen is in David Garvin, 
“The Processes of Organization and Management,” Sloan Management Review , Summer, 1998. When 
we use the term “processes,” we mean for it to incl ude all of the types of processes that Garvin has 
defined.  
4.  See Dorothy Leonard-Barton, “Core Capabilities and  Core Rigidities: A Paradox in Managing New 
Product Development,” Strategic Management Journal  (13), 1992, 111–125. Professor Leonardi’s 
work on this topic, in my opinion, constitutes the fundamental paradigm upon which much subsequent  143  research is being built.  
5.  See Wickham Skinner, “The Focused Factory,” Harvard Business Review, 1974.  
6.  See, for example, Thomas Peters and Robert Waterma n, In Search of Excellence  (New York: Harper 
& Row Publishers, 1982).  
7.  See Edgar Schein, Organizational Culture and Leadership  (San Francisco: Jossey-Bass Publishers, 
1988). This description of the development of an or ganization’s culture draws heavily from Schein’s 
research.  
8.  See Nicole Tempest, “Cisco Systems, Inc. Post-Acqu isition Manufacturing Integration,“r a teaching 
case published jointly by the Stanford University G raduate School of Business and the Harvard 
Business School, 1998.  
9.  Steven C. Wheelwright and Kim B. Clark, Revolutionizing Product Development  (New York: The 
Free Press, 1992).  
10.  See Kim B. Clark and Steven C. Wheelwright, “Organ izing and Leading Heavyweight 
Development Teams,” California Management Review  (34), Spring, 1992, 9–28. The concepts 
described in this article are extremely important. We highly recommend that managers interested in 
these problems study it thoughtfully. They define a  heavyweight team as one in which team members 
typically are dedicated and colocated. The charge o f each team member is not to represent their 
functional group on the team, but to act as a general manager —to assume responsibility for the success 
of the entire project, and to be actively involved in the decisions and work of members who come from 
each functional area. As they work together to comp lete their project, they will work out new ways of 
interacting, coordinating, and decision-making that  will come to comprise the new processes, or new 
capabilities, that will be needed to succeed in the  new enterprise on an ongoing basis. These ways of 
getting work done then get institutionalized as the  new business or product line grows.  
11.  See Jeff Dyer, “How Chrysler Created an American K eiretsu,” Harvard Business Review , July-
August, 1996, 42–56; Clayton M. Christensen, “We’ve  Got Rhythm! Medtronic Corporation’s Cardiac 
Pacemaker Business,” Harvard Business School, Case No. 698–004; and Steven C. Wheelwright, “Eli 
Lilly: The Evista Project,” Harvard Business School , Case No. 699-016.  
  
 
 
 
 
 
 
 
 
 
  144   
CHAPTER NINE  
Performance Provided,  
Market Demand, and the  
Product Life Cycle 
 
 
The graphs in this book showing the intersecting te chnology and market trajectories have proven useful  
in explaining how leading firms can stumble from po sitions of industry leadership. In each of the 
several industries explored, technologists were abl e to provide rates of performance improvement that 
have exceeded the rates of performance improvement that the market has needed or was able to absorb. 
Historically, when this performance oversupply  occurs, it creates an opportunity for a disruptive  
technology to emerge and subsequently to invade est ablished markets from below.  
As it creates this threat or opportunity for a disr uptive technology, performance oversupply also 
triggers a fundamental change in the basis of compe tition in the product’s market: The rank-ordering o f 
the criteria by which customers choose one product or service over another will change, signaling a 
transition from one phase (variously defined by man agement theorists) to the next of the product life 
cycle. In other words, the intersecting trajectorie s of performance supplied and performance demanded 
are fundamental triggers behind the phases in the p roduct life cycle. Because of this, trajectory maps  
such as those used in this book usefully characteri ze how an industry’s competitive dynamics and its 
basis of competition are likely to change over time . 
As with past chapters, this discussion begins with an analysis from the disk drive industry of what ca n 
happen when the performance supplied exceeds the ma rket’s demands. After seeing the same 
phenomenon played out in the markets for accounting  software and for diabetes care products, the link 
between this pattern and the phases of the product life cycle will be clear. 
 
 
PERFORMANCE OVERSUPPLY AND CHANGING BASES OF COMPET ITION   
 
The phenomenon of performance oversupply is charted  in Figure 9.1, an extract from Figure 1.7. It 
shows that by 1988, the capacity of the average 3.5 -inch drive had finally increased to equal the 
capacity demanded in the mainstream desktop persona l computer market, and that the capacity of the 
average 5.25-inch drive had by that time surpassed what the mainstream desktop market demanded by 
nearly 300 percent. At this point, for the first ti me since the desktop market emerged, computer maker s 
had a choice of drives to buy: The 5.25- and 3.5-in ch drives both  provided perfectly adequate capacity. 
What was the result? The desktop personal computer makers began switching to 3.5-inch drives in 
droves. Figure 9.2 illustrates this, using a substi tution curve format in which the vertical axis meas ures 
the ratio of new- to old-technology units sold. In 1985 this measure was .007, meaning that less than 1 
percent (.0069) of the desktop market had switched to the 3.5-inch format. By 1987, the ratio had 
advanced 0.20, meaning that 16.7 percent of the uni ts sold into this market that year were 3.5-inch  145  drives. By 1989, the measure was 1.5, that is, only  four years after the 3.5-inch product had appeared  as 
a faint blip on the radar screen of the market, it accounted for 60 percent of drive sales. 
Why did the 3.5-inch drive so decisively conquer th e desktop PC market? A standard economic guess 
might be that the 3.5-inch format represented a mor e cost-effective architecture: If there were no lon ger 
any meaningful differentiation between two types of  products (both had adequate capacity), price 
competition would intensify. This was not the case here, however. Indeed, computer makers had to pay, 
on average, 20 percent more per megabyte to use 3.5 -inch drives, and yet they still  flocked to the 
product. Moreover, computer manufacturers opted for  the costlier drive while facing fierce price 
competition in their own product markets. Why? 
 
  
Figure 9.1 Intersecting Trajectories of Capacity Demanded vers us Capacity Supplied in Rigid Disk 
Drives  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
  
Performance oversupply triggered a change in the ba sis of competition. Once the demand for capacity 
was satiated, other attributes, whose performance h ad not yet satisfied market demands, came to be 
more highly valued and to constitute the dimensions  along which drive makers sought to differentiate 
their products. In concept, this meant that the mos t important attribute measured on the vertical axis  of 
figures such as 8.1 changed, and that new trajector ies of product performance, compared to market 
demands, took shape.  146  Specifically, in the desktop personal computer mark etplace between 1986 and 1988, the smallness of 
the drive began to matter more than other features.  The smaller 3.5-inch drive allowed computer 
manufacturers to reduce the size, or desktop footpr int, of their machines. At IBM, for example, the 
large XT/AT box gave way to the much smaller PS1/PS 2 generation machines. 
 
  
Figure 9.2 Substitution of 8-, 5.25-, and 3.5-Inch Drives of 3 0 to 100 MB  
 
 
 
Source: Data are from various issues of Disk/Trend Report.   
 
  
For a time, when the availability of small drives d id not satisfy market demands, desktop computer 
makers continued to pay a hefty premium for 3.5-inc h drives. In fact, using the hedonic regression 
analysis described in chapter 4, the 1986 shadow pr ice for a one-cubic-inch reduction in the volume of  
a disk drive was $4.72. But once the computer maker s had configured their new generations of desktop 
machines to use the smaller drive, their demand for  even more smallness was satiated. As a result, the  
1989 shadow price, or the price premium accorded to  smaller drives, diminished to $0.06 for a one-
cubic-inch reduction. 
Generally, once the performance level demanded of a  particular attribute has been achieved, customers 
indicate their satiation by being less willing to p ay a premium price for continued improvement in tha t 
attribute. Hence, performance oversupply triggers a  shift in the basis of competition, and the criteri a 
used by customers to choose one product over anothe r changes to attributes for which market demands 
are not yet satisfied. 
Figure 9.3 summarizes what seems to have happened i n the desktop PC market: The attribute measured 
on the vertical axis repeatedly changed. Performanc e oversupply in capacity triggered the first 
redefinition of the vertical axis, from capacity to  physical size. When performance on this new  147  dimension satisfied market needs, the definition of  performance on the vertical axis changed once 
more, to reflect demand for reliability. For a time , products offering competitively superior shock 
resistance and mean time between failure (MTBF) wer e accorded a significant price premium, 
compared to competitive offerings. But as MTBF valu es approached one million hours, 1 the shadow 
price accorded to an increment of one hundred hours  MTBF approached zero, suggesting performance 
oversupply on that dimension of product performance . The subsequent and current phase is an intense 
price-based competition, with gross margins tumblin g below 12 percent in some instances. 
 
 
WHEN DOES A PRODUCT BECOME A COMMODITY?   
 
The process of commoditization of disk drives was d efined by the interplay between the trajectories of  
what the market demanded and what the technology su pplied. The 5.25-inch drive had become a price-
driven commodity in the desktop market by about 198 8, when the 3.5-inch drive was still at a premium 
price. The 5.25-inch drive, in addition, even thoug h priced as a commodity in desktop applications, wa s 
at the same time, relative to 8-inch drives, achiev ing substantial price premiums in higher-tier marke ts. 
As described in chapter 4, this explains the aggres sive moves upmarket made by established 
companies. 
 
  
Figure 9.3 Changes in the Basis of Competition in the Disk Dri ve Industry  
  148  
 
 
  
A product becomes a commodity within a specific mar ket segment when the repeated changes in the 
basis of competition, as described above, completel y play themselves out, that is, when market needs 
on each attribute or dimension of performance have been fully satisfied by more than one available 
product. The performance oversupply framework may h elp consultants, managers, and researchers to 
understand the frustrated comments they regularly h ear from salespeople beaten down in price 
negotiations with customers: “Those stupid guys are  just treating our product like it was a commodity.  
Can’t they see how much better our product is than the competition’s?” It may, in fact, be the case th at 
the product offerings of competitors in a market co ntinue to be differentiated from each other. But 
differentiation loses its meaning when the features  and functionality have exceeded what the market 
demands. 
 
 
PERFORMANCE OVERSUPPLY AND THE EVOLUTION OF PRODUCT  COMPETITION   
 
The marketing literature provides numerous descript ions of the product life cycle and of the ways in 
which the characteristics of products within given categories evolve over time. 2 The findings in this 
book suggest that, for many of these models, perfor mance oversupply is an important factor driving the  
transition from one phase of the cycle to the next.   149  Consider, for example, the product evolution model,  called the buying hierarchy  by its creators, 
Windermere Associates of San Francisco, California,  which describes as typical the following four 
phases: functionality, reliability, convenience, an d price. Initially, when no available product satis fies 
the functionality requirements the market, the basi s of competition, or the criteria by which product 
choice is made, tends to be product functionality.  (Sometimes, as in disk drives, a market may cycle 
through several different functionality dimensions. ) Once two or more products credibly satisfy the 
market’s demand for functionality, however, custome rs can no longer base their choice of products on 
functionality, but tend to choose a product and ven dor based on reliability.  As long as market demand 
for reliability exceeds what vendors are able to pr ovide, customers choose products on this basis—and 
the most reliable vendors of the most reliable prod ucts earn a premium for it. 
But when two or more vendors improve to the point t hat they more than satisfy the reliability 
demanded by the market, the basis of competition sh ifts to convenience.  Customers will prefer those 
products that are the most convenient to use and th ose vendors that are most convenient to deal with. 
Again, as long as the market demand for convenience  exceeds what vendors are able to provide, 
customers choose products on this basis and reward vendors with premium prices for the convenience 
they offer. Finally, when multiple vendors offer a package of convenient products and services that 
fully satisfies market demand, the basis of competi tion shifts to price.  The factor driving the transition 
from one phase of the buying hierarchy to the next is performance oversupply. 
Another useful conception of industry evolution, fo rmulated by Geoffrey Moore in his book Crossing 
the Chasm,  3 has a similar underlying logic, but articulates the  stages in terms of the user rather than the 
product. Moore suggests that products are initially  used by innovators and early adopters  in an 
industry—customers who base their choice solely on the product’s functionality. During this phase the 
top-performing products command significant price p remiums. Moore observes that markets then 
expand dramatically after the demand for functional ity in the mainstream market has been met, and 
vendors begin to address the need for reliability a mong what he terms early majority  customers. A third 
wave of growth occurs when product and vendor relia bility issues have been resolved, and the basis of 
innovation and competition shifts to convenience, t hus pulling in the late majority  customers. 
Underlying Moore’s model is the notion that technol ogy can improve to the point that market demand 
for a given dimension of performance can be satiate d. 
This evolving pattern in the basis of competition—f rom functionality, to reliability and convenience, 
and finally to price—has been seen in many of the m arkets so far discussed. In fact, a key characteris tic 
of a disruptive technology is that it heralds a cha nge in the basis of competition. 
 
 
OTHER CONSISTENT CHARACTERISTICS OF DISRUPTIVE TECH NOLOGIES   
 
Two additional important characteristics of disrupt ive technologies consistently affect product life 
cycles and competitive dynamics: First, the attribu tes that make disruptive products worthless in 
mainstream markets typically become their strongest  selling points in emerging markets; and second, 
disruptive products tend to be simpler, cheaper, an d more reliable and convenient than established 
products. Managers must understand these characteri stics to effectively chart their own strategies for  
designing, building, and selling disruptive product s. Even though the specific market applications for  
disruptive technologies cannot be known in advance,  managers can bet on these two regularities.  150   
 
1. The Weaknesses of Disruptive Technologies Are Th eir Strengths   
 
The relation between disruptive technologies and th e basis of competition in an industry is complex. I n 
the interplay among performance oversupply, the pro duct life cycle, and the emergence of disruptive 
technologies, it is often the very attributes that render disruptive technologies useless in mainstrea m 
markets that constitute their value in new markets.  
In general, companies that have succeeded in disrup tive innovation initially took the characteristics and 
capabilities of the technology for granted and soug ht to find or create a new market that would value or 
accept those attributes. Thus, Conner Peripherals c reated a market for small drives in portable 
computers, where smallness was valued; J. C. Bamfor d and J. I. Case built a market for excavators 
among residential contractors, where small buckets and tractor mobility actually created value; and 
Nucor found a market that didn’t mind the surface b lemishes on its thin-slab-cast sheet steel. 
The companies toppled by these disruptive technolog ies, in contrast, each took the established market’ s 
needs  as given, and did not attempt to market the techno logy until they felt it was good enough to be 
valued in the mainstream market. Thus, Seagate’s ma rketers took the firm’s early 3.5-inch drives to 
IBM for evaluation, rather than asking, “Where is t he market that would actually value a smaller, 
lower-capacity drive?” When Bucyrus Erie acquired i ts Hydrohoe hydraulic excavator line in 1951, its 
managers apparently did not ask, “Where is the mark et that actually wants  a mobile excavator that can 
only dig narrow trenches?” They assumed instead tha t the market needed the largest possible bucket 
size and the longest possible reach; they jury-rigg ed the Hydrohoe with cables, pulleys, clutches, and  
winches and attempted to sell it to general excavat ion contractors. When U.S. Steel was evaluating 
continuous thin-slab casting, they did not ask, “Wh ere is the market for low-priced sheet steel with 
poor surface appearance?” Rather, they took it for granted that the market needed the highest-possible  
quality of surface finish and invested more capital  in a conventional caster. They applied to a disrup tive 
innovation a way of thinking appropriate to a susta ining technology. 
In the instances studied in this book, established firms confronted with disruptive technology typical ly 
viewed their primary development challenge as a technological  one: to improve the disruptive 
technology enough that it suits known markets. In c ontrast, the firms that were most successful in 
commercializing a disruptive technology were those framing their primary development challenge as a 
marketing  one: to build or find a market where product compe tition occurred along dimensions that 
favored the disruptive attributes of the product. 4 
It is critical that managers confronting disruptive  technology observe this principle. If history is a ny 
guide, companies that keep disruptive technologies bottled up in their labs, working to improve them 
until they suit mainstream markets, will not be nea rly as successful as firms that find markets that 
embrace the attributes of disruptive technologies a s they initially stand. These latter firms, by crea ting a 
commercial base and then moving upmarket, will ulti mately address the mainstream market much more 
effectively than will firms that have framed disrup tive technology as a laboratory, rather than a 
marketing, challenge. 
 
 
2. Disruptive Technologies Are Typically Simpler, C heaper, and More Reliable and Convenient than 
Established Technologies    151   
When performance oversupply has occurred and a disr uptive technology attacks the underbelly of a 
mainstream market, the disruptive technology often succeeds both because it satisfies the market’s nee d 
for functionality, in terms of the buying hierarchy , and because it is simpler, cheaper, and more reli able 
and convenient than mainstream products. Recall, fo r example, the attack of hydraulic excavation 
technology into the mainstream sewer and general ex cavation markets recounted in chapter 3. Once 
hydraulically powered excavators had the strength t o handle buckets of 2 to 4 cubic yards of earth 
(surpassing the performance demanded in mainstream markets), contractors rapidly switched to these 
products even though the cable-actuated machines we re capable of moving even more earth per scoop. 
Because both technologies provided adequate bucket capacity for their needs, contractors opted for the  
technology that was most reliable: hydraulics. 
Because established companies are so prone to push for high-performance, high-profit products and 
markets, they find it very difficult not to overloa d their first disruptive products with features and  
functionality. Hewlett-Packard’s experience in desi gning its 1.3-inch Kittyhawk disk drive teaches jus t 
this lesson. Unable to design a product that was tr uly simple and cheap, Kittyhawk’s champions pushed 
its capacity to the limits of technology and gave i t levels of shock resistance and power consumption 
that would make it competitive as a sustaining prod uct. When very high volume applications for a 
cheap, simple, single-function, 10 MB drive began t o emerge, HP’s product was not disruptive enough 
to catch that wave. Apple committed a similar error  in stretching the functionality of its Newton, 
instead of initially targeting simplicity and relia bility. 
 
 
PERFORMANCE OVERSUPPLY IN THE ACCOUNTING SOFTWARE M ARKET   
 
Intuit, the maker of financial management software,  is known primarily for its extraordinarily 
successful personal financial software package, Quicken. Quicken  dominates its market because it is 
easy and convenient. Its makers pride themselves on  the fact that the vast majority of Quicken  
customers simply buy the program, boot it up on the ir computers, and begin using it without having to 
read the instruction manual. Its developers made it  so convenient to use, and continue to make it 
simpler and more convenient, by watching how custom ers use  the product, not by listening to what 
they or the “experts” say they need. By watching fo r small hints of where the product might be difficu lt 
or confusing to use, the developers direct their en ergies toward a progressively simpler, more 
convenient product that provides adequate, rather t han superior, functionality. 5  
Less well known is Intuit’s commanding 70 percent s hare of the North American small business 
accounting software market. 6 Intuit captured that share as a late entrant when it launched Quickbooks,  a 
product based on three simple insights. First, prev iously available small business accounting packages  
had been created under the close guidance of certif ied public accountants and required users to have a  
basic knowledge of accounting (debits and credits, assets and liabilities, and so on) and to make ever y 
journal entry twice (thus providing an audit trail for each transaction). Second, most existing packag es 
offered a comprehensive and sophisticated array of reports and analyses, an array that grew ever more 
complicated and specialized with each new release a s developers sought to differentiate their products  
by offering greater functionality. And third, 85 pe rcent of all companies in the United States were to o 
small to employ an accountant: The books were kept by the proprietors or by family members, who had 
no need for or understanding of most of the entries  and reports available from mainstream accounting 
software. They did not know what an audit trail was , let alone sense a need to use one.  152  Scott Cook, Intuit’s founder, surmised that most of  these small companies were run by proprietors who 
relied more on their intuition and direct knowledge  of the business than on the information contained in 
accounting reports. In other words, Cook decided th at the makers of accounting software for small 
businesses had overshot the functionality required by that market, thus creating an opportunity for a 
disruptive software technology that provided adequa te, not superior functionality and was simple and 
more convenient to use. Intuit’s disruptive Quickbooks  changed the basis of product competition from 
functionality to convenience and captured 70 percen t of its market within two years of its introductio n. 7 
In fact, by 1995 Quickbooks  accounted for a larger share of Intuit’s revenues than did Quicken.  
The response of established makers of small busines s accounting software to Intuit’s invasion, quite 
predictably, has been to move upmarket, continuing to release packages loaded with greater 
functionality; these focus on specific market subse gments, targeted at sophisticated users of 
information systems at loftier tiers of the market.  Of the three leading suppliers of small business 
accounting software (each of which claimed about 30  percent of the market in 1992), one has 
disappeared and one is languishing. The third has i ntroduced a simplified product to counter the 
success of Quickbooks,  but it has claimed only a tiny portion of the mark et. 
 
 
PERFORMANCE OVERSUPPLY IN THE PRODUCT LIFE CYCLE OF  INSULIN   
 
Another case of performance oversupply and disrupti ve technology precipitating a change in the basis 
of competition—and threatening a change in industry  leadership—is found in the worldwide insulin 
business. In 1922, four researchers in Toronto firs t successfully extracted insulin from the pancrease s 
of animals and injected it, with miraculous results , into humans with diabetes. Because insulin was 
extracted from the ground-up pancreases of cows and  pigs, improving the purity of insulin (measured 
in impure parts per million, or ppm) constituted a critical trajectory of performance improvement. 
Impurities dropped from 50,000 ppm in 1925 to 10,00 0 ppm in 1950 to 10 ppm in 1980, primarily as 
the result of persistent investment and effort by t he world’s leading insulin manufacturer, Eli Lilly and 
Company. 
Despite this improvement, animal insulins, which ar e slightly different from human insulin, caused a 
fraction of a percent of diabetic patients to build  up resistance in their immune systems. Thus, in 19 78, 
Eli Lilly contracted with Genentech to create genet ically altered bacteria that could produce insulin 
proteins that were the structural equivalent of hum an insulin proteins and 100 percent pure. The proje ct 
was technically successful, and in the early 1980s,  after a nearly $1 billion investment, Lilly introd uced 
its Humulin-brand insulin to the market. Priced at a 25 percent premium over insulins of animal 
extraction, because of its human equivalence and it s purity, Humulin was the first commercial-scale 
product for human consumption to emerge from the bi otechnology industry. 
The market’s response to this technological miracle , however, was tepid. Lilly found it very difficult  to 
sustain a premium price over animal insulin, and th e growth in the sales volume of Humulin was 
disappointingly slow. “In retrospect,” noted a Lill y researcher, “the market was not terribly dissatis fied 
with pork insulin. In fact, it was pretty happy wit h it.” 8 Lilly had spent enormous capital and 
organizational energy overshooting the market’s dem and for product purity. Once again, this was a 
differentiated product to which the market did not accord a price premium because the performance it 
provided exceeded what the market demanded.  153  Meanwhile, Novo, a much smaller Danish insulin make r, was busy developing a line of insulin pens,  a 
more convenient way for taking insulin. Conventiona lly, people with diabetes carried a separate 
syringe, inserted its needle into one glass insulin  vial, pulled its plunger out to draw slightly more  than 
the desired amount of insulin into the syringe, and  held up the needle and flicked the syringe several  
times to dislodge any air bubbles that clung to the  cylinder walls. They generally then had to repeat this 
process with a second, slower acting type of insuli n. Only after squeezing the plunger slightly to for ce 
any remaining bubbles—and, inevitably, some insulin —out of the syringe could they inject themselves 
with the insulin. This process typically took one t o two minutes. 
Novo’s pen, in contrast, held a cartridge containin g a couple of weeks’ supply of insulin, usually 
mixtures of both the fast-acting and the gradually released types. People using the Novo pen simply 
had to turn a small dial to the amount of insulin t hey needed to inject, poke the pen’s needle under t he 
skin, and press a button. The procedure took less t han ten seconds. In contrast to Lilly’s struggle to  
command a premium price for Humulin, Novo’s conveni ent pens easily sustained a 30 percent price 
premium per unit of insulin. Through the 1980s, pro pelled largely by the success of its line of pens a nd 
pre-mixed cartridges, Novo increased its share of t he worldwide insulin market substantially—and 
profitably. Lilly’s and Novo’s experiences offer fu rther proof that a product whose performance 
exceeds market demands suffers commodity-like prici ng, while disruptive products that redefine the 
basis of competition command a premium. 
Teaching the Harvard Business School case to execut ives and MBA students about Lilly overshooting 
the market demand for insulin purity has been one o f my most interesting professional experiences. In 
every class, the majority of students quickly pounc e on Lilly for having missed something so 
obvious—that only a fraction of a percent of people  with diabetes develop insulin resistance—and that 
the differentiation between highly purified pork in sulin at 10 ppm and perfectly pure Humulin was not 
significant. Surely, they assert, a few simple focu s groups in which patients and doctors were asked 
whether they wanted purer insulin would have given Lilly adequate guidance. 
In every discussion, however, more thoughtful stude nts soon begin to sway class opinion toward the 
view that (as we have seen over and over) what is o bvious in retrospect might not be at all obvious in  
the thick of battle. Of all the physicians to whom Lilly’s marketers listened, for example, which ones  
tended to carry the most credibility? Endocrinologi sts whose practices focused on diabetes care, the 
leading customers in this business. What sorts of p atients are most likely to consume the professional  
interests of these specialists? Those with the most  advanced and intractable problems, among which 
insulin resistance was prominent. What, therefore, were these leading customers likely to tell Lilly’s  
marketers when they asked what should be done to im prove the next-generation insulin product? 
Indeed, the power and influence of leading customer s is a major reason why companies’ product 
development trajectories overshoot the demands of m ainstream markets. 
Furthermore, thoughtful students observe that it wo uld not even occur to most marketing managers to 
ask the question of whether a 100 percent pure huma n insulin might exceed market needs. For more 
than fifty years in a very successful company with a very strong culture, greater purity was the very 
definition of a better product. Coming up with pure r insulins had always  been the formula for staying 
ahead of the competition. Greater purity had always  been a catching story that the salesforce could us e 
to attract the time and attention of busy physician s. What in the company’s history would cause its 
culture-based assumptions suddenly to change and it s executives to begin asking questions that never 
before had needed to be answered? 9  154   
 
CONTROLLING THE EVOLUTION OF PRODUCT COMPETITION   
 
Figure 9.4 summarizes the model of performance over supply, depicting a multi-tiered market in which 
the trajectory of performance improvement demanded by the market is shallower than the trajectory of 
improvement supplied by technologists. Hence, each tier of the market progresses through an 
evolutionary cycle marked by a shifting basis for p roduct choice. Although other terms for product lif e 
cycles would yield similar results, this diagram us es the buying hierarchy devised by Windermere 
Associates, in which competition centers first on f unctionality, followed by reliability, convenience,  
and, finally, price. In each of the cases reviewed in this chapter, the products heralding shifts in t he 
basis of competition and progression to the next pr oduct life cycle phase were disruptive technologies . 
 
  
Figure 9.4 Managing Changes in the Basis of Competition  
 
 
 
 
  
The figure shows the strategic alternatives availab le to companies facing performance oversupply and 
the consequent likelihood that disruptive approache s will change the nature of competition in their 
industry. The first general option, labeled strateg y 1 and the one most commonly pursued in the 
industries explored in this book, is to ascend the trajectory of sustaining technologies into ever-hig her 
tiers of the market, ultimately abandoning lower-ti er customers when simpler, more convenient, or less  
costly disruptive approaches emerge. 
A second alternative, labeled strategy 2, is to mar ch in lock-step with the needs of customers in a gi ven 
tier of the market, catching successive waves of ch ange in the basis of competition. Historically, thi s 
appears to have been difficult to do, for all of th e reasons described in earlier chapters. In the per sonal 
computer industry, for example, as the functionalit y of desktop machines came to satiate the demands 
of the lower tiers of the market, new entrants such  as Dell and Gateway 2000 entered with value  155  propositions centered on convenience of purchase an d use. In the face of this, Compaq responded by 
actively pursuing this second approach, aggressivel y fighting any upmarket drift by producing a line o f 
computers with low prices and modest functionality targeted to the needs of the lower tiers of the 
market. 
The third strategic option for dealing with these d ynamics is to use marketing initiatives to steepen the 
slopes of the market trajectories so that customers  demand the performance improvements that the 
technologists provide. Since a necessary condition for the playing out of these dynamics is that the 
slope of the technology trajectory be steeper than the market’s trajectory, when the two slopes are 
parallel, performance oversupply—and the progressio n from one stage of the product life cycle to the 
next—does not occur or is at least postponed. 
Some computer industry observers believe that Micro soft, Intel, and the disk drive companies have 
pursued this last strategy very effectively. Micros oft has used its industry dominance to create and 
successfully market software packages that consume massive amounts of disk memory and require 
ever-faster microprocessors to execute. It has, ess entially, increased the slopes of the trajectories of 
improvement in functionality demanded by their cust omers to parallel the slope of improvement 
provided by their technologists. The effect of this  strategy is described in Figure 9.5, depicting rec ent 
events in the disk drive industry. (This chart upda tes through 1996 the disk drive trajectory map in 
Figure 1.7 .) Notice how the trajectories of capacity demanded  in the mid-range, desktop, and notebook 
computer segments kinked upward in the 1990s along a path that essentially paralleled the capacity 
path blazed by the makers of 3.5-inch and 2.5-inch disk drives. Because of this, these markets have no t 
experienced performance oversupply in recent years.  The 2.5-inch drive remains locked within the 
notebook computer market because capacity demanded on the desktop is increasing at too brisk a pace. 
The 3.5-inch drive remains solidly ensconced in the  desktop market, and the 1.8-inch drive has 
penetrated few notebook computers, for the same rea sons. In this situation, the companies whose 
products are positioned closest to the top of the m arket, such as Seagate and IBM, have been the most 
profitable, because in the absence of technology ov ersupply, a shift in the stages of the product life  
cycle at the high end of the market has been held a t bay. 
 
  
Figure 9.5 Changed Performance Demand Trajectories and the Def erred Impact of Disruptive 
Technologies  
  156  
 
 
Source: An earlier version of this figure was published in Clayton M. Christensen, “The Rigid Disk 
Drive Industry: A History of Commercial and Technol ogical Turbulence,” Business History Review 67, 
no. 4 (Winter 1993): 559.  
 
  
It is unclear how long the marketers at Microsoft, Intel, and Seagate can succeed in creating demand 
for whatever functionality their technologists can supply. Microsoft’s Excel  spreadsheet software, for 
example, required 1.2 MB of disk storage capacity i n its version 1.2, released in 1987. Its version 5. 0, 
released in 1995, required 32 MB of disk storage ca pacity. Some industry observers believe that if a 
team of developers were to watch typical users, the y would find that functionality has substantially 
overshot mainstream market demands. If true, this c ould create an opportunity for a disruptive 
technology—applets picked off the internet and used  in simple internet appliances rather than in full-
function computers, for example—to invade this mark et from below. 
 
 
RIGHT AND WRONG STRATEGIES   
 
Which of the strategies illustrated in Figure 9.4 i s best? This study finds clear evidence that there is no 
one best strategy. Any of the three, consciously pu rsued, can be successful. Hewlett-Packard’s pursuit  
of the first strategy in its laser jet printer busi ness has been enormously profitable. In this instan ce, it 
has been a safe strategy as well, because HP is att acking its own position with disruptive ink-jet 
technology. Compaq Computer and the trinity of Inte l, Microsoft, and the disk drive makers have 
successfully—at least to date—implemented the secon d and third strategies, respectively. 
These successful practitioners have in common their  apparent understanding—whether explicit or 
intuitive—of both their customers’ trajectories of need and their own technologists’ trajectories of  157  supply. Understanding these trajectories is the key  to their success thus far. But the list of firms t hat 
have consistently done this is disturbingly short. Most well-run companies migrate unconsciously to th e 
northeast, setting themselves up to be caught by a change in the basis of competition and an attack fr om 
below by disruptive technology. 
 
 
NOTES   
 
1.  In disk drive industry convention, a mean time bet ween failure measure of one million hours means 
that if one million disk drives were turned on simu ltaneously and operated continuously for one hour, 
one of those drives would fail within the first hou r.  
2.  Three of the earliest and most influential papers that proposed the existence of product life cycles 
were Jay W. Forrester, “Industrial Dynamics,” Harvard Business Review,  July–August, 1958, 9–14; 
Arch Patton, “Stretch Your Products’ Earning Years— Top Management’s Stake in the Product Life 
Cycle,” Management Review  (38), June, 1959, 67–79; and William E. Cox, “Prod uct Life Cycles as 
Marketing Models,” Journal of Business  (40), October, 1967, 375. Papers summarizing the c onceptual 
and empirical problems surrounding the product life  cycle concept include Nariman K. Dhalla and 
Sonia Yuspeh, “Forget the Product Life Cycle Concep t!” Harvard Business Review,  January–February, 
1976, 102–112; David R. Rink and John E. Swan, “Pro duct Life Cycle Research: A Literature 
Review,” Journal of Business Research,  1979, 219; and George S. Day, ”The Product Life Cy cle: 
Analysis and Applications Issues,” Journal of Marketing  (45), Fall, 1981, 60–67. A paper by Gerard J. 
Tellis and C. Merle Crawford, “An Evolutionary Appr oach to Product Growth Theory,” Journal of 
Marketing  (45), Fall, 1981, 125–132, contains a cogent criti que of the product life cycle concept, and 
presents a theory of product evolution that presage s many of the ideas presented in this section.  
3.  Geoffrey A. Moore, Crossing the Chasm  (New York: HarperBusiness, 1991).  
4.  The same behavior characterized the emergence of p ortable radios. In the early 1950s, Akio Morita, 
the chairman of Sony, took up residence in an inexp ensive New York City hotel in order to negotiate a 
license to AT&T’s patented transistor technology, w hich its scientists had invented in 1947. Morita 
found AT&T to be a less-than-willing negotiator and  had to visit the company repeatedly badgering 
AT&T to grant the license. Finally AT&T relented. A fter the meeting ended in which the licensing 
documents were signed, an AT&T official asked Morit a what Sony planned to do with the license. “We 
will build small radios,” Morita replied. “Why woul d anyone care about smaller radios?” the official 
queried. “We’ll see,” was Morita’s answer. Several months later Sony introduced to the U.S. market 
the first portable transistor radio. According to t he dominant metrics of radio performance in the 
mainstream market, these early transistor radios we re really bad, offering far lower fidelity and much  
more static than the vacuum tube-based tabletop rad ios that were the dominant design of the time. But 
rather than work in his labs until his transistor r adios were performance-competitive in the major 
market (which is what most of the leading electroni cs companies did with transistor technology), 
Morita instead found a market that valued the attri butes of the technology as it existed at the time—t he 
portable personal radio. Not surprisingly, none of the leading makers of tabletop radios became a 
leading producer of portable radios, and all were s ubsequently driven from the radio market. (This 
story was recounted to me by Dr. Sheldon Weinig, re tired vice chairman for manufacturing and 
technology of Sony Corporation.)  
5.  John Case, “Customer Service: The Last Word,” Inc. Magazine,  April, 1991, 1–5.  
6.  This information in this section was given to the author by Scott Cook, the founder and chairman of 
Intuit Corporation, and by Jay O’Connor, marketing manager for Quickbooks.   
7.  Cook recounts that in the process of designing a s imple and convenient accounting software 
package, Intuit’s developers arrived at a profound insight. The double-entry accounting system  158  originally developed by Venetian merchants to catch  arithmetical mistakes continued to be used in 
every available package of accounting software—even  though computers typically do not make 
mistakes in addition and subtraction. Intuit was ab le to greatly simplify its product by eliminating t his 
unneeded dimension of product functionality.  
8.  See “Eli Lilly & Co.: Innovation in Diabetes Care, ” Harvard Business School, Case No. 9-696-077. 
This case notes that although Lilly was not able to  achieve premium pricing for its Humulin insulin, i t 
benefited from the investment. Humulin protected Li lly against a possible shortfall in the pancreas 
supply, threatened by declining red meat consumptio n, and it gave Lilly a very valuable experience and  
asset base in the volume manufacturing of bioengine ered drugs.  
9.  Once such minority opinions have been raised in cl ass, many students then begin to see that 
institutions widely regarded as among the best-mana ged and most successful in the world may have 
overshot what their mainstream markets demand. Inte l, for example, has always measured the speed of 
its microprocessors on the vertical axis of its per formance graphs. It has always assumed that the 
market demands ever-faster microprocessors, and evi dence to the tune of billions of dollars in profit 
has certainly confirmed that belief. Certainly some  leading-edge customers need chips that process 
instructions at rates of 200, 400, and 800 MHz. But  what about the mainstream market? Is it possible 
that sometime soon the speed and cost of Intel’s ne w microprocessors might overshoot market 
demands? And if technology oversupply is possible, how will thousands of Intel employees be able to 
recognize when this has occurred, accepting the cha nge with enough conviction to completely alter the 
trajectory of their development efforts? Discerning  technology oversupply is difficult. Doing somethin g 
about it is even more so. 
  
 
 
 
 
 
 
 
 
 
 
 
 
  159   
CHAPTER TEN  
Managing Disruptive  
Technological Change:  
A Case Study 
 
 
As we approach the end of this book, we should bett er understand why great companies can stumble. 
Incompetence, bureaucracy, arrogance, tired executi ve blood, poor planning, and short-term investment 
horizons obviously have played leading roles in top pling many companies. But we have learned here 
that even the best managers are subject to certain laws that make disruptive innovation difficult. It is 
when great managers haven’t understood or have atte mpted to fight these forces that their companies 
have stumbled.  
This chapter uses the forces and principles describ ed in earlier chapters to illustrate how managers c an 
succeed when faced with disruptive technology chang e. To do so, I employ a case study format, using a 
personal voice, to suggest how I, as a hypothetical  employee of a major automaker, might manage a 
program to develop and commercialize one of the mos t vexing innovations of our day: the electric 
vehicle. My purpose here is explicitly not  to offer any so-called right answer to this partic ular 
challenge, nor to predict whether or how electric v ehicles may become commercially successful. 
Rather, it is to suggest in a familiar but challeng ing context how managers might structure their 
thinking about a similar problem by proposing a seq uence of questions that, if asked, can lead to a 
sound and useful answer. 
 
 
HOW CAN WE KNOW IF A TECHNOLOGY IS DISRUPTIVE?   
 
Electric-powered vehicles have hovered at the fring e of legitimacy since the early 1900s, when they 
lost the contest for the dominant vehicle design to  gasoline power. Research on these vehicles 
accelerated during the 1970s, however, as policy ma kers increasingly looked to them as a way to 
reduce urban air pollution. The California Air Reso urces Board (CARB) forced an unprecedented 
infusion of resources into the effort in the early 1990s when it mandated that, starting in 1998, no 
automobile manufacturer would be allowed to sell any  cars in California if electric vehicles did not 
constitute at least 2 percent of its unit sales in the state. 1  
In my hypothetical responsibility for managing an a utomaker’s program, my first step would be to ask 
a series of questions: How much do we need to worry  about electric cars? That is, aside from 
California’s mandate, does the electric car pose a legitimate disruptive threat to companies making 
gasoline-powered automobiles? Does it constitute an  opportunity for profitable growth? 
To answer these questions, I would graph the trajec tories of performance improvement demanded in 
the market versus the performance improvement suppl ied by the technology; in other words, I would 
create for electric vehicles a trajectory map simil ar to those in Figures 1.7 or 9.5. Such charts are the 
best method I know for identifying disruptive techn ologies.  160  The first step in making this chart involves defini ng current mainstream market needs and comparing 
them with the current capacity of electric vehicles . To measure market needs, I would watch carefully 
what customers do,  not simply listen to what they say.  Watching how customers actually use a product 
provides much more reliable information than can be  gleaned from a verbal interview or a focus 
group. 2 Thus, observations indicate that auto users today require a minimum cruising range (that is, the 
distance that can be driven without refueling) of a bout 125 to 150 miles; most electric vehicles only 
offer a minimum cruising range of 50 to 80 miles. S imilarly, drivers seem to require cars that acceler ate 
from 0 to 60 miles per hour in less than 10 seconds  (necessary primarily to merge safely into high-
speed traffic from freeway entrance ramps); most el ectric vehicles take nearly 20 seconds to get there . 
And, finally, buyers in the mainstream market deman d a wide array of options, but it would be 
impossible for electric vehicle manufacturers to of fer a similar variety within the small initial unit  
volumes that will characterize that business. 3 According to almost any definition of functionalit y used 
for the vertical axis of our proposed chart, the el ectric vehicle will be deficient compared to a gaso line-
powered car. 
This information is not sufficient to characterize electric vehicles as disruptive, however. They will  
only be disruptive if we find that they are also on  a trajectory of improvement that might someday 
make them competitive in parts of the mainstream ma rket. To assess this possibility, we need to projec t 
trajectories measuring the performance improvement demanded in the market versus the performance 
improvement that electric vehicle technology may pr ovide. If these trajectories are parallel, then 
electric vehicles are unlikely to become factors in  the mainstream market; but if the technology will 
progress faster than the pace of improvement demand ed in the market, then the threat of disruption is 
real. 
Figure 10.1 shows that the trajectories of performa nce improvement demanded in the market—whether 
measured in terms of required acceleration, cruisin g range, or top cruising speed—are relatively flat.  
This is because traffic laws impose a limit on the usefulness of ever-more-powerful cars, and 
demographic, economic, and geographic consideration s limit the increase in commuting miles for the 
average driver to less than 1 percent per year. 4 At the same time, the performance of electric vehi cles is 
improving at a faster rate—between 2 and 4 percent per year—suggesting that sustaining technological 
advances might indeed carry electric vehicles from their position today, where they cannot compete in 
mainstream markets, to a position in the future whe re they might. 5 
In other words, as an automotive company executive,  I would worry about the electric vehicle, not just  
because it is politically correct to be investing i n environmentally friendly technologies, but becaus e 
electric vehicles have the smell of a disruptive te chnology. They can’t be used in mainstream markets;  
they offer a set of attributes that is orthogonal t o those that command attention in the gasoline-powe red 
value network; and the technology is moving ahead a t a faster rate than the market’s trajectory of nee d. 
Because electric vehicles are not sustaining innova tions, however, mainstream automakers naturally 
doubt that there is a market for them—another sympt om of a disruptive innovation. Consider this 
statement by the director of Ford’s electric vehicl e program: “The electric Ranger will sell at 
approximately $30,000 and have a lead-acid battery that will give it a range of 50 miles . . . . The 1 998 
electric vehicle will be a difficult sell. The prod ucts that will be available will not meet customer 
expectations in terms of range, cost or utility.” 6 Indeed, given their present performance along thes e 
parameters, it will be about as easy to sell electr ic vehicles into the mainstream car market as it wa s to 
sell 5.25-inch disk drives to mainframe computer ma kers in 1980. 
 
   161  Figure 10.1  The Electric Car  
 
 
 
Source: Data are from Dr. Paul J. Miller, Senior Energy Fel low, W. Alton Jones Foundation and from 
numerous articles about electric vehicles.  
 
  
In evaluating these trajectories, I would be carefu l to keep asking the right question: Will the traje ctory 
of electric vehicle performance ever intersect the trajectory of market  demands (as revealed in the way 
customers use  cars)? Industry experts may contend that electric vehicles will never perform as well as 
gasoline-powered cars, in effect comparing the traj ectories of the two technologies.  They are probably 
correct. But, recalling the experience of their cou nterparts in the disk drive industry, they will hav e the 
right answer to the wrong question. I also would no te, but not be deterred by, the mountain of expert 
opinion averring that without a major technological  breakthrough in battery technology, there will 
never be a substantial market for electric vehicles . The reason? If electric vehicles are viewed as a 
sustaining technology  for established market value networks, they are cl early right. But because the 
track records of experts predicting the nature and size of markets for disruptive technologies is very  
poor, I would be particularly skeptical of the expe rts’ skepticism, even as I remain uncertain about m y 
own conclusions. 
 
 
WHERE IS THE MARKET FOR ELECTRIC VEHICLES?    162   
Having decided that electric vehicles are a potenti ally disruptive technology, my next challenge would  
be to define a marketing strategy that could lead m y company to a legitimate, unsubsidized market in 
which electric cars might first be used. In formula ting this marketing strategy, I would apply three 
findings from earlier chapters in this book. 
First, I would acknowledge that, by definition, electric vehicles cannot initially be used in mains tream 
applications  because they do not satisfy the basic performance requirements of that market. I would 
therefore be sure that everybody  having anything  to do with my program understands this point: 
Although we don’t have a clue about where the marke t is, the one thing we know for certain is that it 
isn’t  in an established automobile market segment. Ironi cally, I would expect most automakers to focus 
precisely and myopically on the mainstream market b ecause of the principle of resource dependence 
and the principle that small markets don’t solve th e growth and profit needs of big companies. I would  
not, therefore, follow the lead of other automakers  in my search for customers, because I would 
recognize that their instincts and capabilities are  likely to be trained on the wrong target. 7 
Nonetheless, my task is to find a market in which t he vehicles can be used, because the early entrants  
into disruptive technology markets develop capabili ties that constitute strong advantages over later 
entrants. They’re the ones that, from a profitable business base in this beachhead market, will most 
successfully throw impetus behind the sustaining in novations required to move the disruptive 
technology upmarket, toward the mainstream. Holding  back from the market, waiting for laboratory 
researchers to develop a breakthrough battery techn ology, for example, is the path of least resistance  
for managers. But this strategy has rarely  proven to be a viable route to success with a disr uptive 
innovation. 
Historically, as we have seen, the very attributes that make disruptive technologies uncompetitive in 
mainstream markets actually count as positive  attributes in their emerging value network. In dis k 
drives, the smallness of 5.25-inch models made them  unusable in large computers but very useful on 
the desktop. While the small bucket capacity and sh ort reach of early hydraulic excavators made them 
useless in general excavation, their ability to dig  precise, narrow trenches made them useful in 
residential construction. Odd as it sounds, therefo re, I would direct my marketers to focus on 
uncovering somewhere  a group of buyers who have an undiscovered need fo r a vehicle that accelerates 
relatively slowly and can’t be driven farther than 100 miles! 
The second point on which I would base my marketing  approach is that no one can learn from market 
research what the early market(s) for electric vehi cles will be.  I can hire consultants, but the only thing 
I can know for sure is that their findings will be wrong. Nor can customers tell me whether or how the y 
might use electric vehicles, because they  will discover how they might use the products at t he same 
time as we  discover it—just as Honda’s Supercub opened an unf oreseen new application for 
motorbiking. The only useful information about the market will be what I create through expeditions 
into the market, through testing and probing, trial  and error, by selling real products to real people  who 
pay real money. 8 Government mandates, incidentally, are likely to d istort rather than solve the problem 
of finding a market. I would, therefore, force my o rganization to live by its wits rather than to rely  on 
capricious subsidies or non-economic–based Californ ia regulation to fuel my business. 
The third point is that my business plan must be a plan for learning,  not one for executing a 
preconceived strategy. Although I will do my best t o hit the right market with the right product and t he 
right strategy the first time out, there is a high probability that a better direction will emerge as the 
business heads toward its initial target. I must th erefore plan to be wrong and to learn what is right  as 
fast as possible. 9 I cannot spend all of my resources or all of my or ganizational credibility on an all-or- 163  nothing first-time bet, as Apple did with its Newto n or Hewlett-Packard did with its Kittyhawk. I need  
to conserve resources to get it right on the second  or third try. 
These three concepts would constitute the foundatio n of my marketing strategy. 
 
 
Potential Markets: Some Speculation   
 
What might emerge as the initial value network for electric vehicles? Again, though it is impossible t o 
predict, it almost surely will be one in which the weaknesses of the electric vehicle will be seen as 
strengths. One of my students has suggested that th e parents of high school students, who buy their 
children cars for basic transportation to and from school, friends’ homes, and school events, might 
constitute a fertile market for electric vehicles. 10  Given the option, these parents might see the prod uct 
simplicity, slow acceleration, and limited driving range of electric vehicles as very desirable  attributes 
for their teenagers’ cars—especially if they were s tyled with teenagers in mind. Given the right 
marketing approach, who knows what might happen? An  earlier generation met a lot of nice people on 
their Hondas. 
Another possible early market might be taxis or sma ll-parcel delivery vehicles destined for the 
growing, crowded, noisy, polluted cities of Southea st Asia. Vehicles can sit on Bangkok’s roads all 
day, mostly idling in traffic jams and never accele rating above 30 miles per hour. Electric motors 
would not need to run and hence would not drain the  battery while idling. The maneuverability and 
ease of parking of these small vehicles would be ad ditional attractions. 
These or similar market ideas, whether or not they ultimately prove viable, are at least consistent wi th 
the way disruptive technologies develop and emerge.  
 
 
How Are Today’s Automobile Companies Marketing Elec tric Vehicles?   
 
The strategy proposed here for finding and defining  the initial market for electric vehicles stands in  
stark contrast to the marketing approaches being us ed by today’s major automakers, each of which is 
struggling to sell electric vehicles into its mains tream market in the time-honored tradition of 
established firms mishandling disruptive technologi es. Consider this statement made in 1995 by 
William Glaub, Chrysler general sales manager, disc ussing his company’s planned offering for 1998. 11   
Chrysler Corporation is preparing to provide an ele ctric powered version of our slick new minivan in 
time for the 1998 model year. After an in-depth stu dy of the option between a purpose-built vehicle an d 
modification of an existing platform, the choice of  the minivan to use as an electric powered platform , 
in retrospect, is an obvious best choice for us. Ou r experience shows that fleets will likely be the b est 
opportunity to move any number of these vehicles . . . . The problem that we face is not  in creating an 
attractive package. The new minivan is an attractiv e package. The problem is that sufficient energy 
storage capacity is not available on board the vehi cle. 12  
To position its offering in the mainstream market, Chrysler has had to pack its minivan with 1,600 
pounds  of batteries. This, of course, makes its accelerat ion much slower, its driving range shorter, and  164  its braking distance longer than other available ga soline-powered automobiles. Because of the way 
Chrysler has positioned its electric vehicle, indus try analysts naturally compare it to gasoline-power ed 
minivans, using the metrics paramount in the mainst ream value network. At an estimated cost of 
$100,000 (compared with $22,000 for the gasoline-po wered model), nobody in their right mind would 
consider buying Chrysler’s product. 
Chrysler’s marketers are, naturally enough, very pe ssimistic about their ability to sell any  electric 
minivans in California, despite the government’s ma ndate that they do so. William Glaub, for example, 
continued the remarks cited above with the followin g observation: 
Markets are developed with fine products that custo mers desire to own. No salesman can take marginal 
product into the marketplace and have any hope of e stablishing a sustainable consumer base. 
Consumers will not be forced into a purchase that t hey do not want. Mandates will not work in a 
consumer-driven, free market economy. For electric vehicles to find a place in the market, respectable  
products comparable to today’s gasoline-powered car s must be available. 13  
Chrysler’s conclusion is absolutely correct, given the way its marketers have framed their challenge. 14  
Mainstream customers can never  use a disruptive technology at its outset. 
 
 
WHAT SHOULD BE OUR PRODUCT, TECHNOLOGY, AND DISTRIB UTION 
STRATEGIES?   
 
Product Development for Disruptive Innovations   
 
Guiding my engineers in designing our initial elect ric vehicle will be a challenge, because of the cla ssic 
chicken-and-egg problem: Without a market, there is  no obvious or reliable source of customer input; 
without a product that addresses customers’ needs, there can be no market. How can we design a 
product in such a vacuum? Fortunately, the principl es described in this book give us some help. 
The most valuable guidance comes from chapter 9 , which indicated that the basis of competition wil l 
change over a product’s life cycle and that the cyc le of evolution itself is driven by the phenomenon of 
performance oversupply, that is, the condition in w hich the performance provided by a technology 
exceeds the actual needs of the market. Historicall y, performance oversupply opens the door for 
simpler, less expensive, and more convenient—and al most always disruptive—technologies to enter. 
Performance oversupply indeed seems to have occurre d in autos. There are practical limits to the size 
of auto bodies and engines, to the value of going f rom 0 to 60 in fewer seconds, and to the consumer’s  
ability to cope with overchoice in available option s. Thus, we can safely predict that the basis of 
product competition and customer choice will shift away from these measures of functionality toward 
other attributes, such as reliability and convenien ce. This is borne out by the nature of the most 
successful entrants into the North American market during the past thirty years; they have succeeded 
not because they introduced products with superior functionality, but because they competed on the 
basis of reliability and convenience. 
Toyota, for example, entered the U.S. market with i ts simple, reliable Corona, establishing a low-end 
market position. Then, consistent with the inexorab le attraction to migrate upmarket, Toyota introduce d  165  models, such as Camry, Previa, and Lexus, with adde d features and functionality, creating a vacuum at 
the low end of the market into which entrants such as Saturn and Hyundai have entered. Saturn’s 
strategy has been to characterize the customer’s en tire experience of buying and owning the vehicle as  
reliable and convenient, but it, too, judging by re cent reports, 15  will soon take its turn moving 
upmarket, creating a new vacuum at the low end for even simpler, more convenient transportation. 
In all likelihood, therefore, the winning design in  the first stages of the electric vehicle race will  be 
characterized by simplicity and convenience and wil l be incubated in an emerging value network in 
which these attributes are important measures of va lue. Each of the disruptive technologies studied in  
this book has been smaller, simpler, and more conve nient than preceding products. Each was initially 
used in a new value network in which simplicity and  convenience were valued. This was true for 
smaller, simpler disk drives; desktop and portable computers; hydraulic backhoes; steel minimills as 
opposed to integrated mills; insulin-injecting pens  as opposed to syringes. 16  
Using these qualities as my guiding principles, I w ould instruct my design engineers to proceed 
according to the following three criteria. 
First, this vehicle must be simple, reliable, and convenient.  That probably means, for example, that 
figuring out a way to recharge its batteries quickl y, using the commonly available electrical service,  
would be an immutable technological objective. 
Second, because no one knows the ultimate market fo r the product or how it will ultimately be used, we  
must design a product platform in which feature, function, and styling changes can be made quickly 
and at low cost.  Assuming, for example, that the initial customers for electric vehicles will be parents 
who buy them for their teenaged children to drive t o and from school, friends’ homes, and activities, 
the first model would have features and styling app ropriate and appealing to teenagers. But, although 
we may target this market first, there’s a high pro bability that our initial concept will prove wrong.  So 
we’ve got to get the first models done fast and on a shoestring—leaving ample budget to get it right 
once feedback from the market starts coming in. 17  
Third, we must hit a low price point.  Disruptive technologies typically have a lower sticker  price per 
unit than products that are used in the mainstream,  even though their cost in use is often higher. Wha t 
enabled the use of disk drives in desktop computers  was not just their smaller size; it was their low unit 
price, which fit within the overall price points th at personal computer makers needed to hit. The pric e 
per megabyte  of the smaller disk drives was always higher than for the larger drives. Similarly, in 
excavators the price per excavator  was lower for the early hydraulic models than for the established 
cable-actuated ones, but their total cost per cubic  yard of earth moved per hour was much higher. 
Accordingly, our electric vehicle must have a lower  sticker price than the prevailing price for gasoli ne-
powered cars, even if the operating cost per mile d riven is higher. Customers have a long track record  
of paying price premiums for convenience. 
 
 
Technology Strategy for Disruptive Innovations   
 
Our technology plan cannot call for any technologic al breakthroughs on the path critical for the 
project’s success. Historically, disruptive technol ogies involve no new technologies; rather, they 
consist of components built around proven technolog ies and put together in a novel product 
architecture that offers the customer a set of attr ibutes never before available.  166  The major automakers engaged in electric vehicle de velopment today all maintain that a breakthrough 
in battery technology is absolutely essential befor e electric vehicles can be commercially viable. Joh n 
R. Wallace, of Ford, for example, has stated the fo llowing: 
The dilemma is that today’s batteries cannot satisf y these consumer needs. As anybody who is familiar 
with today’s battery technology will tell you, elec tric vehicles are not ready for prime time. All of the 
batteries expected to be available in 1998 fall sho rt of the 100-mile range [required by consumers]. T he 
only solution for the problems of range and cost is  improved battery technology. To ensure a 
commercially successful electric vehicle market, th e focus of our resources should be on the 
development of battery technology. Industry efforts  such as those through the U.S. Advanced Battery 
consortium, along with cooperative efforts among al l electric vehicle stakeholders—such as utilities, 
battery companies, environmentalists, regulators an d converters—are the most effective way to ensure 
the marketability of electric vehicles. 18  
 
William Glaub, of Chrysler, takes a similar positio n: “The advanced lead-acid batteries that will be 
used will provide less than the fuel storage equiva lent of two gallons of gasoline. This is like leavi ng 
home every day with the ‘low fuel’ light on. In oth er words, the battery technology is simply not 
ready.” 19   
The reason these companies view a breakthrough in b attery technology as the critical bottleneck to the  
commercial success of electric vehicles, of course,  is that their executives have positioned their min ds 
and their products in the mainstream market. For Ch rysler, this means an electric minivan; for Ford, a n 
electric Ranger. Given this position, they must del iver a sustaining technological impact from what is  
inherently a disruptive technology. They need a bre akthrough in battery technology because they made 
the choice to somehow position electric vehicles as  a sustaining technology. A battery breakthrough is  
not  likely to be required of companies whose executive s choose to harness or account for the basic laws 
of disruptive technology by creating a market in wh ich the weaknesses of the electric vehicle become 
its strengths. 
Where will advances in battery technology eventuall y come from? Looking at the historical record, we 
can assert the following. The companies that ultima tely achieve the advances in battery technology 
required to power cars for 150-mile cruises (if the y are ever developed) will be those that pioneer th e 
creation of a new value network using proven techno logy and then develop the sustaining technologies 
needed to carry them upward into more attractive ma rkets. 20  Our finding that well-managed companies 
are generally upwardly mobile and downwardly immobi le, therefore, suggests that the impetus to find 
the battery breakthrough will indeed be strongest a mong the disruptive innovators, which will have 
built a low-end market for electric vehicles before  trying to move upmarket toward the larger, more 
profitable mainstream. 
 
 
Distribution Strategy for Disruptive Innovations   
 
It has almost always been the case that disruptive products redefine the dominant distribution channel s, 
because dealers’ economics—their models for how to make money—are powerfully shaped by the 
mainstream value network, just as the manufacturer’ s are. Sony’s disruptive introduction of convenient  
and reliable portable transistorized radios and tel evisions shifted the dominant retail channel from 
appliance and department stores with expensive sale s support and field service networks (required for  167  sets built with vacuum tubes) to volume-oriented, l ow-overhead discount retailers. Honda’s disruptive 
motorbikes were rejected by mainstream motorcycle d ealers, forcing the company to create a new 
channel among sporting goods retailers. We saw, in fact, that a major reason why Harley-Davidson’s 
small-bike initiative failed is that its dealers re jected it: The image and economics of the small Ita lian 
bikes Harley had acquired did not fit its dealer ne twork. 
The reason disruptive technologies and new distribu tion channels frequently go hand-in-hand is, in 
fact, an economic one. Retailers and distributors t end to have very clear formulas for making money, a s 
the histories of Kresge and Woolworth in chapter 4 showed. Some make money by selling low volumes 
of big-ticket products at high margins; others make  money by selling large volumes at razor-thin 
margins that cover minimal operating overheads; sti ll others make their money servicing products 
already sold. Just as disruptive technologies don’t  fit the models of established firms for improving 
profits, they often don’t fit the models of their distributors,  either. 
My electric vehicle program would, therefore, have as a basic strategic premise the need to find or 
create new distribution channels for electric vehic les. Unless proven otherwise, I’d bet that mainstre am 
dealers of gasoline-powered automobiles would not v iew the sorts of disruptive electric vehicles we 
have in mind as critical to their success. 
 
 
WHAT ORGANIZATION BEST SERVES DISRUPTIVE INNOVATION S?   
 
After identifying the electric vehicle as a potenti ally disruptive technology; setting realistic beari ngs for 
finding its potential markets; and establishing str ategic parameters for the product’s design, technol ogy, 
and distribution network, as program manager I woul d next turn to organization. Creating an 
organizational context in which this effort can pro sper will be crucial, because rational resource 
allocation processes in established companies consi stently deny disruptive technologies the resources 
they need to survive, regardless of the commitment senior management may ostensibly have made to 
the program. 
 
 
Spinning Off an Independent Organization   
 
As we saw in the discussion of resource dependence in chapter 5, established firms that successfully 
built a strong market position in a disruptive tech nology were those that spun off from the mainstream  
company an independent, autonomously operated organ ization. Quantum, Control Data, IBM’s PC 
Division, Allen Bradley, and Hewlett-Packard’s desk -jet initiative all succeeded because they created 
organizations whose survival was predicated upon su ccessful commercialization of the disruptive 
technology: These firms embedded a dedicated organi zation squarely within the emerging value 
network. 
As program manager, therefore, I would strongly urg e corporate management to create an independent 
organization to commercialize electric vehicle tech nology, either an autonomous business unit, such as  
GM’s Saturn Division or the IBM PC Division, or an independent company whose stock is largely 
owned by the corporation. In an independent organiz ation, my best employees would be able to focus 
on electric vehicles without being repeatedly withd rawn from the project to solve pressing problems fo r  168  customers who pay the present bills. Demands from o ur own customers, on the other hand, would help 
us to focus on and lend impetus and excitement to o ur program. 
An independent organization would not only make res ource dependence work for us rather than against 
us, but it would also address the principle that sm all markets cannot solve the growth or profit probl ems 
of large companies. For many years into the future,  the market for electric vehicles will be so small that 
this business is unlikely to contribute significant ly to the top or bottom lines of a major automaker’ s 
income statement. Thus, since senior managers at th ese companies cannot be expected to focus either 
their priority attention or their priority resource s on electric vehicles, the most talented managers and 
engineers would be unlikely to want to be associate d with our project, which must inevitably be seen a s 
a financially insignificant effort: To secure their  own futures within the company, they naturally wil l 
want to work on mainstream programs, not peripheral  ones. 
In the early years of this new business, orders are  likely to be denominated in hundreds, not tens of 
thousands. If we are lucky enough to get a few wins , they almost surely will be small ones. In a small , 
independent organization, these small wins will gen erate energy and enthusiasm. In the mainstream, 
they would generate skepticism about whether we sho uld even be in the business. I want my 
organization’s customers  to answer the question of whether we should be in the business. I don’t want 
to spend my precious managerial energy constantly d efending our existence to efficiency analysts in 
the mainstream. 
Innovations are fraught with difficulties and uncer tainties. Because of this, I want always  to be sure that 
the projects that I manage are positioned directly on the path everyone believes the organization must  
take to achieve higher growth and greater profitabi lity. If my program is widely viewed as being on th at 
path, then I have confidence that when the inevitab le problems arise, somehow the organization will 
work with me to muster whatever it takes to solve t hem and succeed. If, on the other hand, my program 
is viewed by key people as nonessential to the orga nization’s growth and profitability, or even worse,  is 
viewed as an idea that might erode  profits, then even if the technology is simple, th e project will fail. 
I can address this challenge in one of two ways: I could convince everyone in the mainstream (in their  
heads and  their guts) that the disruptive technology is prof itable, or I could create an organization that 
is small enough, with an appropriate cost structure , that my program can be viewed as being on its 
critical path to success. The latter alternative is  a far more tractable management challenge. 
In a small, independent organization I will more li kely be able to create an appropriate attitude towa rd 
failure. Our initial stab into the market is not li kely to be successful. We will, therefore, need the  
flexibility to fail, but to fail on a small  scale, so that we can try again without having des troyed our 
credibility. Again, there are two ways to create th e proper tolerance toward failure: change the value s 
and culture of the mainstream organization or creat e a new organization. The problem with asking the 
mainstream organization to be more tolerant of risk -taking and failure is that, in general, we don’t want  
to tolerate marketing failure when, as is most ofte n the case, we are investing in sustaining technolo gy 
change. The mainstream organization is involved in taking sustaining technological innovations into 
existing markets populated by known customers with researchable needs. Getting it wrong the first 
time is not an intrinsic part of these processes: S uch innovations are amenable to careful planning an d 
coordinated execution. 
Finally, I don’t want my organization to have pocke ts that are too deep. While I don’t want my people 
to feel pressure to generate significant profit for  the mainstream company (this would force us into a  
fruitless search for an instant large market), I wa nt them to feel constant  pressure to find some way—
some  set of customers somewhere —to make our small organization cash-positive as fa st as possible.  169  We need a strong motivation to accelerate through t he trials and errors inherent in cultivating a new 
market. 
Of course, the danger in making this unequivocal ca ll for spinning out an independent company is that 
some managers might apply this remedy indiscriminat ely, viewing skunkworks and spinoffs as a 
blanket solution—an industrial-strength aspirin tha t cures all sorts of problems. In reality, spinning  out 
is an appropriate step only when confronting disrup tive innovation. The evidence is very  strong that 
large, mainstream organizations can be extremely cr eative in developing and implementing sustaining  
innovations. 21  In other words, the degree of disruptiveness inher ent in an innovation provides a fairly 
clear indication of when a mainstream organization might be capable of succeeding with it and when it 
might be expected to fail. 
In this context, the electric vehicle is not only a  disruptive innovation, but it involves massive 
architectural reconfiguration as well, a reconfigur ation that must occur not only within the product 
itself but across the entire value chain. From proc urement through distribution, functional groups wil l 
have to interface differently than they have ever b efore. Hence, my project would need to be managed 
as a heavyweight team in an organization independen t of the mainstream company. This organizational 
structure cannot guarantee the success of our elect ric vehicle program, but it would at least allow my  
team to work in an environment that accounts for, r ather than fights, the principles of disruptive 
innovation. 
 
 
NOTES   
 
1.  In 1996, the state government delayed implementati on of this requirement until the year 2002, in 
response to motor vehicle manufacturers’ protests t hat, given the performance and cost of the vehicles  
they had been able to design, there was no demand f or electric vehicles.  
2.  An excellent study on this subject is summarized i n Dorothy Leonard-Barton, Wellsprings of 
Knowledge  (Boston: Harvard Business School Press, 1995).  
3.  This information was taken from an October 1994 su rvey conducted by The Dohring Company and 
quoted by the Toyota Motor Sales Company at the CAR B (California Air Resources Board) Workshop 
on Electric Vehicle Consumer Marketability held in El Monte, California, on June 28, 1995.  
4.  This information was provided by Dr. Paul J. Mille r, Senior Energy Fellow, W. Alton Jones 
Foundation, Inc., Charlottesville, Virginia. It was  augmented with information from the following 
sources: Frank Keith, Paul Norton, and Dana Sue Pot estio, Electric Vehicles: Promise and Reality  
(California State Legislative Report [19], No. 10, July, 1994); W. P. Egan, Electric Cars  (Canberra, 
Australia: Bureau of Transport Economics, 1974); Da niel Sperling, Future Drive: Electric Vehicles and 
Sustainable Transportation  (Washington, D.C.: Island Press, 1995); and Willia m Hamilton, Electric 
Automobiles  (New York: McGraw Hill Company, 1980).  
5.  Based on the graphs in Figure 10.1, it will take a  long time for disruptive electric vehicle technolo gy 
to become competitive in mainstream markets if futu re rates of improvement resemble those of the 
past. The historical rate of performance improvemen t is, of course, no guarantee that the future rate can 
be maintained. Technologists very well might run in to insurmountable technological barriers. What we 
can  say for sure, however, is that the incentive of di sruptive technologists to find some way to engineer  
around such barriers will be just as strong as the disincentive that established car makers will feel to 
move down-market. If present rates of improvement c ontinue, however, we would expect the cruising 
range of electric cars, for example, to intersect w ith the average range demanded in the mainstream 
market by 2015, and electric vehicle acceleration t o intersect with mainstream demands by 2020.  170  Clearly, as will be discussed below, it will be cru cial for electric vehicle innovators to find market s that 
value the attributes of the technology as it curren tly is capable, rather than waiting until the techn ology 
improves to the point that it can be used in the ma instream market.  
6.  This statement was made by John R. Wallace, Direct or of Electric Vehicle Programs, Ford Motor 
Company, at the CARB Workshop on Electric Vehicle C onsumer Marketability held at El Monte, 
California, on June 28, 1995.  
7.  It is remarkable how instinctively and consistentl y good companies try to force innovations toward 
their existing base of customers, regardless of whe ther they are sustaining or disruptive in character . 
We have seen this several times in this book: for e xample, in mechanical excavators, where Bucyrus 
Erie tried with its “Hydrohoe” to make hydraulic ex cavation technology work for mainstream 
excavation contractors; in motorcycles, where Harle y-Davidson tried to launch low-end brand name 
bikes through its dealer network; and in the electr ic vehicle case described here, in which Chrysler 
packed nearly a ton of batteries into a minivan. Ch arles Ferguson and Charles Morris, in their book 
Computer Wars,  recount a similar story about IBM’s efforts to com mercialize Reduced Instruction Set 
Computing (RISC) microprocessor technology. RISC wa s invented at IBM, and its inventors built 
computers with RISC chips that were “screamingly fa st.” IBM subsequently spent massive amounts of 
time, money, and manpower trying to make the RISC c hip work in its main line of minicomputers. This 
required so many design compromises, however, that the program was never successful. Several key 
members of IBM’s RISC team left in frustration, sub sequently playing key roles in establishing the 
RISC chipmaker MIPS and Hewlett-Packard’s RISC chip  business. These efforts were successful 
because, having accepted the attributes of the prod uct for what they were, they found a market, in 
engineering workstations, that valued those attribu tes. IBM failed because it tried to force the 
technology into a market it had already found. Inte restingly, IBM ultimately built a successful busine ss 
around a RISC-architecture chip when it launched it s own engineering workstation. See Charles 
Ferguson and Charles Morris, Computer Wars  (New York: Time Books, 1994).  
8.  The notion that non-existent markets are best rese arched through action, rather than through passive 
observation, is explored in Gary Hamel and C. K. Pr ahalad, “Corporate Imagination and Expeditionary 
Marketing,” Harvard Business Review,  July–August, 1991, 81–92.  
9.  The concept that business plans dealing with disru ptive innovations should be plans for learning 
rather than plans for executing a preconceived stra tegy is taught clearly by Rita G. McGrath and Ian 
MacMillan in “Discovery-Driven Planning,” Harvard Business Review, July–August, 1995, 44–54.  
10.  Jeffrey Thoresen Severts, “Managing Innovation: El ectric Vehicle Development at Chrysler,” 
Harvard Business School MBA student paper, 1996. A copy of this paper is available on request from 
Clayton Christensen, Harvard Business School.  
11.  Glaub’s remarks were made in the context of the Ca lifornia Air Resources Board mandate that by 
1998 all companies selling gasoline-powered vehicle s in the state must, in order to sell any cars at a ll, 
sell enough electric-powered vehicles to constitute  2 percent of their total vehicle unit sales in the  state. 
As already noted, the state government, in 1996, de layed implementation of that requirement until 
2002.  
12.  This statement was made by William Glaub, General Sales Manager, Field Sales Operations, 
Chrysler Corporation, at the CARB Workshop on Elect ric Vehicle Consumer Marketability held in El 
Monte, California, on June 28, 1995; see p. 5 of th e company’s press release about the workshop.  
13.  Ibid.  
14.  It is important to note that these statistics for Chrysler’s offering were determined by Chrysler’s 
efforts to commercialize the disruptive technology;  they are not intrinsic to electrically powered 
vehicles per se.  Electric vehicles designed for different, lighter- duty applications, such as one by 
General Motors, have driving ranges of up to 100 mi les. (See Jeffrey Thoresen Severts, “Managing 
Innovation: Electric Vehicle Development at Chrysle r,” Harvard Business School student paper, 1996.)  
15.  See, for example, Gabriella Stern and Rebecca Blum enstein, “GM Is Expected to Back Proposal for 
Midsize Version of Saturn Car,” The Wall Street Journal,  May 24, 1996, B4.   171  16.  This list of smaller, simpler, more convenient dis ruptive technologies could be extended to include 
a host of others whose histories could not be squee zed into this book: tabletop photocopiers; surgical  
staplers; portable, transistorized radios and telev isions; helican scan VCRs; microwave ovens; bubble 
jet printers. Each of these disruptive technologies  has grown to dominate both its initial and its 
mainstream markets, having begun with simplicity an d convenience as their primary value 
propositions.  
17.  The notion that it takes time, experimentation, an d trial and error to achieve a dominant product 
design, a very common pattern with disruptive techn ologies, is discussed later in this chapter.  
18.  This statement was made by John R. Wallace, of For d, at the CARB Workshop on Electric Vehicle 
Consumer Marketability held in El Monte, California , on June 28, 1995; see p. 5 of the company’s 
press release.  
19.  Glaub, statement made at the CARB Workshop.  
20.  Two excellent articles in which the relative roles  of product development and incremental versus 
radical technology development are researched and d iscussed are Ralph E. Gomory, “From the ‘Ladder 
of Science’ to the Product Development Cycle,” Harvard Business Review,  November-December, 
1989, 99–105, and Lowell Steele, “Managers’ Misconc eptions About Technology,” Harvard Business 
Review,  1983, 733–740.  
21.  In addition to the findings from the disk drive st udy summarized in chapters 1 and 2 that 
established firms were able to muster the wherewith al to lead in extraordinarily complex and risky 
sustaining innovations, there is similar evidence f rom other industries; see, for example, Marco Iansi ti, 
“Technology Integration: Managing Technological Evo lution in a Complex Environment,” Research 
Policy  24, 1995, 521–542. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  172    
CHAPTER ELEVEN  
The Dilemmas of Innovation:  
A Summary 
 
 
One of the most gratifying outcomes of the research  reported in this book is the finding that managing  
better, working harder, and not making so many dumb  mistakes is not the answer to the innovator’s 
dilemma. This discovery is gratifying because I hav e never met a group of people who are smarter or 
work harder or are as right so often as the manager s I know. If finding better people than these were the 
answer to the problems posed by disruptive technolo gies, the dilemma would indeed be intractable.  
We have learned in this book that in their straight forward search for profit and growth, some very 
capable executives in some extraordinarily successf ul companies, using the best managerial techniques,  
have led their firms toward failure. Yet companies must not throw out the capabilities, organizational  
structures, and decision-making processes that have  made them successful in their mainstream markets 
just because they don’t work in the face of disrupt ive technological change. The vast majority of the 
innovation challenges they will face are sustaining  in character, and these are just the sorts of 
innovations that these capabilities are designed to  tackle. Managers of these companies simply need to  
recognize that these capabilities, cultures, and pr actices are valuable only in certain conditions. 
I have found that many of life’s most useful insigh ts are often quite simple. In retrospect, many of t he 
findings of this book fit that mold: Initially they  seemed somewhat counterintuitive, but as I came to  
understand them, the insights were revealed as simp le and sensible. I review them here, in the hope th at 
they will prove useful to those readers who may be wrestling with the innovator’s dilemmas. 
First,  the pace of progress that markets demand or can ab sorb may be different from the progress 
offered by technology. This means that products tha t do not appear to be useful to our customers today  
(that is, disruptive technologies) may squarely add ress their needs tomorrow. Recognizing this 
possibility, we cannot expect our customers to lead  us toward innovations that they do not now need. 
Therefore, while keeping close to our customers is an important management paradigm for handling 
sustaining innovations, it may provide misleading d ata for handling disruptive ones. Trajectory maps 
can help to analyze conditions and to reveal which situation a company faces. 
Second,  managing innovation mirrors the resource allocatio n process: Innovation proposals that get the 
funding and manpower they require may succeed; thos e given lower priority, whether formally or de 
facto, will starve for lack of resources and have l ittle chance of success. One major reason for the 
difficulty of managing innovation is the complexity  of managing the resource allocation process. A 
company’s executives may seem to make resource allo cation decisions, but the implementation of 
those decisions is in the hands of a staff whose wi sdom and intuition have been forged in the 
company’s mainstream value network: They understand  what the company should do to improve 
profitability. Keeping a company successful require s that employees continue to hone and exercise that  
wisdom and intuition. This means, however, that unt il other alternatives that appear to be financially  
more attractive have disappeared or been eliminated , managers will find it extraordinarily difficult t o 
keep resources focused on the pursuit of a disrupti ve technology.  173  Third,  just as there is a resource allocation side to eve ry innovation problem, matching the market to 
the technology is another. Successful companies hav e a practiced capability in taking sustaining 
technologies to market, routinely giving their cust omers more and better versions of what they say the y 
want. This is a valued capability for handling sust aining innovation, but it will not serve the purpos e 
when handling disruptive technologies. If, as most successful companies try to do, a company stretches  
or forces a disruptive technology to fit the needs of current, mainstream customers—as we saw happen 
in the disk drive, excavator, and electric vehicle industries—it is almost sure to fail. Historically,  the 
more successful approach has been to find a new mar ket that values the current characteristics of the 
disruptive technology. Disruptive technology should  be framed as a marketing challenge, not a 
technological one. 
Fourth,  the capabilities of most organizations are far mor e specialized and context-specific than most 
managers are inclined to believe. This is because c apabilities are forged within value networks. Hence , 
organizations have capabilities to take certain new  technologies into certain markets. They have 
disabilities in taking technology to market in othe r ways. Organizations have the capability to tolera te 
failure along some dimensions, and an incapacity to  tolerate other types of failure. They have the 
capability to make money when gross margins are at one level, and an inability to make money when 
margins are at another. They may have the capabilit y to manufacture profitably at particular ranges of  
volume and order size, and be unable to make money with different volumes or sizes of customers. 
Typically, their product development cycle times an d the steepness of the ramp to production that they  
can negotiate are set in the context of their value  network. 
All of these capabilities—of organizations and of i ndividuals—are defined and refined by the types of 
problems tackled in the past, the nature of which h as also been shaped by the characteristics of the 
value networks in which the organizations and indiv iduals have historically competed. Very often, the 
new markets enabled by disruptive technologies requ ire very different capabilities along each of these  
dimensions. 
Fifth,  in many instances, the information required to mak e large and decisive investments in the face of 
disruptive technology simply does not exist. It nee ds to be created through fast, inexpensive, and 
flexible forays into the market and the product. Th e risk is very high that any particular idea about the 
product attributes or market applications of a disr uptive technology may not prove to be viable. Failu re 
and interative learning are, therefore, intrinsic t o the search for success with a disruptive technolo gy. 
Successful organizations, which ought not and canno t tolerate failure in sustaining innovations, find it 
difficult simultaneously to tolerate failure in dis ruptive ones. 
Although the mortality rate for ideas about disrupt ive technologies is high, the overall business of 
creating new markets for disruptive technologies ne ed not be inordinately risky. Managers who don’t 
bet the farm on their first idea, who leave room to  try, fail, learn quickly, and try again, can succe ed at 
developing the understanding of customers, markets,  and technology needed to commercialize 
disruptive innovations. 
Sixth,  it is not wise to adopt a blanket technology strat egy to be always a leader or always a follower. 
Companies need to take distinctly different posture s depending on whether they are addressing a 
disruptive or a sustaining technology. Disruptive i nnovations entail significant first-mover advantage s: 
Leadership is important. Sustaining situations, how ever, very often do not. The evidence is quite stro ng 
that companies whose strategy is to extend the perf ormance of conventional technologies through 
consistent incremental improvements do about as wel l as companies whose strategy is to take big, 
industry-leading technological leaps.  174  Seventh,  and last, the research summarized in this book sug gests that there are powerful barriers to 
entry and mobility that differ significantly from t he types defined and historically focused on by 
economists. Economists have extensively described b arriers to entry and mobility and how they work. 
A characteristic of almost all of these formulation s, however, is that they relate to things,  such as assets 
or resources, that are difficult to obtain or repli cate. 1 Perhaps the most powerful protection that small 
entrant firms enjoy as they build the emerging mark ets for disruptive technologies is that they are do ing 
something that it simply does not make sense for th e established leaders to do. Despite their 
endowments in technology, brand names, manufacturin g prowess, management experience, distribution 
muscle, and just plain cash, successful companies p opulated by good managers have a genuinely hard 
time doing what does not fit their model for how to  make money. Because disruptive technologies 
rarely make sense during the years when investing i n them is most important, conventional managerial 
wisdom at established firms constitutes an entry an d mobility barrier that entrepreneurs and investors  
can bank on. It is powerful and pervasive. 
Established companies can  surmount this barrier, however. The dilemmas posed  to innovators by the 
conflicting demands of sustaining and disruptive te chnologies can be resolved. Managers must first 
understand what these intrinsic conflicts are. They  then need to create a context in which each 
organization’s market position, economic structure,  developmental capabilities, and values are 
sufficiently aligned with the power of their custom ers that they assist, rather than impede, the very 
different work of sustaining and disruptive innovat ors. I hope this book helps them in this effort. 
 
 
NOTES   
 
1.  By things  I mean barriers such as proprietary technology; ow nership of expensive manufacturing 
plants with large minimum efficient manufacturing s cales; pre-emption of the most powerful 
distributors in major markets; exclusive control of  key raw materials or unique human resources; the 
credibility and reputation that comes from strong b rand names; cumulative production experience 
and/or the presence of steep economies of scale; an d so on. The seminal work on entry barriers from an  
economist’s perspective is Joseph Bain, Barriers to New Competition  (Cambridge, MA: Harvard 
University Press, 1956); see also Richard Caves and  Michael Porter, “From Entry Barriers to Mobility 
Barriers,” Quarterly Journal of Economics  (91), May, 1977, 241–261. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  175   
The Innovator’s Dilemma   
Book Group Guide  
 
 
The summary and questions in this guide are designe d to stimulate thinking and discussion about The 
Innovator’s Dilemma, how its findings are manifest in many industries to day, and the implications of 
those findings for the future. 
 
Thesis of the Book   
 
In The Innovator’s Dilemma, Professor Clayton Christensen asks the question: Wh y do well-managed 
companies fail? He concludes that they often fail b ecause the very management practices that have 
allowed them to become industry leaders also make i t extremely difficult for them to develop the 
disruptive technologies that ultimately steal away their markets. 
Well-managed companies are excellent at developing the sustaining technologies that improve the 
performance of their products in the ways that matt er to their customers. This is because their 
management practices are biased toward: 
Listening to customers  
 
Investing aggressively in technologies that give th ose customers what they say they want  
 
Seeking higher margins  
 
Targeting larger markets rather than smaller ones  
Disruptive technologies, however, are distinctly di fferent from sustaining technologies. Disruptive 
technologies change the value proposition in a mark et. When they first appear, they almost always 
offer lower performance in terms of the attributes that mainstream customers care about. In computer 
disk drives, for example, disruptive technologies h ave always had less capacity than the old 
technologies. But disruptive technologies have othe r attributes that a few fringe (generally new) 
customers value. They are typically cheaper, smalle r, simpler, and frequently more convenient to use. 
Therefore, they open new markets. Further, because with experience and sufficient investment, the 
developers of disruptive technologies will always i mprove their products’ performance, they eventually  
are able to take over the older markets. This is be cause they are able to deliver sufficient performan ce 
on the old attributers, and they add some new ones.  
The Innovator’s Dilemma describes both the processes through which disrupti ve technologies supplant 
older technologies and the powerful forces within w ell-managed companies that make them unlikely to  176  develop those technologies themselves. Professor Ch ristensen offers a framework of four Principles of 
Disruptive Technology to explain why the management  practices that are the most productive for 
exploiting existing technologies are antiproductive  when it comes to developing disruptive ones. And, 
finally, he suggests ways that managers can harness  these principles so that their companies can 
become more effective at developing for themselves the new technologies that are going to capture 
their markets in the future. 
 
 
Principles of Disruptive Technology   
1. Companies Depend on Customers and Investors for Resources  
In order to survive, companies must provide custome rs and investors with the products, 
services, and profits that they require. The highes t performing companies, therefore, have well-
developed systems for killing ideas that their cust omers don’t want. As a result, these 
companies find it very difficult to invest adequate  resources in disruptive technologies—lower-
margin opportunities that their customers don’t wan t—until their customers want them. And by 
then, it is too late.  
 
2. Small Markets Don’t Solve the Growth Needs of La rge Companies  
To maintain their share prices and create internal opportunities for their employees, successful 
companies need to grow. It isn’t necessary that the y increase their growth rates, but they must 
maintain them. And as they get larger, they need in creasing amounts of new revenue just to 
maintain the same growth rate. Therefore, it become s progressively more difficult for them to 
enter the newer, smaller markets that are destined to become the large markets of the future. To 
maintain their growth rates, they must focus on lar ge markets.  
 
3. Markets That Don’t Exist Can’t Be Analyzed  
Sound market research and good planning followed by  execution according to plan are the 
hallmarks of good management. But companies whose i nvestment processes demand 
quantification of market size and financial returns  before they can enter a market get paralyzed 
when faced with disruptive technologies because the y demand data on markets that don’t yet 
exist.  
 
4. Technology Supply May Not Equal Market Demand  
Although disruptive technologies can initially be u sed only in small markets, they eventually 
become competitive in mainstream markets. This is b ecause the pace of technological progress 
often exceeds the rate of improvement that mainstre am customers want or can absorb. As a 
result, the products that are currently in the main stream eventually will overshoot the 
performance that mainstream markets demand, while t he disruptive technologies that 
underperform relative to customer expectations in t he mainstream market today may become 
directly competitive tomorrow. Once two or more pro ducts are offering adequate performance, 
customers will find other criteria for choosing. Th ese criteria tend to move toward reliability, 
convenience, and price, all of which are areas in w hich the newer technologies often have 
advantages.  
A big mistake that managers make in dealing with ne w technologies is that they try to fight or 
overcome the Principles of Disruptive Technology. A pplying the traditional management practices that  177  lead to success with sustaining technologies always  leads to failure with disruptive technologies, say s 
Professor Christensen. The more productive route, w hich often leads to success, he says, is to 
understand the natural laws that apply to disruptiv e technologies and to use them to create new market s 
and new products. Only by recognizing the dynamics of how disruptive technologies develop can 
managers respond effectively to the opportunities t hat they present. 
Specifically, he advises managers faced with disrup tive technologies to:  
1. Give responsibility for disruptive technologies to organizations whose customers need them 
so that resources will flow to them.  
 
2. Set up a separate organization small enough to g et excited by small gains.  
 
3. Plan for failure. Don’t bet all your resources o n being right the first time. Think of your 
initial efforts at commercializing a disruptive tec hnology as learning opportunities. Make 
revisions as you gather data.  
 
4. Don't count on breakthroughs. Move ahead early a nd find the market for the current attributes 
of the technology. You will find it outside the cur rent mainstream market. You will also find 
that the attributes that make disruptive technologi es unattractive to mainstream markets are the 
attributes on which the new markets will be built.  
 
Questions for Discussion   
1. The characteristics of a disruptive technology a re:  
 
They are simpler and cheaper and lower performing.  
 
They generally promise lower margins, not higher pr ofits.  
 
Leading firms’ most profitable customers generally can’t use and don’t want them.  
 
They are first commercialized in emerging or insign ificant markets.   
 
The Innovator’s Dilemma 
discusses disruptive innovations in the disk-drive,  excavator, steel, and auto industries. Looking 
back through history, can you identify some disrupt ive technologies that eventually replaced 
older products and industries? Can you think of oth ers that are emerging today, maybe even 
ones that could threaten your business?   178   
2. There is a tendency in all markets for companies  to move upmarket toward more complicated 
products with higher prices. Why is it difficult fo r companies to enter markets for simpler, 
cheaper products? Can you think of companies that h ave upscaled themselves out of business? 
How might they have avoided that?  
 
3. The same tendency for companies to move upmarket  that can be fatal for established 
companies also accounts for the eventual developmen t of emerging markets into mainstream 
markets. Besides the examples in the book, can you think of companies that have upscaled 
themselves to success?  
 
4. In attempting to commercialize a disruptive tech nology, why is it important to begin 
investing on the assumption that your expectations will be wrong? Besides the motorcycle, 
excavator, and disk-drive examples in the book, can  you think of other examples in which a 
company began marketing a product for one applicati on but the big market turned out to be for 
another application?  
 
5. One of the hallmarks of disruptive technologies is that initially they underperform the current 
technology on the attributes that matter most to ma instream customers. The companies that 
succeed in commercializing them, therefore, must fi nd different customers for whom the new 
technology’s attributes are most valuable. Can you think of any markets that are emerging today 
based on attributes or qualities that seemed unimpo rtant to the mainstream markets when they 
were introduced? What older, mainstream products or  companies are threatened?  
 
6. When two or more products meet the minimum speci fications for the functionality of a 
product, customers begin to look for other deciding  factors. According to a Windermere 
Associates study cited in the book, the progression  usually is from functionality to reliability to 
convenience to price. What are some current markets  that have recently moved one or more 
steps along this progression?  
 
7. Most people think that senior executives make th e important decisions about where a 
company will go and how it will invest its resource s, but the real power lies with the people 
deeper in the organization who decide which proposa ls will be presented to senior management. 
What are the corporate factors that lead midlevel e mployees to ignore or kill disruptive 
technologies? Should well-managed companies change these practices and policies?  
 
8. What are the personal career considerations that  lead ambitious employees in large 
corporations to ignore or kill disruptive technolog ies? Should well-managed companies change 
the policies that encourage employees to think this  way?  
 
9. What do the findings in this book suggest about how companies will be organized in the 
future? Should large organizations with structures created around functionalities redesign 
themselves into interconnected teams, as some manag ement theorists currently believe? Or,  179  recognizing that different technologies and differe nt markets have differing needs, should they 
try to have distinct organizational structures and management practices for different 
circumstances? Is this realistically possible?  
 
10. The CEO of a disk-drive maker is quoted in chap ter 4 as saying that “We got way ahead of 
the market” in explaining why his company failed to  commercialize a 1.8-inch disk drive that it 
had developed. At the time, however, there was a bu rgeoning market for 1.8-inch drives among 
new users that his company hadn't discovered. Profe ssor Christensen argues that “disruptive 
technology should be framed as a marketing challeng e, not a technological one.” Do you think 
there is a market somewhere for all technologies? I f not, how would you as a manager go about 
figuring out which technologies to shelve and which  ones to pursue aggressively?  
 
11. Similarly, Professor Christensen argues that co mpanies should not wait for new 
breakthroughs to improve a technology’s performance . Instead, they need to find customers 
who value the very attributes that others consider to be shortcomings. As a manager, how do 
you decide when a technology—or idea—needs more dev elopment and when it's time to 
aggressively put it on the market?  
 
12. The primary thesis of The Innovator’s Dilemma is that the management practices that allow 
companies to be leaders in mainstream markets are t he same practices that cause them to miss 
the opportunities offered by disruptive technologie s. In other words, well-managed companies 
fail because  they are well managed. Do you think that the defin ition of what constitutes “good 
management” is changing? In the future, will listen ing to customers, investing aggressively in 
producing what those customers say they want, and c arefully analyzing markets become “bad 
management”? What kind of system might combine the best of both worlds? 
  
About the Author 
 
Clayton M. Christensen  is an associate professor of business administratio n at the Harvard Business 
School, where he holds a joint appointment with the  Technology and Operations Management and 
General Management faculty groups. His research and  writing focus on the management of 
technological innovation, the problems in finding n ew markets for new technologies, and the 
identification and development of organizational ca pabilities. Prior to joining the Harvard Business 
School faculty, Professor Christensen served as cha irman and president of Ceramics Process Systems 
Corporation, a firm he co-founded in 1984 with seve ral MIT professors. He also served as a White 
House Fellow and as a member of the staff of the Bo ston Consulting Group. He is the author or co-
author of numerous articles in journals such as Research Policy, Strategic Management Journal, 
Industrial and Corporate Change,  the Business History Review,  and the Harvard Business Review.  
 