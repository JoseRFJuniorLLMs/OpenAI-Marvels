{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using OpenAI LLMs (GPT 3.5/GPT4)\n",
    "\n",
    "## Access this code on Github - https://github.com/abhinav-kimothi/OpenAI-Marvels\n",
    "\n",
    "<img src=\"../Assets/Images/LLM LS.png\" width=1000>\n",
    "\n",
    "---\n",
    "\n",
    "- Installation & Importing Libraries\n",
    "- Accessing OpenAI (Keys) & Setting up Client\n",
    "\n",
    "---\n",
    "- Chat Completion API (Basic Text Generation)\n",
    "- Tokens\n",
    "- Embeddings\n",
    "- RAG\n",
    "- Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Time (75 minutes coding + 15 minutes QnA) \n",
    "- We'll also take questions at the end of each section\n",
    "- Aim is to explore OpenAI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kim/Desktop/Github/OpenAI-Marvels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai #OpenAI python library\n",
    "from openai import OpenAI #OpenAI Client\n",
    "from configparser import ConfigParser #library to read the config file\n",
    "\n",
    "import tiktoken #library to count tokens\n",
    "\n",
    "import gradio as gr #library for gradio interface\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity #for calculating similarities between embeddings\n",
    "\n",
    "from bs4 import BeautifulSoup #for extracting text from webpages\n",
    "import requests\n",
    "\n",
    "import PyPDF2 #for reading text from pdf \n",
    "\n",
    "from langchain.document_loaders import TextLoader #to load text for RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #to chunk data for RAG\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings #to crate embeddings for RAG\n",
    "from langchain.vectorstores import FAISS # to store embeddings in a vector index\n",
    "from sklearn.model_selection import train_test_split #for model finetuning\n",
    "\n",
    "import json #for creating finetuning files\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans # for clustering of text using embeddings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing OpenAI (Keys) & Setting up Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__ : Have you procured your OpenAI API key?\n",
    "\n",
    "If not,  create one here - https://platform.openai.com/api-keys\n",
    "\n",
    "Also, check if you have sufficient balance - https://platform.openai.com/account/billing/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__ : Read and set the OpenAI API key in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_object = ConfigParser()\n",
    "config_object.read(\"../config.ini\")\n",
    "openai.api_key = config_object[\"OPENAI\"][\"openai_key\"] #read the api key from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key=\"<Your API Key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of storing the API key in the environment. You may choose as per your preference or your organisation's policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: Initialize the OpenAI client. This serves as an interface to interact with OpenAI's services and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready! Let's try and make the first call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion API (Basic Text Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of applications</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px;\"><i>\"LLMs can be, generally, thought of as a next word prediction model\"</i></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>What is an LLM?</b></span>\n",
    "\n",
    "- LLMs are __machine learning models__ that have learned from __massive datasets__ of human-generated content, finding statistical patterns to replicate human-like abilities.\n",
    "\n",
    "- __Foundation models__, also known as base models, have been trained on trillions of words for weeks or months using extensive compute power. These models have __billions of parameters__, which represent their memory and enable sophisticated tasks.\n",
    "\n",
    "- __Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code syntax, you provide natural language prompts to the models__.\n",
    "\n",
    "- When you pass a __prompt__ to the model, it predicts the next words and generates a __completion__. This process is known as __inference__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>Prompts, Completions and Inference!</b></span>\n",
    "\n",
    "<img src=\"../Assets/Images/LLM Inference.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>Available OpenAI models</b></span>\n",
    "\n",
    "---\n",
    "\n",
    "__GPT 4__\n",
    "\n",
    "__<u>(Production)</u>__\n",
    "\n",
    "<u>Name        | Context Window    | Cut-off date      | Snapshot</u>\n",
    "\n",
    "__gpt-4__       | 8,192 tokens      | Up to Sep 2021    | gpt-4-0613\n",
    "\n",
    "__gpt-4-32k__   | 32,768 tokens     | Up to Sep 2021    | gpt-4-32k-0613\n",
    "\n",
    "<b><u>(Preview)</b></u>\n",
    "\n",
    "__gpt-4-turbo-preview__     | 128,000 tokens | Up to Dec 2023    | gpt-4-1106-preview\n",
    "\n",
    "__gpt-4-vision-preview__    | 128,000 tokens | Up to Apr 2023    | gpt-4-1106-vision-preview\n",
    "\n",
    "---\n",
    "\n",
    "__GPT 3.5__\n",
    "\n",
    "__gpt-3.5-turbo__ | 16,385 tokens | Up to Sep 2021 | gpt-3.5-turbo-1106\n",
    "\n",
    "__gpt-3.5-turbo-instruct__ | 4,096 tokens | Up to Sep 2021\n",
    "\n",
    "---\n",
    "\n",
    "For more details, visit the official documentation -> https://platform.openai.com/docs/models\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"model\"__ is passed as a parameter in the chat completions API</span>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>OpenAI Chat Messages (Prompt) Structure</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px;\"><u>Role</u></span> : OpenAI allows for *three* roles/personas - \n",
    "1. __System__ : The overarching constraints/definitions/intructions of the system that the LLM should \"remember\"\n",
    "2. __User__ : Any instruction a user wants to pass to the LLM\n",
    "3. __Assistant__ : The response from the LLM\n",
    "\n",
    "<span style=\"font-size: 16px;\"><u>Content</u></span> : Any message or \"prompt\" of these personas are passes as \"Content\"\n",
    "\n",
    "\n",
    "\n",
    "__Why is this important?__ : Makes it easier to adapt an LLM to store conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"role\"__ and __\"content\"__ are passed as a dictionary in the __\"messages\"__ parameter in the chat completions API </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to claim their maiden title.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In the final of the 1987 Cricket World Cup, Australia scored 253 runs for '\n",
      " 'the loss of 5 wickets in their allotted 50 overs. England, in response, '\n",
      " 'could only manage 246 runs, giving Australia a 7-run victory.')\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>\"model\"</b> and <b>\"messages\"</b> are the two required API parameters</span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"> There are several other optional parameters that help configure the response</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n__ : Number of responses you want the LLM to generate for the instriction\n",
    "\n",
    "__max_tokens__ : Maximum number of tokens you want to restrict the Inference to (This includes both the prompt/messages and the completion)\n",
    "\n",
    "__temperature__ : Temperature controls the \"randomness\" of the responses. Higher value increases the randomness; lower value makes the output deterministic (value between 0 and 2)\n",
    "\n",
    "__top_p__ : The model considers the results of the tokens with top_p probability mass (value between 0 and 1)\n",
    "\n",
    "\n",
    "<img src=\"../Assets/Images/Temperature - Top P.png\" width=600>\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\"> __IMP__ : It is recommended to configure either one of \"temperature\" and \"top_p\" but not both</span>\n",
    "\n",
    "\n",
    "__frequency_penalty__ : Penalize new tokens based on their existing frequency in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__presence_penalty__ : Penalize new tokens based on whether they appear in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__logprobs__ : Flag to return log probability of the generated tokens *(True/False)*\n",
    "\n",
    "__logit_bias__ : Parameter to control the presence of particular tokens in the output *(Value between -100 and 100)\n",
    "\n",
    "__response_format__ : Response of the model can be requested in a particular format *(Currently : JSON and Text)*\n",
    "\n",
    "__seed__ : Beta feature for reproducible outputs (setting a seed value may produce the same output repeatedly)\n",
    "\n",
    "__stop__ : End of Sequence tokens that will stop the generation\n",
    "\n",
    "__stream__ : To receive partial message deltas *(True/False)*\n",
    "\n",
    "__user__ : ID representing end user (This helps OpenAI detect abuse. May be mandatory for higher rate limits)\n",
    "\n",
    "__tools__ : used in function calling\n",
    "\n",
    "__tool_choice__ : used in function calling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_call(model:str=\"gpt-3.5-turbo\",prompt:str=\"Have I provided any input\",n:int=1,max_tokens:int=100,temperature:float=0.5,presence_penalty:float=0):\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    presence_penalty=presence_penalty,\n",
    "    n=n\n",
    "    )\n",
    "    \n",
    "    output=''\n",
    "\n",
    "    for i in response.choices:\n",
    "        output+=str(i.message.content)+'\\n------------\\n'\n",
    "        \n",
    "    \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Harnessing the OpenAI API\"\n",
      "------------\n",
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Harnessing the OpenAI API\"\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gpt_call(prompt=\"Write a title for a workshop on openai API\",n=2,temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_uppercase(model,n,max_tokens,temperature,presence_penalty,prompt):\n",
    "    return str(type(n)) + str(type(temperature))\n",
    "\n",
    "model=gr.Radio([\"gpt-4\",\"gpt-3.5-turbo\"], label=\"Select Model\")\n",
    "n=gr.Radio([1,2,3], label=\"Number of Responses\")\n",
    "max_tokens=gr.Slider(minimum=10, maximum=500, label=\"Maximum Tokens\")\n",
    "temperature=gr.Slider(minimum=0.0, maximum=1.0, label=\"Temperature\")\n",
    "prompt=gr.Text(label=\"Prompt\")\n",
    "\n",
    "\n",
    "iface = gr.Interface(fn=gpt_call, inputs=[model,prompt,n,max_tokens,temperature], outputs=\"text\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7882\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "     \"id\": \"chatcmpl-995GVNXDheol2CTW4BE8pHP42kL0C\",\n",
      "     \"choices\": [\n",
      "          {\n",
      "               \"finish_reason\": \"stop\",\n",
      "               \"index\": 0,\n",
      "               \"logprobs\": null,\n",
      "               \"message\": {\n",
      "                    \"content\": \"In the 1987 Cricket World Cup final, Australia scored 253 runs for the loss of 5 wickets in their allotted 50 overs. England, in response, scored 246 runs, falling short by 7 runs, giving Australia their first Cricket World Cup victory.\",\n",
      "                    \"role\": \"assistant\",\n",
      "                    \"function_call\": null,\n",
      "                    \"tool_calls\": null\n",
      "               }\n",
      "          }\n",
      "     ],\n",
      "     \"created\": 1711952219,\n",
      "     \"model\": \"gpt-3.5-turbo-0125\",\n",
      "     \"object\": \"chat.completion\",\n",
      "     \"system_fingerprint\": \"fp_3bc1b5746c\",\n",
      "     \"usage\": {\n",
      "          \"completion_tokens\": 56,\n",
      "          \"prompt_tokens\": 77,\n",
      "          \"total_tokens\": 133\n",
      "     }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print((response.model_dump_json(indent=5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/Chat Completion Object.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In\n",
      " the\n",
      " \n",
      "198\n",
      "7\n",
      " Cricket\n",
      " World\n",
      " Cup\n",
      " final\n",
      ",\n",
      " Australia\n",
      " scored\n",
      " \n",
      "253\n",
      " runs\n",
      " for\n",
      " the\n",
      " loss\n",
      " of\n",
      " \n",
      "5\n",
      " w\n",
      "ickets\n",
      " in\n",
      " their\n",
      " allotted\n",
      " \n",
      "50\n",
      " overs\n",
      ".\n",
      " England\n",
      ",\n",
      " in\n",
      " reply\n",
      ",\n",
      " could\n",
      " only\n",
      " manage\n",
      " \n",
      "246\n",
      " runs\n",
      ",\n",
      " falling\n",
      " short\n",
      " by\n",
      " \n",
      "7\n",
      " runs\n",
      ",\n",
      " giving\n",
      " Australia\n",
      " their\n",
      " first\n",
      " World\n",
      " Cup\n",
      " title\n",
      ".\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Write an email requesting for a leave of absence\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"generate the entire text for a blog on a cricket match. \\\"Title\\\" is a catchy and attractive title of the blog. The \\\"Heading\\\" is the heading for each point in the blog and the \\\"Body\\\" is the text for that heading.Output in a json structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Title\": \"Thrilling Cricket Match: A Nail-biting Encounter Between Rivals\",\n",
      "  \"Heading1\": \"The Buildup\",\n",
      "  \"Body1\": \"The anticipation was palpable as the date of the match approached. Both teams were in top form and fans were eagerly waiting for the clash between the two fierce rivals.\",\n",
      "  \"Heading2\": \"The Toss\",\n",
      "  \"Body2\": \"The toss was won by Team A, who elected to bat first. It was a crucial decision as the pitch was expected to deteriorate later in the day.\",\n",
      "  \"Heading3\": \"Team A's Innings\",\n",
      "  \"Body3\": \"Team A got off to a solid start, with their top order batsmen scoring freely. However, they suffered a collapse in the middle overs, with Team B's bowlers wreaking havoc. They managed to post a competitive total of 250 runs.\",\n",
      "  \"Heading4\": \"Team B's Chase\",\n",
      "  \"Body4\": \"Team B's chase got off to a shaky start, losing early wickets. However, their middle order batsmen steadied the ship and kept them in the game. The match went down to the wire, with Team B needing 10 runs off the last over.\",\n",
      "  \"Heading5\": \"The Thrilling Finish\",\n",
      "  \"Body5\": \"In a nail-biting finish, Team B's lower order batsman hit a boundary off the last ball to win the match for their team. The crowd erupted in joy as Team B celebrated their hard-fought victory.\",\n",
      "  \"Conclusion\": \"It was a thrilling match that had fans on the edge of their seats throughout. Both teams put up a great fight, but it was Team B who emerged victorious in the end. Cricket fans will surely remember this match for a long time to come.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "  response_format={ \"type\": \"json_object\" }\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Tokens are the fundamental units of NLP</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>These units are typically words, punctuation marks, or other meaningful substrings that make up the text</b></span>\n",
    "\n",
    "Counting the number of tokens becomes important because - \n",
    "- Number of Tokens determine the amount of computation required and hence the cost you incur\n",
    "- Context Window or the maximum number of tokens an LLM can process in one go is limited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####num_tokens_from_string function to count number of tokens in a text string\n",
    "####uses tiktoken to count number of tokens in a text string\n",
    "####parameters: \"string\" is the text string, \"encoding_name\" is the encoding name to be used by tiktoken\n",
    "####returns: num_tokens->number of tokens in the text string\n",
    "####This function is used within extract_data, extract_page, extract_YT, extract_audio, extract_image functions\n",
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\") -> int: #### Function to count number of tokens in a text string ####\n",
    "    encoding = tiktoken.get_encoding(encoding_name) #### Initialize encoding ####\n",
    "    return len(encoding.encode(string)) #### Return number of tokens in the text string ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38680"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../Assets/Data/alice_in_wonderland.txt\") as f:\n",
    "    AliceInWonderland = f.read()\n",
    "\n",
    "num_tokens_from_string(AliceInWonderland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__gpt-3.5-turbo-0125__\t    |  PROMPT - $0.50 / 1M tokens   |   RESPONSE - $1.50 / 1M tokens\n",
    "\n",
    "__gpt-4__\t                |   PROMPT - $30.00 / 1M tokens\t|   RESPONSE - $60.00 / 1M tokens\n",
    "\n",
    "__gpt-4-turbo__             |\tPROMPT - $10.00 / 1M tokens\t|   RESPONSE - $30.00 / 1M tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Embeddings are vector representations of data that capture meaningful relationships between entities</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>These units are typically words, punctuation marks, or other meaningful substrings that make up the text</b></span>\n",
    "\n",
    "- All Machine Learning/AI models work with numerical data. Before the performance of any operation all text/image/audio/video data has to be transformed into a numerical representation\n",
    "\n",
    "- As a general definition, embeddings are data that has been transformed into n-dimensional matrices for use in deep learning computations.\n",
    "\n",
    "<img src=\"../Assets/Images/Embeddings.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"><b>OpenAI Embeddings</b></span>\n",
    "\n",
    "__text-embedding-3-small__\t| $0.02 / 1M tokens\n",
    "\n",
    "__text-embedding-3-large__\t| $0.13 / 1M tokens\n",
    "\n",
    "__ada v2__\t| $0.10 / 1M tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=\"The food was delicious\",\n",
    "  encoding_format=\"float\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=[\"The food was delicious\",\"The ambience was nice\",\"The service was ordinary\"],\n",
    "  encoding_format=\"float\",\n",
    "  dimensions=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"embedding\": [\n",
      "                -0.16478635,\n",
      "                -0.18134578,\n",
      "                -0.5129379,\n",
      "                -0.32290855,\n",
      "                0.0938535,\n",
      "                -0.2699992,\n",
      "                -0.0649755,\n",
      "                0.5856378,\n",
      "                -0.073911525,\n",
      "                -0.37177902\n",
      "            ],\n",
      "            \"index\": 0,\n",
      "            \"object\": \"embedding\"\n",
      "        },\n",
      "        {\n",
      "            \"embedding\": [\n",
      "                -0.029102806,\n",
      "                -0.34336767,\n",
      "                -0.66657615,\n",
      "                -0.37806797,\n",
      "                0.08133914,\n",
      "                -0.4815079,\n",
      "                -0.12963039,\n",
      "                0.060725518,\n",
      "                -0.060477655,\n",
      "                -0.17713675\n",
      "            ],\n",
      "            \"index\": 1,\n",
      "            \"object\": \"embedding\"\n",
      "        },\n",
      "        {\n",
      "            \"embedding\": [\n",
      "                -0.27565208,\n",
      "                0.1992017,\n",
      "                -0.5211547,\n",
      "                -0.37973946,\n",
      "                -0.046884183,\n",
      "                -0.20189361,\n",
      "                -0.28731704,\n",
      "                0.56458426,\n",
      "                0.01179956,\n",
      "                0.15532349\n",
      "            ],\n",
      "            \"index\": 2,\n",
      "            \"object\": \"embedding\"\n",
      "        }\n",
      "    ],\n",
      "    \"model\": \"text-embedding-3-small\",\n",
      "    \"object\": \"list\",\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 13,\n",
      "        \"total_tokens\": 13\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_q=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=\"food\",\n",
    "  encoding_format=\"float\",\n",
    "  dimensions=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=embeddings_q.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.012315562,\n",
       " -0.16567738,\n",
       " -0.069231726,\n",
       " -0.10728083,\n",
       " 0.4480219,\n",
       " -0.61836094,\n",
       " 0.29532152,\n",
       " 0.4434862,\n",
       " -0.24505134,\n",
       " -0.17046502]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=embeddings.data[0].embedding #\"The food was delicious\"\n",
    "d2=embeddings.data[1].embedding #\"The ambience was nice\"\n",
    "d3=embeddings.data[2].embedding #\"The service was ordinary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6332542]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([query],[d1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51180575]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([query],[d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28721449]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([query],[d3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1300+ Towards DataScience Medium Articles Dataset__\n",
    "\n",
    "Data Source - https://www.kaggle.com/datasets/meruvulikith/1300-towards-datascience-medium-articles-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../Assets/Data/medium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...  \n",
       "1  In my last article, I introduced the concept o...  \n",
       "2  Introduction\\n\\nThanks to its strict implement...  \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...  \n",
       "4  A Step-by-Step Implementation of Gradient Desc...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1391, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data=data.iloc[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data['embedding'] = trunc_data.Title.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>[-0.024416513741016388, 0.019436374306678772, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>[-0.011570210568606853, -0.029422052204608917,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>[-0.0054690418764948845, -0.024078190326690674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>[-0.003941336181014776, -0.022551342844963074,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>[-0.0020211779046803713, -0.000974284252151846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>[-0.026935778558254242, -0.02546488121151924, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Deep Learning on a Budget</td>\n",
       "      <td>Introduction\\n\\nWhy?\\n\\nThere are many article...</td>\n",
       "      <td>[-0.016848241910338402, -0.045139651745557785,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Generating Startup names with Markov Chains</td>\n",
       "      <td>Generating Startup names with Markov Chains\\n\\...</td>\n",
       "      <td>[0.013092967681586742, -0.0023525876458734274,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>[-0.017321426421403885, -0.021059678867459297,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How to Choose Between Multiple Models</td>\n",
       "      <td>How to Choose Between Multiple Models\\n\\nIn a ...</td>\n",
       "      <td>[0.004752688109874725, -0.0009391147177666426,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1   Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                        How to Use ggplot2 in Python   \n",
       "3   Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96                          Deep Learning on a Budget   \n",
       "97        Generating Startup names with Markov Chains   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99              How to Choose Between Multiple Models   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1   In my last article, I introduced the concept o...   \n",
       "2   Introduction\\n\\nThanks to its strict implement...   \n",
       "3   Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96  Introduction\\n\\nWhy?\\n\\nThere are many article...   \n",
       "97  Generating Startup names with Markov Chains\\n\\...   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99  How to Choose Between Multiple Models\\n\\nIn a ...   \n",
       "\n",
       "                                            embedding  \n",
       "0   [-0.024416513741016388, 0.019436374306678772, ...  \n",
       "1   [-0.011570210568606853, -0.029422052204608917,...  \n",
       "2   [-0.0054690418764948845, -0.024078190326690674...  \n",
       "3   [-0.003941336181014776, -0.022551342844963074,...  \n",
       "4   [-0.0020211779046803713, -0.000974284252151846...  \n",
       "..                                                ...  \n",
       "95  [-0.026935778558254242, -0.02546488121151924, ...  \n",
       "96  [-0.016848241910338402, -0.045139651745557785,...  \n",
       "97  [0.013092967681586742, -0.0023525876458734274,...  \n",
       "98  [-0.017321426421403885, -0.021059678867459297,...  \n",
       "99  [0.004752688109874725, -0.0009391147177666426,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string=\"Deep Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_embedding=get_embedding(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data['relevance'] = trunc_data.embedding.apply(lambda x: float(cosine_similarity([search_embedding],[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>[-0.024416513741016388, 0.019436374306678772, ...</td>\n",
       "      <td>0.300155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>[-0.011570210568606853, -0.029422052204608917,...</td>\n",
       "      <td>0.344597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>[-0.0054690418764948845, -0.024078190326690674...</td>\n",
       "      <td>0.109014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>[-0.003941336181014776, -0.022551342844963074,...</td>\n",
       "      <td>0.169241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>[-0.0020211779046803713, -0.000974284252151846...</td>\n",
       "      <td>0.388754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>[-0.026935778558254242, -0.02546488121151924, ...</td>\n",
       "      <td>0.313273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Deep Learning on a Budget</td>\n",
       "      <td>Introduction\\n\\nWhy?\\n\\nThere are many article...</td>\n",
       "      <td>[-0.016848241910338402, -0.045139651745557785,...</td>\n",
       "      <td>0.719206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Generating Startup names with Markov Chains</td>\n",
       "      <td>Generating Startup names with Markov Chains\\n\\...</td>\n",
       "      <td>[0.013092967681586742, -0.0023525876458734274,...</td>\n",
       "      <td>0.165855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>[-0.017321426421403885, -0.021059678867459297,...</td>\n",
       "      <td>0.387291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How to Choose Between Multiple Models</td>\n",
       "      <td>How to Choose Between Multiple Models\\n\\nIn a ...</td>\n",
       "      <td>[0.004752688109874725, -0.0009391147177666426,...</td>\n",
       "      <td>0.233198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1   Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                        How to Use ggplot2 in Python   \n",
       "3   Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96                          Deep Learning on a Budget   \n",
       "97        Generating Startup names with Markov Chains   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99              How to Choose Between Multiple Models   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1   In my last article, I introduced the concept o...   \n",
       "2   Introduction\\n\\nThanks to its strict implement...   \n",
       "3   Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96  Introduction\\n\\nWhy?\\n\\nThere are many article...   \n",
       "97  Generating Startup names with Markov Chains\\n\\...   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99  How to Choose Between Multiple Models\\n\\nIn a ...   \n",
       "\n",
       "                                            embedding  relevance  \n",
       "0   [-0.024416513741016388, 0.019436374306678772, ...   0.300155  \n",
       "1   [-0.011570210568606853, -0.029422052204608917,...   0.344597  \n",
       "2   [-0.0054690418764948845, -0.024078190326690674...   0.109014  \n",
       "3   [-0.003941336181014776, -0.022551342844963074,...   0.169241  \n",
       "4   [-0.0020211779046803713, -0.000974284252151846...   0.388754  \n",
       "..                                                ...        ...  \n",
       "95  [-0.026935778558254242, -0.02546488121151924, ...   0.313273  \n",
       "96  [-0.016848241910338402, -0.045139651745557785,...   0.719206  \n",
       "97  [0.013092967681586742, -0.0023525876458734274,...   0.165855  \n",
       "98  [-0.017321426421403885, -0.021059678867459297,...   0.387291  \n",
       "99  [0.004752688109874725, -0.0009391147177666426,...   0.233198  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Deep Learning on a Budget</td>\n",
       "      <td>Introduction\\n\\nWhy?\\n\\nThere are many article...</td>\n",
       "      <td>[-0.016848241910338402, -0.045139651745557785,...</td>\n",
       "      <td>0.719206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Applied AI: Going From Concept to ML Components</td>\n",
       "      <td>Opening your mind to different ways of applyin...</td>\n",
       "      <td>[-0.016721589490771294, -0.026498831808567047,...</td>\n",
       "      <td>0.482872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Transfer Learning Intuition for Text Classific...</td>\n",
       "      <td>Transfer Learning Intuition for Text Classific...</td>\n",
       "      <td>[-0.022648293524980545, 0.0017097401432693005,...</td>\n",
       "      <td>0.478084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Reinforcement Learning Introduction</td>\n",
       "      <td>Reinforcement Learning Introduction\\n\\nAn intr...</td>\n",
       "      <td>[0.009179973974823952, -0.05973218381404877, 0...</td>\n",
       "      <td>0.470448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Wild Wide AI: responsible data science</td>\n",
       "      <td>Wild Wide AI: responsible data science\\n\\nData...</td>\n",
       "      <td>[0.041022028774023056, -0.00013012583076488227...</td>\n",
       "      <td>0.445177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Why Machine Learning Models Degrade In Production</td>\n",
       "      <td>After several failed ML projects due to unexpe...</td>\n",
       "      <td>[0.012906364165246487, 0.030608268454670906, 0...</td>\n",
       "      <td>0.437601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>An Introduction to Recurrent Neural Networks f...</td>\n",
       "      <td>An Introduction to Recurrent Neural Networks f...</td>\n",
       "      <td>[-0.01791239343583584, -0.02631079964339733, 0...</td>\n",
       "      <td>0.424425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What if AI model understanding were easy?</td>\n",
       "      <td>Irreverent Demystifiers\\n\\nWhat if AI model un...</td>\n",
       "      <td>[-0.011677316389977932, -0.0018296980997547507...</td>\n",
       "      <td>0.422850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Getting Started with Google BigQuery’s Machine...</td>\n",
       "      <td>While still in Beta, BigQuery ML has been avai...</td>\n",
       "      <td>[-0.034346841275691986, 0.012737160548567772, ...</td>\n",
       "      <td>0.416386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Review: DeepPose — Cascade of CNN (Human Pose ...</td>\n",
       "      <td>Review: DeepPose — Cascade of CNN (Human Pose ...</td>\n",
       "      <td>[0.01773509941995144, -0.03974172845482826, 0....</td>\n",
       "      <td>0.412199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "96                          Deep Learning on a Budget   \n",
       "79    Applied AI: Going From Concept to ML Components   \n",
       "73  Transfer Learning Intuition for Text Classific...   \n",
       "54                Reinforcement Learning Introduction   \n",
       "80             Wild Wide AI: responsible data science   \n",
       "26  Why Machine Learning Models Degrade In Production   \n",
       "29  An Introduction to Recurrent Neural Networks f...   \n",
       "9           What if AI model understanding were easy?   \n",
       "68  Getting Started with Google BigQuery’s Machine...   \n",
       "69  Review: DeepPose — Cascade of CNN (Human Pose ...   \n",
       "\n",
       "                                                 Text  \\\n",
       "96  Introduction\\n\\nWhy?\\n\\nThere are many article...   \n",
       "79  Opening your mind to different ways of applyin...   \n",
       "73  Transfer Learning Intuition for Text Classific...   \n",
       "54  Reinforcement Learning Introduction\\n\\nAn intr...   \n",
       "80  Wild Wide AI: responsible data science\\n\\nData...   \n",
       "26  After several failed ML projects due to unexpe...   \n",
       "29  An Introduction to Recurrent Neural Networks f...   \n",
       "9   Irreverent Demystifiers\\n\\nWhat if AI model un...   \n",
       "68  While still in Beta, BigQuery ML has been avai...   \n",
       "69  Review: DeepPose — Cascade of CNN (Human Pose ...   \n",
       "\n",
       "                                            embedding  relevance  \n",
       "96  [-0.016848241910338402, -0.045139651745557785,...   0.719206  \n",
       "79  [-0.016721589490771294, -0.026498831808567047,...   0.482872  \n",
       "73  [-0.022648293524980545, 0.0017097401432693005,...   0.478084  \n",
       "54  [0.009179973974823952, -0.05973218381404877, 0...   0.470448  \n",
       "80  [0.041022028774023056, -0.00013012583076488227...   0.445177  \n",
       "26  [0.012906364165246487, 0.030608268454670906, 0...   0.437601  \n",
       "29  [-0.01791239343583584, -0.02631079964339733, 0...   0.424425  \n",
       "9   [-0.011677316389977932, -0.0018296980997547507...   0.422850  \n",
       "68  [-0.034346841275691986, 0.012737160548567772, ...   0.416386  \n",
       "69  [0.01773509941995144, -0.03974172845482826, 0....   0.412199  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_data.sort_values(by=\"relevance\",ascending=False).iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix = np.vstack(trunc_data.embedding.values)\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "trunc_data['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>relevance</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>[-0.024416513741016388, 0.019436374306678772, ...</td>\n",
       "      <td>0.300155</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>[-0.011570210568606853, -0.029422052204608917,...</td>\n",
       "      <td>0.344597</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>[-0.0054690418764948845, -0.024078190326690674...</td>\n",
       "      <td>0.109014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>[-0.003941336181014776, -0.022551342844963074,...</td>\n",
       "      <td>0.169241</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>[-0.0020211779046803713, -0.000974284252151846...</td>\n",
       "      <td>0.388754</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>Data Scientist’s toolkit — How to gather data ...</td>\n",
       "      <td>[-0.026935778558254242, -0.02546488121151924, ...</td>\n",
       "      <td>0.313273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Deep Learning on a Budget</td>\n",
       "      <td>Introduction\\n\\nWhy?\\n\\nThere are many article...</td>\n",
       "      <td>[-0.016848241910338402, -0.045139651745557785,...</td>\n",
       "      <td>0.719206</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Generating Startup names with Markov Chains</td>\n",
       "      <td>Generating Startup names with Markov Chains\\n\\...</td>\n",
       "      <td>[0.013092967681586742, -0.0023525876458734274,...</td>\n",
       "      <td>0.165855</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>A Recipe for using Open Source Machine Learnin...</td>\n",
       "      <td>[-0.017321426421403885, -0.021059678867459297,...</td>\n",
       "      <td>0.387291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How to Choose Between Multiple Models</td>\n",
       "      <td>How to Choose Between Multiple Models\\n\\nIn a ...</td>\n",
       "      <td>[0.004752688109874725, -0.0009391147177666426,...</td>\n",
       "      <td>0.233198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1   Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                        How to Use ggplot2 in Python   \n",
       "3   Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96                          Deep Learning on a Budget   \n",
       "97        Generating Startup names with Markov Chains   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99              How to Choose Between Multiple Models   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1   In my last article, I introduced the concept o...   \n",
       "2   Introduction\\n\\nThanks to its strict implement...   \n",
       "3   Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4   A Step-by-Step Implementation of Gradient Desc...   \n",
       "..                                                ...   \n",
       "95  Data Scientist’s toolkit — How to gather data ...   \n",
       "96  Introduction\\n\\nWhy?\\n\\nThere are many article...   \n",
       "97  Generating Startup names with Markov Chains\\n\\...   \n",
       "98  A Recipe for using Open Source Machine Learnin...   \n",
       "99  How to Choose Between Multiple Models\\n\\nIn a ...   \n",
       "\n",
       "                                            embedding  relevance  Cluster  \n",
       "0   [-0.024416513741016388, 0.019436374306678772, ...   0.300155        3  \n",
       "1   [-0.011570210568606853, -0.029422052204608917,...   0.344597        3  \n",
       "2   [-0.0054690418764948845, -0.024078190326690674...   0.109014        1  \n",
       "3   [-0.003941336181014776, -0.022551342844963074,...   0.169241        2  \n",
       "4   [-0.0020211779046803713, -0.000974284252151846...   0.388754        3  \n",
       "..                                                ...        ...      ...  \n",
       "95  [-0.026935778558254242, -0.02546488121151924, ...   0.313273        2  \n",
       "96  [-0.016848241910338402, -0.045139651745557785,...   0.719206        3  \n",
       "97  [0.013092967681586742, -0.0023525876458734274,...   0.165855        3  \n",
       "98  [-0.017321426421403885, -0.021059678867459297,...   0.387291        0  \n",
       "99  [0.004752688109874725, -0.0009391147177666426,...   0.233198        0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data[\"Prompt\"] = trunc_data[\"Title\"] + \" belongs to Cluster number \" + trunc_data[\"Cluster\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=''\n",
    "for i in trunc_data[\"Prompt\"]:\n",
    "    info+=i+'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Below is information of blog titles grouped into clusters. There are four clusters. Come up with Names for these four clusters basis the titles present in them\\n\\n\"\"\"+info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Names:\n",
      "1. Data Visualization and Analysis\n",
      "2. Data Management and Processing\n",
      "3. Machine Learning Implementations\n",
      "4. AI Ethics and Understanding\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Users look at LLMs for knowledge and wisdom, yet LLMs are sophisticated predictors of what word comes next</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>Hallucinations and Restricted \"Parameteric\" Memory are the biggest drawbacks of LLMs</b></span>\n",
    "\n",
    "- Hallucinations - Very confidently, provide incorrect information.\n",
    "- Missing Knowledge - Not having information (information available only that is available the training data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What is amica developed by Portable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amica is a comprehensive digital solution developed by Portable, a design '\n",
      " 'and technology company based in London. It is designed to provide support '\n",
      " 'and information to individuals seeking asylum in the UK, helping them '\n",
      " 'navigate the complex immigration system and access the services and '\n",
      " 'resources they need. Amica aims to make the asylum process more transparent '\n",
      " 'and accessible for refugees, empowering them to make informed decisions '\n",
      " 'about their future.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14px; color: blue\"><b>Retrieval Augmented Generation or RAG seems to solve these problems</b></span>\n",
    "\n",
    "\n",
    "<img src=\"../Assets/Images/RAG.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_page(link): #### Function to extract text from weblink ####\n",
    "    address=link #### Store weblink in address variable ####\n",
    "    response=requests.get(address) #### Get response from weblink using requests ####\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') #### Parse response using BeautifulSoup ####\n",
    "    text=soup.get_text() #### Extract text from parsed response ####\n",
    "    lines = filter(lambda x: x.strip(), text.splitlines()) #### Filter out empty lines ####\n",
    "    website_text = \"\\n\".join(lines) #### Join lines to form text ####\n",
    "    return website_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=extract_page(\"https://portable.com.au/work/amica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt=f\"You have been provided a context about below. Based only on the context answer the following question \\n\\n context : {text}\\n\\n question : {prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": augmented_prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amica is a digital solution developed by Portable for separating couples to '\n",
      " 'help guide former partners towards an amicable resolution of family law '\n",
      " 'issues.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_name=\"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(encoding_name)\n",
    "len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = PyPDF2.PdfReader(\"../Assets/Data/InnovatorsDilemma.pdf\")\n",
    "pages = reader.pages\n",
    "# get all pages data\n",
    "text = \"\".join([page.extract_text() for page in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117932"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG At Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_create_embeddings(text): #### Function to create embeddings from text ####\n",
    "    with open('../Assets/Data/temp.txt','w') as f: #### Write text to a temporary file ####\n",
    "         f.write(text) #### Write text to a temporary file ####\n",
    "         f.close() #### Close temporary file ####\n",
    "    loader=TextLoader('../Assets/Data/temp.txt') #### Load temporary file using TextLoader ####\n",
    "    document=loader.load() #### Extract text from temporary file ####\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=2000) #### Initialize text splitter to split text into chunks of 10000 tokens ####\n",
    "    docs = text_splitter.split_documents(document) #### Split document into chunks of 10000 tokens ####\n",
    "    num_emb=len(docs) #### Count number of embeddings ####\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key) #### Initialize embeddings ####\n",
    "    db = FAISS.from_documents(docs, embeddings) #### Create embeddings from text ####\n",
    "    return db, num_emb #### Return database with embeddings and number of embeddings ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db, num_emb=split_text_and_create_embeddings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(num_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.save_local(folder_path=\"../Assets/Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vectors=FAISS.load_local(folder_path=\"../Assets/Data/\",embeddings=embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_context(db,query): ###### search_context function\n",
    "     defin=db.similarity_search(query) ###### call the FAISS similarity_search function that searches the database for the most relevant section to the user question and orders the results in descending order of relevance\n",
    "     return defin[0].page_content ###### return the most relevant section to the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Why do great firms fail?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=search_context(local_vectors,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt=f\"You have been provided a context in [[[]]] and a prompt below. Respond to the prompt only from the context. If the information is not present in the context, respond with \\\"I don't know\\\"\\nContext : [[[{context}]]]\\nPrompt : {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Great firms fail due to their inability to stay atop their industries when '\n",
      " 'they confront certain types of market and technological change. It is not '\n",
      " 'about the failure of simply any company, but of well-managed companies that '\n",
      " 'have their competitive antennae up, listen astutely to their customers, '\n",
      " 'invest aggressively in new technologies, and yet still lose market '\n",
      " 'dominance. This failure can happen in industries that move fast or slow, '\n",
      " 'built on different types of technology, and in both manufacturing and '\n",
      " 'service industries.')\n"
     ]
    }
   ],
   "source": [
    "pprint(client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": augmented_prompt}\n",
    "  ]\n",
    ").choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/SFT.png\" width=1000>\n",
    "\n",
    "Taking a general purpose model and train it to perform a specialized/specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hallucinations\n",
    "- RAG Misses\n",
    "- Learn New Information (When data size is large)\n",
    "- Cost Optimization\n",
    "- Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges**\n",
    "\n",
    "- Need quality data\n",
    "- Upfront cost\n",
    "- Expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three broad steps in LLM finetuning -\n",
    "\n",
    "1. Data Preparation (for the specific task/use case)\n",
    "2. Training (chosen training methodology)\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path='../Assets/Data/'\n",
    "training_data_file='fine_tuning_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning=pd.read_csv(data_folder_path+training_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data_for_finetuning, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_end=\"<--\"\n",
    "completion_end=\"-->\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_for_turbo(data,file):\n",
    "  with open(file,'w') as f:\n",
    "    for _,rows in data.iterrows():\n",
    "      prompt=rows['prompt']+prompt_end\n",
    "      completion=rows['completion']+completion_end\n",
    "      json_line = {'messages': [{'role': 'system',\n",
    "                                'content': ''},\n",
    "                                {'role': 'user',\n",
    "                                 'content': prompt},\n",
    "                                {'role': 'assistant',\n",
    "                                'content': completion}]}\n",
    "      f.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "  print(f'JSONlines file \"{file}\" has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_turbo_name='train_turbo.jsonl'\n",
    "test_file_turbo_name='test_turbo.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_data_for_turbo(train_data,data_folder_path+train_file_turbo_name)\n",
    "convert_data_for_turbo(test_data,data_folder_path+test_file_turbo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "                file=open(data_folder_path+train_file_turbo_name, \"rb\"),\n",
    "                purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "                file=open(data_folder_path+test_file_turbo_name, \"rb\"),\n",
    "                purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "              training_file=\"file-6AUxqwaqG16AFy1PVLEGFKZI\",\n",
    "              validation_file=\"file-7GByyQB3nnXSKZtgLQt542UU\",\n",
    "              model=\"gpt-3.5-turbo-0125\",\n",
    "              suffix=\"Workshop\",\n",
    "              hyperparameters={\"n_epochs\":1,\n",
    "                               \"batch_size\":1,\n",
    "                               }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list(limit=2).model_dump_json(indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-OP2LgxuFTe4UxHoxYbYzG7Vw\", limit=2).model_dump_json(indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[10][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=str(test_data.iloc[10][\"prompt\"])+prompt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:artificial-kimtelligence:workshop:94dD2dNr\",\n",
    "  messages=[\n",
    "            {'role': 'system','content': ''},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homomorphic encryption may sound complex, but it enables operations on encrypted data without decryption, essential in secure computati\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homomorphic encryption may sound complex, but it enables operations on encrypted data without decryption, essential in secure computation.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:artificial-kimtelligence:workshop:94dD2dNr\",\n",
    "  messages=[\n",
    "    {'role': 'system','content': ''},\n",
    "    {\"role\": \"user\", \"content\": \"explain How Homomorphic Encryption Works\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/profile.png\" width=50> [Hi! I'm Abhinav!](https://www.linkedin.com/in/abhinav-kimothi/)\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\">>><b>Hope to stay connected!</b></span>\n",
    "\n",
    "\n",
    " \n",
    "[![GitHub followers](https://img.shields.io/github/followers/abhinav-kimothi?label=Follow&style=social)](https://github.com/abhinav-kimothi)\n",
    "[![Me](https://img.shields.io/badge/Medium-8A2BE2)](https://medium.com/@abhinavkimothi)\n",
    "[![LIn](https://img.shields.io/badge/LinkedIn-blue)](https://www.linkedin.com/in/abhinav-kimothi/)\n",
    "[![Mail](https://img.shields.io/badge/eMail-green)](mailto:abhinav.kimothi.ds@gmail.com)\n",
    "[![Twitter Follow](https://img.shields.io/twitter/follow/@?style=social)](https://twitter.com/abhinav_kimothi)\n",
    "\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\">>><b>Also, read these for more details on Generative AI!</b></span>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://abhinavkimothi.gumroad.com/l/GenAILLM\">\n",
    "    <img src=\"https://public-files.gumroad.com/jsdnnne2gnhu61f6hrdprwx2255i\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/RAG\">\n",
    "    <img src=\"https://public-files.gumroad.com/v17k9tp2fnbbtg8iwoxt4m3xgivq\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/GenAITaxonomy\">\n",
    "    <img src=\"https://public-files.gumroad.com/a730ysxb7a928bb5xkz6fuqabaqp\" width=150>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/KCBAI.png\" height =300>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
