{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using OpenAI LLMs (GPT 3.5/GPT4)\n",
    "\n",
    "- Installation & Importing Libraries\n",
    "- Accessing OpenAI (Keys) & Setting up Client\n",
    "\n",
    "---\n",
    "- Chat Completion API (Basic Text Generation)\n",
    "- Tokens\n",
    "- Embeddings\n",
    "- RAG\n",
    "- Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Time (75 minutes coding + 15 minutes QnA) \n",
    "- We'll also take questions at the end of each section\n",
    "- Aim is to explore OpenAI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai #OpenAI python library\n",
    "from openai import OpenAI #OpenAI Client\n",
    "from configparser import ConfigParser #library to read the config file\n",
    "\n",
    "import tiktoken #library to count tokens\n",
    "\n",
    "import gradio as gr #library for gradio interface\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity #for calculating similarities between embeddings\n",
    "\n",
    "from bs4 import BeautifulSoup #for extracting text from webpages\n",
    "import requests\n",
    "\n",
    "import PyPDF2 #for reading text from pdf \n",
    "\n",
    "from langchain.document_loaders import TextLoader #to load text for RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #to chunk data for RAG\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings #to crate embeddings for RAG\n",
    "from langchain.vectorstores import FAISS # to store embeddings in a vector index\n",
    "from sklearn.model_selection import train_test_split #for model finetuning\n",
    "\n",
    "import json #for creating finetuning files\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing OpenAI (Keys) & Setting up Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__ : Have you procured your OpenAI API key?\n",
    "\n",
    "If not,  create one here - https://platform.openai.com/api-keys\n",
    "\n",
    "Also, check if you have sufficient balance - https://platform.openai.com/account/billing/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__ : Read and set the OpenAI API key in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_object = ConfigParser()\n",
    "config_object.read(\"../config.ini\")\n",
    "openai.api_key = config_object[\"OPENAI\"][\"openai_key\"] #read the api key from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key=\"<Your API Key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of storing the API key in the environment. You may choose as per your preference or your organisation's policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: Initialize the OpenAI client. This serves as an interface to interact with OpenAI's services and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready! Let's try and make the first call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion API (Basic Text Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of applications</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px;\"><i>\"LLMs can be, generally, thought of as a next word prediction model\"</i></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>What is an LLM?</b></span>\n",
    "\n",
    "- LLMs are __machine learning models__ that have learned from __massive datasets__ of human-generated content, finding statistical patterns to replicate human-like abilities.\n",
    "\n",
    "- __Foundation models__, also known as base models, have been trained on trillions of words for weeks or months using extensive compute power. These models have __billions of parameters__, which represent their memory and enable sophisticated tasks.\n",
    "\n",
    "- __Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code syntax, you provide natural language prompts to the models__.\n",
    "\n",
    "- When you pass a __prompt__ to the model, it predicts the next words and generates a __completion__. This process is known as __inference__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>Prompts, Completions and Inference!</b></span>\n",
    "\n",
    "<img src=\"../Assets/Images/LLM Inference.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>Available OpenAI models</b></span>\n",
    "\n",
    "---\n",
    "\n",
    "__GPT 4__\n",
    "\n",
    "__<u>(Production)</u>__\n",
    "\n",
    "<u>Name        | Context Window    | Cut-off date      | Snapshot</u>\n",
    "\n",
    "__gpt-4__       | 8,192 tokens      | Up to Sep 2021    | gpt-4-0613\n",
    "\n",
    "__gpt-4-32k__   | 32,768 tokens     | Up to Sep 2021    | gpt-4-32k-0613\n",
    "\n",
    "<b><u>(Preview)</b></u>\n",
    "\n",
    "__gpt-4-turbo-preview__     | 128,000 tokens | Up to Dec 2023    | gpt-4-1106-preview\n",
    "\n",
    "__gpt-4-vision-preview__    | 128,000 tokens | Up to Apr 2023    | gpt-4-1106-vision-preview\n",
    "\n",
    "---\n",
    "\n",
    "__GPT 3.5__\n",
    "\n",
    "__gpt-3.5-turbo__ | 16,385 tokens | Up to Sep 2021 | gpt-3.5-turbo-1106\n",
    "\n",
    "__gpt-3.5-turbo-instruct__ | 4,096 tokens | Up to Sep 2021\n",
    "\n",
    "---\n",
    "\n",
    "For more details, visit the official documentation -> https://platform.openai.com/docs/models\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"model\"__ is passed as a parameter in the chat completions API</span>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>OpenAI Chat Messages (Prompt) Structure</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px;\"><u>Role</u></span> : OpenAI allows for *three* roles/personas - \n",
    "1. __System__ : The overarching constraints/definitions/intructions of the system that the LLM should \"remember\"\n",
    "2. __User__ : Any instruction a user wants to pass to the LLM\n",
    "3. __Assistant__ : The response from the LLM\n",
    "\n",
    "<span style=\"font-size: 16px;\"><u>Content</u></span> : Any message or \"prompt\" of these personas are passes as \"Content\"\n",
    "\n",
    "\n",
    "\n",
    "__Why is this important?__ : Makes it easier to adapt an LLM to store conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"role\"__ and __\"content\"__ are passed as a dictionary in the __\"messages\"__ parameter in the chat completions API </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>\"model\"</b> and <b>\"messages\"</b> are the two required API parameters</span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"> There are several other optional parameters that help configure the response</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n__ : Number of responses you want the LLM to generate for the instriction\n",
    "\n",
    "__max_tokens__ : Maximum number of tokens you want to restrict the Inference to (This includes both the prompt/messages and the completion)\n",
    "\n",
    "__temperature__ : Temperature controls the \"randomness\" of the responses. Higher value increases the randomness; lower value makes the output deterministic (value between 0 and 2)\n",
    "\n",
    "__top_p__ : The model considers the results of the tokens with top_p probability mass (value between 0 and 1)\n",
    "\n",
    "\n",
    "<img src=\"../Assets/Images/Temperature - Top P.png\" width=600>\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\"> __IMP__ : It is recommended to configure either one of \"temperature\" and \"top_p\" but not both</span>\n",
    "\n",
    "\n",
    "__frequency_penalty__ : Penalize new tokens based on their existing frequency in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__presence_penalty__ : Penalize new tokens based on whether they appear in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__logprobs__ : Flag to return log probability of the generated tokens *(True/False)*\n",
    "\n",
    "__logit_bias__ : Parameter to control the presence of particular tokens in the output *(Value between -100 and 100)\n",
    "\n",
    "__response_format__ : Response of the model can be requested in a particular format *(Currently : JSON and Text)*\n",
    "\n",
    "__seed__ : Beta feature for reproducible outputs (setting a seed value may produce the same output repeatedly)\n",
    "\n",
    "__stop__ : End of Sequence tokens that will stop the generation\n",
    "\n",
    "__stream__ : To receive partial message deltas *(True/False)*\n",
    "\n",
    "__user__ : ID representing end user (This helps OpenAI detect abuse. May be mandatory for higher rate limits)\n",
    "\n",
    "__tools__ : used in function calling\n",
    "\n",
    "__tool_choice__ : used in function calling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_call(model:str=\"gpt-3.5-turbo\",prompt:str=\"Have I provided any input\",n:int=1,max_tokens:int=100,temperature:float=0.5,presence_penalty:float=0):\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    presence_penalty=presence_penalty,\n",
    "    n=n\n",
    "    )\n",
    "    \n",
    "    output=''\n",
    "\n",
    "    for i in response.choices:\n",
    "        output+=str(i.message.content)+'\\n------------\\n'\n",
    "        \n",
    "    \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_call(prompt=\"Write a title for a workshop on openai API\",n=2,temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_uppercase(model,n,max_tokens,temperature,presence_penalty,prompt):\n",
    "    return str(type(n)) + str(type(temperature))\n",
    "\n",
    "model=gr.Radio([\"gpt-4\",\"gpt-3.5-turbo\"], label=\"Select Model\")\n",
    "n=gr.Radio([1,2,3], label=\"Number of Responses\")\n",
    "max_tokens=gr.Slider(minimum=10, maximum=500, label=\"Maximum Tokens\")\n",
    "temperature=gr.Slider(minimum=0.0, maximum=1.0, label=\"Temperature\")\n",
    "prompt=gr.Text(label=\"Prompt\")\n",
    "\n",
    "\n",
    "iface = gr.Interface(fn=gpt_call, inputs=[model,prompt,n,max_tokens,temperature], outputs=\"text\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((response.model_dump_json(indent=5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/Chat Completion Object.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Write an email requesting for a leave of absence\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"generate the entire text for a blog on a cricket match. \\\"Title\\\" is a catchy and attractive title of the blog. The \\\"Heading\\\" is the heading for each point in the blog and the \\\"Body\\\" is the text for that heading.Output in a json structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "  response_format={ \"type\": \"json_object\" }\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Tokens are the fundamental units of NLP</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>These units are typically words, punctuation marks, or other meaningful substrings that make up the text</b></span>\n",
    "\n",
    "Counting the number of tokens becomes important because - \n",
    "- Number of Tokens determine the amount of computation required and hence the cost you incur\n",
    "- Context Window or the maximum number of tokens an LLM can process in one go is limited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####num_tokens_from_string function to count number of tokens in a text string\n",
    "####uses tiktoken to count number of tokens in a text string\n",
    "####parameters: \"string\" is the text string, \"encoding_name\" is the encoding name to be used by tiktoken\n",
    "####returns: num_tokens->number of tokens in the text string\n",
    "####This function is used within extract_data, extract_page, extract_YT, extract_audio, extract_image functions\n",
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\") -> int: #### Function to count number of tokens in a text string ####\n",
    "    encoding = tiktoken.get_encoding(encoding_name) #### Initialize encoding ####\n",
    "    return len(encoding.encode(string)) #### Return number of tokens in the text string ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(\"Hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Assets/Data/alice_in_wonderland.txt\") as f:\n",
    "    AliceInWonderland = f.read()\n",
    "\n",
    "num_tokens_from_string(AliceInWonderland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__gpt-3.5-turbo-0125__\t    |  PROMPT - $0.50 / 1M tokens   |   RESPONSE - $1.50 / 1M tokens\n",
    "\n",
    "__gpt-4__\t                |   PROMPT - $30.00 / 1M tokens\t|   RESPONSE - $60.00 / 1M tokens\n",
    "\n",
    "__gpt-4-turbo__             |\tPROMPT - $10.00 / 1M tokens\t|   RESPONSE - $30.00 / 1M tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Embeddings are vector representations of data that capture meaningful relationships between entities</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>These units are typically words, punctuation marks, or other meaningful substrings that make up the text</b></span>\n",
    "\n",
    "- All Machine Learning/AI models work with numerical data. Before the performance of any operation all text/image/audio/video data has to be transformed into a numerical representation\n",
    "\n",
    "- As a general definition, embeddings are data that has been transformed into n-dimensional matrices for use in deep learning computations.\n",
    "\n",
    "<img src=\"../Assets/Images/Embeddings.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"><b>OpenAI Embeddings</b></span>\n",
    "\n",
    "__text-embedding-3-small__\t| $0.02 / 1M tokens\n",
    "\n",
    "__text-embedding-3-large__\t| $0.13 / 1M tokens\n",
    "\n",
    "__ada v2__\t| $0.10 / 1M tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=\"The food was delicious\",\n",
    "  encoding_format=\"float\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=[\"The food was delicious\",\"The ambience was nice\",\"The service was ordinary\"],\n",
    "  encoding_format=\"float\",\n",
    "  dimensions=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_q=client.embeddings.create(\n",
    "  model=\"text-embedding-3-small\",\n",
    "  input=\"food\",\n",
    "  encoding_format=\"float\",\n",
    "  dimensions=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=embeddings_q.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=embeddings.data[0].embedding #\"The food was delicious\"\n",
    "d2=embeddings.data[1].embedding #\"The ambience was nice\"\n",
    "d3=embeddings.data[2].embedding #\"The service was ordinary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([query],[d1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([query],[d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([query],[d3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1300+ Towards DataScience Medium Articles Dataset__\n",
    "\n",
    "Data Source - https://www.kaggle.com/datasets/meruvulikith/1300-towards-datascience-medium-articles-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../Assets/Data/medium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data=data.iloc[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data['embedding'] = trunc_data.Title.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string=\"Deep Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_embedding=get_embedding(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data['relevance'] = trunc_data.embedding.apply(lambda x: float(cosine_similarity([search_embedding],[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data.sort_values(by=\"relevance\",ascending=False).iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix = np.vstack(trunc_data.embedding.values)\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "trunc_data['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data[\"Prompt\"] = trunc_data[\"Title\"] + \" belongs to Cluster number \" + trunc_data[\"Cluster\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=''\n",
    "for i in trunc_data[\"Prompt\"]:\n",
    "    info+=i+'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Below is information of blog titles grouped into clusters. There are four clusters. Come up with Names for these four clusters basis the titles present in them\\n\\n\"\"\"+info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\">>><b>Users look at LLMs for knowledge and wisdom, yet LLMs are sophisticated predictors of what word comes next</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>Hallucinations and Restricted \"Parameteric\" Memory are the biggest drawbacks of LLMs</b></span>\n",
    "\n",
    "- Hallucinations - Very confidently, provide incorrect information.\n",
    "- Missing Knowledge - Not having information (information available only that is available the training data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What is amica developed by Portable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14px; color: blue\"><b>Retrieval Augmented Generation or RAG seems to solve these problems</b></span>\n",
    "\n",
    "\n",
    "<img src=\"../Assets/Images/RAG.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_page(link): #### Function to extract text from weblink ####\n",
    "    address=link #### Store weblink in address variable ####\n",
    "    response=requests.get(address) #### Get response from weblink using requests ####\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') #### Parse response using BeautifulSoup ####\n",
    "    text=soup.get_text() #### Extract text from parsed response ####\n",
    "    lines = filter(lambda x: x.strip(), text.splitlines()) #### Filter out empty lines ####\n",
    "    website_text = \"\\n\".join(lines) #### Join lines to form text ####\n",
    "    return website_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=extract_page(\"https://portable.com.au/work/amica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt=f\"You have been provided a context about below. Based only on the context answer the following question \\n\\n context : {text}\\n\\n question : {prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": augmented_prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_name=\"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(encoding_name)\n",
    "len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = PyPDF2.PdfReader(\"../Assets/Data/InnovatorsDilemma.pdf\")\n",
    "pages = reader.pages\n",
    "# get all pages data\n",
    "text = \"\".join([page.extract_text() for page in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG At Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_create_embeddings(text): #### Function to create embeddings from text ####\n",
    "    with open('../Assets/Data/temp.txt','w') as f: #### Write text to a temporary file ####\n",
    "         f.write(text) #### Write text to a temporary file ####\n",
    "         f.close() #### Close temporary file ####\n",
    "    loader=TextLoader('../Assets/Data/temp.txt') #### Load temporary file using TextLoader ####\n",
    "    document=loader.load() #### Extract text from temporary file ####\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=2000) #### Initialize text splitter to split text into chunks of 10000 tokens ####\n",
    "    docs = text_splitter.split_documents(document) #### Split document into chunks of 10000 tokens ####\n",
    "    num_emb=len(docs) #### Count number of embeddings ####\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key) #### Initialize embeddings ####\n",
    "    db = FAISS.from_documents(docs, embeddings) #### Create embeddings from text ####\n",
    "    return db, num_emb #### Return database with embeddings and number of embeddings ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db, num_emb=split_text_and_create_embeddings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.save_local(folder_path=\"../Assets/Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vectors=FAISS.load_local(folder_path=\"../Assets/Data/\",embeddings=embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_context(db,query): ###### search_context function\n",
    "     defin=db.similarity_search(query) ###### call the FAISS similarity_search function that searches the database for the most relevant section to the user question and orders the results in descending order of relevance\n",
    "     return defin[0].page_content ###### return the most relevant section to the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Why do great firms fail?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=search_context(local_vectors,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt=f\"You have been provided a context in [[[]]] and a prompt below. Respond to the prompt only from the context. If the information is not present in the context, respond with \\\"I don't know\\\"\\nContext : [[[{context}]]]\\nPrompt : {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": augmented_prompt}\n",
    "  ]\n",
    ").choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/SFT.png\" width=1000>\n",
    "\n",
    "Taking a general purpose model and train it to perform a specialized/specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hallucinations\n",
    "- RAG Misses\n",
    "- Learn New Information (When data size is large)\n",
    "- Cost Optimization\n",
    "- Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges**\n",
    "\n",
    "- Need quality data\n",
    "- Upfront cost\n",
    "- Expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three broad steps in LLM finetuning -\n",
    "\n",
    "1. Data Preparation (for the specific task/use case)\n",
    "2. Training (chosen training methodology)\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path='../Assets/Data/'\n",
    "training_data_file='fine_tuning_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning=pd.read_csv(data_folder_path+training_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_finetuning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data_for_finetuning, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_end=\"<--\"\n",
    "completion_end=\"-->\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_for_turbo(data,file):\n",
    "  with open(file,'w') as f:\n",
    "    for _,rows in data.iterrows():\n",
    "      prompt=rows['prompt']+prompt_end\n",
    "      completion=rows['completion']+completion_end\n",
    "      json_line = {'messages': [{'role': 'system',\n",
    "                                'content': ''},\n",
    "                                {'role': 'user',\n",
    "                                 'content': prompt},\n",
    "                                {'role': 'assistant',\n",
    "                                'content': completion}]}\n",
    "      f.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "  print(f'JSONlines file \"{file}\" has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_turbo_name='train_turbo.jsonl'\n",
    "test_file_turbo_name='test_turbo.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_data_for_turbo(train_data,data_folder_path+train_file_turbo_name)\n",
    "convert_data_for_turbo(test_data,data_folder_path+test_file_turbo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "                file=open(data_folder_path+train_file_turbo_name, \"rb\"),\n",
    "                purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "                file=open(data_folder_path+test_file_turbo_name, \"rb\"),\n",
    "                purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "              training_file=\"file-6AUxqwaqG16AFy1PVLEGFKZI\",\n",
    "              validation_file=\"file-7GByyQB3nnXSKZtgLQt542UU\",\n",
    "              model=\"gpt-3.5-turbo-0125\",\n",
    "              suffix=\"Workshop\",\n",
    "              hyperparameters={\"n_epochs\":1,\n",
    "                               \"batch_size\":1,\n",
    "                               }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list(limit=2).model_dump_json(indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-OP2LgxuFTe4UxHoxYbYzG7Vw\", limit=2).model_dump_json(indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[10][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=str(test_data.iloc[10][\"prompt\"])+prompt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:artificial-kimtelligence:workshop:94dD2dNr\",\n",
    "  messages=[\n",
    "            {'role': 'system','content': ''},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:artificial-kimtelligence:workshop:94dD2dNr\",\n",
    "  messages=[\n",
    "    {'role': 'system','content': ''},\n",
    "    {\"role\": \"user\", \"content\": \"explain How Homomorphic Encryption Works\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/profile.png\" width=50> [Hi! I'm Abhinav!](https://www.linkedin.com/in/abhinav-kimothi/)\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\">>><b>Hope to stay connected!</b></span>\n",
    "\n",
    "\n",
    " \n",
    "[![GitHub followers](https://img.shields.io/github/followers/abhinav-kimothi?label=Follow&style=social)](https://github.com/abhinav-kimothi)\n",
    "[![Me](https://img.shields.io/badge/Medium-8A2BE2)](https://medium.com/@abhinavkimothi)\n",
    "[![LIn](https://img.shields.io/badge/LinkedIn-blue)](https://www.linkedin.com/in/abhinav-kimothi/)\n",
    "[![Mail](https://img.shields.io/badge/eMail-green)](mailto:abhinav.kimothi.ds@gmail.com)\n",
    "[![Twitter Follow](https://img.shields.io/twitter/follow/@?style=social)](https://twitter.com/abhinav_kimothi)\n",
    "\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\">>><b>Also, read these for more details on Generative AI!</b></span>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://abhinavkimothi.gumroad.com/l/GenAILLM\">\n",
    "    <img src=\"https://public-files.gumroad.com/jsdnnne2gnhu61f6hrdprwx2255i\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/RAG\">\n",
    "    <img src=\"https://public-files.gumroad.com/v17k9tp2fnbbtg8iwoxt4m3xgivq\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/GenAITaxonomy\">\n",
    "    <img src=\"https://public-files.gumroad.com/a730ysxb7a928bb5xkz6fuqabaqp\" width=150>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/KCBAI.png\" height =300>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
